{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.5.1\n",
            "Torchvision version: 0.20.1\n",
            "CUDA available: True\n",
            "CUDA version: 12.1\n",
            "Device name: Quadro RTX 5000\n",
            "CUDA_HOME: None\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "print(\"CUDA_HOME:\", os.environ.get(\"CUDA_HOME\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-22T13:09:05.901785Z",
          "iopub.status.busy": "2025-04-22T13:09:05.901368Z",
          "iopub.status.idle": "2025-04-22T13:20:26.682044Z",
          "shell.execute_reply": "2025-04-22T13:20:26.681120Z",
          "shell.execute_reply.started": "2025-04-22T13:09:05.901745Z"
        },
        "id": "8AD0e5irzQc0",
        "outputId": "832e85af-2e68-4714-b9b5-5a1e81faaf64",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=32, LR=2.0e-04, WD=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/100: train=79.1%  val=92.6%\n",
            "  Ep2/100: train=92.8%  val=94.4%\n",
            "  Ep3/100: train=95.3%  val=96.4%\n",
            "  Ep4/100: train=96.6%  val=95.9%\n",
            "  Ep5/100: train=97.4%  val=95.9%\n",
            "  Ep6/100: train=97.7%  val=94.3%\n",
            "  Ep7/100: train=98.0%  val=96.5%\n",
            "  Ep8/100: train=98.5%  val=95.9%\n",
            "  Ep9/100: train=98.6%  val=95.4%\n",
            "  Ep10/100: train=98.7%  val=96.0%\n",
            "  Ep11/100: train=98.9%  val=96.5%\n",
            "  Ep12/100: train=98.9%  val=96.1%\n",
            "  Ep13/100: train=98.9%  val=95.5%\n",
            "  Ep14/100: train=99.1%  val=95.7%\n",
            "  Ep15/100: train=99.1%  val=95.9%\n",
            "  Ep16/100: train=99.3%  val=96.6%\n",
            "  Ep17/100: train=99.3%  val=94.8%\n",
            "  Ep18/100: train=99.3%  val=96.5%\n",
            "  Ep19/100: train=99.4%  val=95.7%\n",
            "  Ep20/100: train=99.3%  val=96.4%\n",
            "  Ep21/100: train=99.4%  val=96.1%\n",
            "  Ep22/100: train=99.5%  val=96.4%\n",
            "  Ep23/100: train=99.5%  val=96.4%\n",
            "  Ep24/100: train=99.5%  val=96.0%\n",
            "  Ep25/100: train=99.7%  val=96.5%\n",
            "  Ep26/100: train=99.5%  val=96.7%\n",
            "  Ep27/100: train=99.6%  val=96.0%\n",
            "  Ep28/100: train=99.6%  val=95.9%\n",
            "  Ep29/100: train=99.6%  val=96.9%\n",
            "  Ep30/100: train=99.7%  val=96.6%\n",
            "  Ep31/100: train=99.7%  val=95.9%\n",
            "  Ep32/100: train=99.7%  val=96.1%\n",
            "  Ep33/100: train=99.7%  val=96.8%\n",
            "  Ep34/100: train=99.7%  val=95.9%\n",
            "  Ep35/100: train=99.7%  val=95.6%\n",
            "  Ep36/100: train=99.7%  val=96.5%\n",
            "  Ep37/100: train=99.8%  val=95.9%\n",
            "  Ep38/100: train=99.7%  val=95.0%\n",
            "  Ep39/100: train=99.7%  val=96.7%\n",
            "  Ep40/100: train=99.8%  val=96.4%\n",
            "  Ep41/100: train=99.8%  val=96.3%\n",
            "  Ep42/100: train=99.8%  val=95.9%\n",
            "  Ep43/100: train=99.8%  val=96.6%\n",
            "  Ep44/100: train=99.9%  val=96.2%\n",
            "  Ep45/100: train=99.9%  val=96.3%\n",
            "  Ep46/100: train=99.9%  val=96.0%\n",
            "  Ep47/100: train=99.9%  val=96.3%\n",
            "  Ep48/100: train=99.9%  val=95.9%\n",
            "  Ep49/100: train=99.8%  val=96.4%\n",
            "  Ep50/100: train=99.9%  val=95.8%\n",
            "  Ep51/100: train=99.9%  val=96.3%\n",
            "  Ep52/100: train=99.9%  val=96.3%\n",
            "  Ep53/100: train=99.9%  val=96.8%\n",
            "  Ep54/100: train=99.9%  val=96.6%\n",
            "  Ep55/100: train=99.9%  val=96.6%\n",
            "  Ep56/100: train=99.9%  val=96.8%\n",
            "  Ep57/100: train=99.9%  val=96.7%\n",
            "  Ep58/100: train=100.0%  val=96.4%\n",
            "  Ep59/100: train=100.0%  val=96.8%\n",
            "  Ep60/100: train=100.0%  val=96.7%\n",
            "  Ep61/100: train=100.0%  val=96.7%\n",
            "  Ep62/100: train=100.0%  val=96.8%\n",
            "  Ep63/100: train=100.0%  val=96.8%\n",
            "  Ep64/100: train=100.0%  val=97.0%\n",
            "  Ep65/100: train=100.0%  val=96.9%\n",
            "  Ep66/100: train=100.0%  val=96.8%\n",
            "  Ep67/100: train=100.0%  val=96.9%\n",
            "  Ep68/100: train=100.0%  val=97.1%\n",
            "  Ep69/100: train=100.0%  val=96.8%\n",
            "  Ep70/100: train=100.0%  val=97.0%\n",
            "  Ep71/100: train=100.0%  val=97.1%\n",
            "  Ep72/100: train=100.0%  val=97.1%\n",
            "  Ep73/100: train=100.0%  val=97.2%\n",
            "  Ep74/100: train=100.0%  val=97.3%\n",
            "  Ep75/100: train=100.0%  val=97.4%\n",
            "  Ep76/100: train=100.0%  val=97.2%\n",
            "  Ep77/100: train=100.0%  val=97.3%\n",
            "  Ep78/100: train=100.0%  val=97.2%\n",
            "  Ep79/100: train=100.0%  val=97.3%\n",
            "  Ep80/100: train=100.0%  val=97.4%\n",
            "  Ep81/100: train=100.0%  val=97.3%\n",
            "  Ep82/100: train=100.0%  val=97.2%\n",
            "  Ep83/100: train=100.0%  val=97.3%\n",
            "  Ep84/100: train=100.0%  val=97.3%\n",
            "  Ep85/100: train=100.0%  val=97.1%\n",
            "  Ep86/100: train=100.0%  val=97.2%\n",
            "  Ep87/100: train=100.0%  val=97.3%\n",
            "  Ep88/100: train=100.0%  val=97.4%\n",
            "  Ep89/100: train=100.0%  val=97.4%\n",
            "  Ep90/100: train=100.0%  val=97.4%\n",
            "  Ep91/100: train=100.0%  val=97.4%\n",
            "  Ep92/100: train=100.0%  val=97.3%\n",
            "  Ep93/100: train=100.0%  val=97.4%\n",
            "  Ep94/100: train=100.0%  val=97.4%\n",
            "  Ep95/100: train=100.0%  val=97.4%\n",
            "  Ep96/100: train=100.0%  val=97.5%\n",
            "  Ep97/100: train=100.0%  val=97.4%\n",
            "  Ep98/100: train=100.0%  val=97.4%\n",
            "  Ep99/100: train=100.0%  val=97.4%\n",
            "  Ep100/100: train=100.0%  val=97.3%\n",
            "\n",
            ">>> Testing BS=32, LR=1.9e-04, WD=0.5\n",
            "  Ep1/100: train=78.8%  val=91.4%\n",
            "  Ep2/100: train=92.9%  val=95.8%\n",
            "  Ep3/100: train=95.4%  val=95.7%\n",
            "  Ep4/100: train=96.6%  val=96.1%\n",
            "  Ep5/100: train=97.0%  val=95.2%\n",
            "  Ep6/100: train=97.4%  val=93.9%\n",
            "  Ep7/100: train=97.9%  val=96.4%\n",
            "  Ep8/100: train=98.2%  val=95.7%\n",
            "  Ep9/100: train=98.1%  val=95.4%\n",
            "  Ep10/100: train=98.4%  val=96.0%\n",
            "  Ep11/100: train=98.4%  val=95.8%\n",
            "  Ep12/100: train=98.5%  val=96.3%\n",
            "  Ep13/100: train=98.7%  val=95.8%\n",
            "  Ep14/100: train=98.5%  val=95.1%\n",
            "  Ep15/100: train=98.7%  val=96.1%\n",
            "  Ep16/100: train=98.9%  val=94.1%\n",
            "  Ep17/100: train=98.8%  val=95.1%\n",
            "  Ep18/100: train=98.7%  val=95.7%\n",
            "  Ep19/100: train=98.8%  val=96.1%\n",
            "  Ep20/100: train=98.6%  val=95.6%\n",
            "  Ep21/100: train=98.8%  val=93.1%\n",
            "  Ep22/100: train=98.8%  val=95.2%\n",
            "  Ep23/100: train=99.0%  val=94.1%\n",
            "  Ep24/100: train=99.0%  val=94.8%\n",
            "  Ep25/100: train=98.8%  val=95.7%\n",
            "  Ep26/100: train=99.0%  val=95.1%\n",
            "  Ep27/100: train=98.9%  val=95.2%\n",
            "  Ep28/100: train=98.9%  val=95.8%\n",
            "  Ep29/100: train=99.1%  val=94.3%\n",
            "  Ep30/100: train=99.0%  val=93.9%\n",
            "  Ep31/100: train=99.1%  val=93.7%\n",
            "  Ep32/100: train=99.1%  val=93.2%\n",
            "  Ep33/100: train=99.2%  val=94.5%\n",
            "  Ep34/100: train=99.0%  val=94.7%\n",
            "  Ep35/100: train=99.1%  val=93.2%\n",
            "  Ep36/100: train=99.2%  val=95.1%\n",
            "  Ep37/100: train=99.1%  val=95.3%\n",
            "  Ep38/100: train=99.2%  val=94.5%\n",
            "  Ep39/100: train=99.2%  val=94.0%\n",
            "  Ep40/100: train=99.4%  val=95.5%\n",
            "  Ep41/100: train=99.3%  val=94.2%\n",
            "  Ep42/100: train=99.2%  val=95.1%\n",
            "  Ep43/100: train=99.3%  val=94.5%\n",
            "  Ep44/100: train=99.3%  val=94.3%\n",
            "  Ep45/100: train=99.4%  val=91.9%\n",
            "  Ep46/100: train=99.4%  val=94.2%\n",
            "  Ep47/100: train=99.5%  val=94.3%\n",
            "  Ep48/100: train=99.5%  val=94.9%\n",
            "  Ep49/100: train=99.5%  val=94.4%\n",
            "  Ep50/100: train=99.5%  val=94.7%\n",
            "  Ep51/100: train=99.5%  val=94.2%\n",
            "  Ep52/100: train=99.6%  val=94.3%\n",
            "  Ep53/100: train=99.6%  val=94.3%\n",
            "  Ep54/100: train=99.7%  val=95.1%\n",
            "  Ep55/100: train=99.6%  val=94.7%\n",
            "  Ep56/100: train=99.6%  val=94.0%\n",
            "  Ep57/100: train=99.7%  val=92.7%\n",
            "  Ep58/100: train=99.7%  val=94.3%\n",
            "  Ep59/100: train=99.7%  val=94.8%\n",
            "  Ep60/100: train=99.7%  val=94.8%\n",
            "  Ep61/100: train=99.8%  val=94.1%\n",
            "  Ep62/100: train=99.7%  val=95.1%\n",
            "  Ep63/100: train=99.8%  val=95.1%\n",
            "  Ep64/100: train=99.9%  val=95.6%\n",
            "  Ep65/100: train=99.9%  val=95.4%\n",
            "  Ep66/100: train=99.9%  val=94.9%\n",
            "  Ep67/100: train=99.8%  val=94.5%\n",
            "  Ep68/100: train=99.9%  val=95.2%\n",
            "  Ep69/100: train=99.9%  val=94.8%\n",
            "  Ep70/100: train=99.9%  val=95.3%\n",
            "  Ep71/100: train=99.9%  val=95.2%\n",
            "  Ep72/100: train=100.0%  val=95.4%\n",
            "  Ep73/100: train=99.9%  val=94.6%\n",
            "  Ep74/100: train=99.9%  val=95.1%\n",
            "  Ep75/100: train=100.0%  val=95.5%\n",
            "  Ep76/100: train=100.0%  val=95.8%\n",
            "  Ep77/100: train=100.0%  val=95.8%\n",
            "  Ep78/100: train=100.0%  val=95.5%\n",
            "  Ep79/100: train=100.0%  val=95.5%\n",
            "  Ep80/100: train=100.0%  val=95.9%\n",
            "  Ep81/100: train=100.0%  val=95.9%\n",
            "  Ep82/100: train=100.0%  val=95.8%\n",
            "  Ep83/100: train=100.0%  val=95.9%\n",
            "  Ep84/100: train=100.0%  val=95.9%\n",
            "  Ep85/100: train=100.0%  val=96.0%\n",
            "  Ep86/100: train=100.0%  val=96.2%\n",
            "  Ep87/100: train=100.0%  val=96.3%\n",
            "  Ep88/100: train=100.0%  val=96.1%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, urllib.request, zipfile\n",
        "\n",
        "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
        "LOCAL_OR_COLAB = \"LOCAL\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 100\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = 0.6\n",
        "VAL_FRAC   = 0.2\n",
        "TEST_FRAC  = 0.2\n",
        "\n",
        "# hyperparameter grid\n",
        "BATCH_SIZES = [32, 64]\n",
        "GRID        = [\n",
        "    (2e-4,    0.1  ),  # SimCLR\n",
        "    (1.875e-4,0.5  ),  # SatMIP\n",
        "    (3.75e-4, 0.5  ),  # SatMIPS\n",
        "]\n",
        "\n",
        "# ─── DATASET DOWNLOAD ────────────────────────────────────────────────────────────\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB/\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "# ─── HELPERS ─────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485,0.456,0.406],\n",
        "            std =[0.229,0.224,0.225]\n",
        "        )\n",
        "    ])\n",
        "    ds = datasets.ImageFolder(root=data_dir, transform=tf)\n",
        "    n   = len(ds)\n",
        "    n_train = int(TRAIN_FRAC * n)\n",
        "    n_val   = int(VAL_FRAC   * n)\n",
        "    n_test  = n - n_train - n_val\n",
        "    train_ds, val_ds, test_ds = random_split(ds, [n_train, n_val, n_test])\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size, shuffle=True),\n",
        "        DataLoader(val_ds,   batch_size, shuffle=False),\n",
        "        DataLoader(test_ds,  batch_size, shuffle=False),\n",
        "        len(ds.classes)\n",
        "    )\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet18(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "    model.train()\n",
        "    tot_loss, corr, tot = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss   = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if sched: sched.step()\n",
        "        tot_loss += loss.item()\n",
        "        preds    = logits.argmax(dim=1)\n",
        "        corr    += (preds==yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "    return tot_loss/len(loader), 100*corr/tot\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    corr, tot = 0,0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            corr += (preds==yb).sum().item()\n",
        "            tot  += yb.size(0)\n",
        "    return 100 * corr / tot\n",
        "\n",
        "# ─── PHASE 1: GRID SEARCH ────────────────────────────────────────────────────────\n",
        "def hyperparam_search(pretrained = True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "    # loop over all combos in one go\n",
        "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd}\")\n",
        "        set_seed(SEED)\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "        model = build_model(n_cls, pretrained = pretrained)\n",
        "\n",
        "        # optimizer + paper schedule\n",
        "        opt = optim.AdamW(model.parameters(),\n",
        "                          lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "        total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "        warmup_steps = len(tr_dl)\n",
        "        sched = SequentialLR(\n",
        "            opt,\n",
        "            schedulers=[\n",
        "                LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "                CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "            ],\n",
        "            milestones=[warmup_steps]\n",
        "        )\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # train & validate\n",
        "        for ep in range(NUM_EPOCHS):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "            val_acc          = evaluate(model, val_dl)\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train={tr_acc:.1f}%  val={val_acc:.1f}%\")\n",
        "\n",
        "        # pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_cfg = (bs, lr, wd)\n",
        "            best_model = copy.deepcopy(model)   # store the weights\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]} \"\n",
        "          f\"→ val={best_val:.1f}%\")\n",
        "    return best_cfg, best_model\n",
        "\n",
        "# ─── PHASE 2: LINEAR PROBE ───────────────────────────────────────────────────────\n",
        "def linear_probe(frozen_model, train_dl, test_dl, lr, wd):\n",
        "    # freeze backbone\n",
        "    for p in frozen_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # new head\n",
        "    n_in = frozen_model.fc.in_features\n",
        "    n_out = frozen_model.fc.out_features\n",
        "    frozen_model.fc = nn.Linear(n_in, n_out).to(DEVICE)\n",
        "\n",
        "    opt = optim.AdamW(frozen_model.fc.parameters(),\n",
        "                      lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n>>> Running linear probe on frozen backbone\")\n",
        "    for ep in range(NUM_EPOCHS):\n",
        "        loss, acc = train_one_epoch(frozen_model, train_dl, opt, crit, sched=None)\n",
        "        print(f\"  Probe Ep{ep+1}/{NUM_EPOCHS}: train={acc:.1f}%\")\n",
        "    test_acc = evaluate(frozen_model, test_dl)\n",
        "    print(f\"→ Probe test acc: {test_acc:.1f}%\")\n",
        "    return test_acc\n",
        "\n",
        "# ─── MAIN ───────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    best_cfg, best_model = hyperparam_search(pretrained = True)\n",
        "    # rebuild loaders once more so we have the same splits\n",
        "    bs, lr, wd = best_cfg\n",
        "    tr_dl, val_dl, te_dl, _ = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "    # Option A: probe on just the original training split\n",
        "    probe_acc = linear_probe(best_model, tr_dl, te_dl, lr, wd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-22T13:23:12.750684Z",
          "iopub.status.busy": "2025-04-22T13:23:12.750347Z",
          "iopub.status.idle": "2025-04-22T13:25:30.744259Z",
          "shell.execute_reply": "2025-04-22T13:25:30.743546Z",
          "shell.execute_reply.started": "2025-04-22T13:23:12.750660Z"
        },
        "id": "LiAuFSdXsWrh",
        "outputId": "7e2d8dcf-9c75-4525-b644-2e09c7ec6aa5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Running linear probe on frozen backbone\n",
            "  Probe Ep1/20: train=95.4%\n",
            "  Probe Ep2/20: train=98.9%\n",
            "  Probe Ep3/20: train=98.9%\n",
            "  Probe Ep4/20: train=98.9%\n",
            "  Probe Ep5/20: train=98.9%\n",
            "  Probe Ep6/20: train=98.9%\n",
            "  Probe Ep7/20: train=98.9%\n",
            "  Probe Ep8/20: train=99.0%\n",
            "  Probe Ep9/20: train=98.9%\n",
            "  Probe Ep10/20: train=98.9%\n",
            "  Probe Ep11/20: train=98.9%\n",
            "  Probe Ep12/20: train=99.0%\n",
            "  Probe Ep13/20: train=98.9%\n",
            "  Probe Ep14/20: train=98.9%\n",
            "  Probe Ep15/20: train=99.0%\n",
            "  Probe Ep16/20: train=98.9%\n",
            "  Probe Ep17/20: train=99.0%\n",
            "  Probe Ep18/20: train=99.0%\n",
            "  Probe Ep19/20: train=98.9%\n",
            "  Probe Ep20/20: train=99.0%\n",
            "→ Probe test acc: 99.1%\n"
          ]
        }
      ],
      "source": [
        "# Option B (train head on train+val):\n",
        "merged = torch.utils.data.ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "merged_dl = DataLoader(merged, bs, shuffle=True)\n",
        "probe_acc = linear_probe(best_model, merged_dl, te_dl, lr, wd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOeyjqi5sWrm"
      },
      "source": [
        "## Linear probing with scikit learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-22T13:33:59.314333Z",
          "iopub.status.busy": "2025-04-22T13:33:59.313768Z",
          "iopub.status.idle": "2025-04-22T13:36:03.813977Z",
          "shell.execute_reply": "2025-04-22T13:36:03.813245Z",
          "shell.execute_reply.started": "2025-04-22T13:33:59.314310Z"
        },
        "id": "8_hOxDJqsWrq",
        "outputId": "bf710dfd-5780-4dce-cc41-c15d1c0e2e83",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting: 100%|██████████| 254/254 [00:11<00:00, 21.31it/s]\n",
            "Extracting: 100%|██████████| 85/85 [00:03<00:00, 22.55it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn probe test accuracy: 99.07%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9913    0.9879    0.9896       579\n",
            "           1     0.9965    0.9914    0.9939       580\n",
            "           2     0.9828    0.9828    0.9828       581\n",
            "           3     0.9899    0.9820    0.9859       500\n",
            "           4     0.9936    0.9979    0.9957       469\n",
            "           5     0.9861    0.9884    0.9872       430\n",
            "           6     0.9808    0.9884    0.9846       517\n",
            "           7     0.9968    0.9968    0.9968       617\n",
            "           8     0.9865    0.9942    0.9903       513\n",
            "           9     1.0000    0.9967    0.9984       614\n",
            "\n",
            "    accuracy                         0.9907      5400\n",
            "   macro avg     0.9904    0.9906    0.9905      5400\n",
            "weighted avg     0.9908    0.9907    0.9907      5400\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    # remove last classifier layer\n",
        "    backbone = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    backbone.to(device)\n",
        "    all_feats, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(loader, desc=\"Extracting\"):\n",
        "            xb = xb.to(device)\n",
        "            feats = backbone(xb)           # shape: (B, C, 1, 1)\n",
        "            feats = feats.view(feats.size(0), -1)  # (B, C)\n",
        "            all_feats.append(feats.cpu().numpy())\n",
        "            all_labels.append(yb.numpy())\n",
        "    return np.vstack(all_feats), np.concatenate(all_labels)\n",
        "\n",
        "# 1) Extract embeddings from frozen best_model\n",
        "X_train, y_train = extract_embeddings(best_model, tr_dl, DEVICE)\n",
        "X_test,  y_test  = extract_embeddings(best_model, te_dl, DEVICE)\n",
        "\n",
        "# 2) Fit a scikit‑learn “linear probe” (logistic regression)\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.preprocessing   import StandardScaler\n",
        "from sklearn.metrics         import accuracy_score, classification_report\n",
        "\n",
        "# scale features\n",
        "scaler  = StandardScaler().fit(X_train)\n",
        "X_tr_s  = scaler.transform(X_train)\n",
        "X_te_s  = scaler.transform(X_test)\n",
        "\n",
        "# C ≃ 1/weight_decay — try a small grid\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='saga',\n",
        "    multi_class='multinomial',\n",
        "    max_iter=200\n",
        ").fit(X_tr_s, y_train)\n",
        "\n",
        "# 3) Evaluate\n",
        "preds = clf.predict(X_te_s)\n",
        "acc   = accuracy_score(y_test, preds)\n",
        "print(f\"sklearn probe test accuracy: {acc*100:.2f}%\")\n",
        "print(classification_report(y_test, preds, digits=4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
