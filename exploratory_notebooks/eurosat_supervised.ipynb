{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, urllib.request, zipfile\n",
        "\n",
        "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
        "LOCAL_OR_COLAB = \"COLAB\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 20\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = 0.6\n",
        "VAL_FRAC   = 0.2\n",
        "TEST_FRAC  = 0.2\n",
        "\n",
        "# hyperparameter grid\n",
        "BATCH_SIZES = [32, 64]\n",
        "GRID        = [\n",
        "    (2e-4,    0.1  ),  # SimCLR\n",
        "    (1.875e-4,0.5  ),  # SatMIP\n",
        "    (3.75e-4, 0.5  ),  # SatMIPS\n",
        "]\n",
        "\n",
        "# ─── DATASET DOWNLOAD ────────────────────────────────────────────────────────────\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/home/juliana/internship_LINUX/datasets/EuroSAT_RGB\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "# ─── HELPERS ─────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485,0.456,0.406],\n",
        "            std =[0.229,0.224,0.225]\n",
        "        )\n",
        "    ])\n",
        "    ds = datasets.ImageFolder(root=data_dir, transform=tf)\n",
        "    n   = len(ds)\n",
        "    n_train = int(TRAIN_FRAC * n)\n",
        "    n_val   = int(VAL_FRAC   * n)\n",
        "    n_test  = n - n_train - n_val\n",
        "    train_ds, val_ds, test_ds = random_split(ds, [n_train, n_val, n_test])\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size, shuffle=True),\n",
        "        DataLoader(val_ds,   batch_size, shuffle=False),\n",
        "        DataLoader(test_ds,  batch_size, shuffle=False),\n",
        "        len(ds.classes)\n",
        "    )\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet18(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "    model.train()\n",
        "    tot_loss, corr, tot = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss   = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if sched: sched.step()\n",
        "        tot_loss += loss.item()\n",
        "        preds    = logits.argmax(dim=1)\n",
        "        corr    += (preds==yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "    return tot_loss/len(loader), 100*corr/tot\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    corr, tot = 0,0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            corr += (preds==yb).sum().item()\n",
        "            tot  += yb.size(0)\n",
        "    return 100 * corr / tot\n",
        "\n",
        "# ─── PHASE 1: GRID SEARCH ────────────────────────────────────────────────────────\n",
        "def hyperparam_search(pretrained = True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "    # loop over all combos in one go\n",
        "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd}\")\n",
        "        set_seed(SEED)\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "        model = build_model(n_cls, pretrained = pretrained)\n",
        "\n",
        "        # optimizer + paper schedule\n",
        "        opt = optim.AdamW(model.parameters(),\n",
        "                          lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "        total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "        warmup_steps = len(tr_dl)\n",
        "        sched = SequentialLR(\n",
        "            opt,\n",
        "            schedulers=[\n",
        "                LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "                CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "            ],\n",
        "            milestones=[warmup_steps]\n",
        "        )\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # train & validate\n",
        "        for ep in range(NUM_EPOCHS):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "            val_acc          = evaluate(model, val_dl)\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train={tr_acc:.1f}%  val={val_acc:.1f}%\")\n",
        "\n",
        "        # pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_cfg = (bs, lr, wd)\n",
        "            best_model = copy.deepcopy(model)   # store the weights\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]} \"\n",
        "          f\"→ val={best_val:.1f}%\")\n",
        "    return best_cfg, best_model\n",
        "\n",
        "# ─── PHASE 2: LINEAR PROBE ───────────────────────────────────────────────────────\n",
        "def linear_probe(frozen_model, train_dl, test_dl, lr, wd):\n",
        "    # freeze backbone\n",
        "    for p in frozen_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # new head\n",
        "    n_in = frozen_model.fc.in_features\n",
        "    n_out = frozen_model.fc.out_features\n",
        "    frozen_model.fc = nn.Linear(n_in, n_out).to(DEVICE)\n",
        "\n",
        "    opt = optim.AdamW(frozen_model.fc.parameters(),\n",
        "                      lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n>>> Running linear probe on frozen backbone\")\n",
        "    for ep in range(NUM_EPOCHS):\n",
        "        loss, acc = train_one_epoch(frozen_model, train_dl, opt, crit, sched=None)\n",
        "        print(f\"  Probe Ep{ep+1}/{NUM_EPOCHS}: train={acc:.1f}%\")\n",
        "    test_acc = evaluate(frozen_model, test_dl)\n",
        "    print(f\"→ Probe test acc: {test_acc:.1f}%\")\n",
        "    return test_acc\n",
        "\n",
        "# ─── MAIN ───────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    best_cfg, best_model = hyperparam_search(pretrained = True)\n",
        "    # rebuild loaders once more so we have the same splits\n",
        "    bs, lr, wd = best_cfg\n",
        "    tr_dl, val_dl, te_dl, _ = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "    # Option A: probe on just the original training split\n",
        "    probe_acc = linear_probe(best_model, tr_dl, te_dl, lr, wd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AD0e5irzQc0",
        "outputId": "832e85af-2e68-4714-b9b5-5a1e81faaf64",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:09:05.901368Z",
          "iopub.execute_input": "2025-04-22T13:09:05.901785Z",
          "iopub.status.idle": "2025-04-22T13:20:26.682044Z",
          "shell.execute_reply.started": "2025-04-22T13:09:05.901745Z",
          "shell.execute_reply": "2025-04-22T13:20:26.681120Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Testing BS=32, LR=2.0e-04, WD=0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Ep1/20: train=79.2%  val=92.4%\n",
            "  Ep2/20: train=92.8%  val=95.3%\n",
            "  Ep3/20: train=95.0%  val=95.6%\n",
            "  Ep4/20: train=96.6%  val=96.2%\n",
            "  Ep5/20: train=97.5%  val=96.0%\n",
            "  Ep6/20: train=98.0%  val=96.1%\n",
            "  Ep7/20: train=98.5%  val=96.1%\n",
            "  Ep8/20: train=98.9%  val=95.9%\n",
            "  Ep9/20: train=99.1%  val=96.1%\n",
            "  Ep10/20: train=99.4%  val=95.9%\n",
            "  Ep11/20: train=99.6%  val=97.2%\n",
            "  Ep12/20: train=99.7%  val=97.1%\n",
            "  Ep13/20: train=99.8%  val=97.1%\n",
            "  Ep14/20: train=99.9%  val=97.2%\n",
            "  Ep15/20: train=99.9%  val=97.4%\n",
            "  Ep16/20: train=100.0%  val=97.3%\n",
            "  Ep17/20: train=100.0%  val=97.6%\n",
            "  Ep18/20: train=100.0%  val=97.6%\n",
            "  Ep19/20: train=100.0%  val=97.6%\n",
            "  Ep20/20: train=100.0%  val=97.4%\n",
            "\n",
            ">>> Testing BS=32, LR=1.9e-04, WD=0.5\n",
            "  Ep1/20: train=79.0%  val=93.4%\n",
            "  Ep2/20: train=92.8%  val=95.6%\n",
            "  Ep3/20: train=95.2%  val=95.5%\n",
            "  Ep4/20: train=96.6%  val=96.0%\n",
            "  Ep5/20: train=97.4%  val=96.5%\n",
            "  Ep6/20: train=98.0%  val=96.0%\n",
            "  Ep7/20: train=98.4%  val=95.7%\n",
            "  Ep8/20: train=98.7%  val=96.3%\n",
            "  Ep9/20: train=99.0%  val=94.4%\n",
            "  Ep10/20: train=99.3%  val=96.8%\n",
            "  Ep11/20: train=99.5%  val=96.8%\n",
            "  Ep12/20: train=99.7%  val=97.4%\n",
            "  Ep13/20: train=99.7%  val=97.4%\n",
            "  Ep14/20: train=99.9%  val=97.3%\n",
            "  Ep15/20: train=99.9%  val=97.5%\n",
            "  Ep16/20: train=100.0%  val=97.4%\n",
            "  Ep17/20: train=100.0%  val=97.6%\n",
            "  Ep18/20: train=100.0%  val=97.6%\n",
            "  Ep19/20: train=100.0%  val=97.6%\n",
            "  Ep20/20: train=100.0%  val=97.6%\n",
            "\n",
            ">>> Testing BS=32, LR=3.8e-04, WD=0.5\n",
            "  Ep1/20: train=80.9%  val=90.3%\n",
            "  Ep2/20: train=90.5%  val=92.5%\n",
            "  Ep3/20: train=93.3%  val=93.1%\n",
            "  Ep4/20: train=95.1%  val=93.0%\n",
            "  Ep5/20: train=95.9%  val=94.0%\n",
            "  Ep6/20: train=96.4%  val=94.4%\n",
            "  Ep7/20: train=96.9%  val=94.6%\n",
            "  Ep8/20: train=97.6%  val=95.3%\n",
            "  Ep9/20: train=98.2%  val=93.5%\n",
            "  Ep10/20: train=98.5%  val=95.4%\n",
            "  Ep11/20: train=98.8%  val=95.0%\n",
            "  Ep12/20: train=99.2%  val=95.8%\n",
            "  Ep13/20: train=99.5%  val=95.9%\n",
            "  Ep14/20: train=99.5%  val=96.2%\n",
            "  Ep15/20: train=99.8%  val=96.8%\n",
            "  Ep16/20: train=99.9%  val=96.7%\n",
            "  Ep17/20: train=99.9%  val=96.8%\n",
            "  Ep18/20: train=100.0%  val=97.0%\n",
            "  Ep19/20: train=100.0%  val=96.9%\n",
            "  Ep20/20: train=100.0%  val=97.0%\n",
            "\n",
            ">>> Testing BS=64, LR=2.0e-04, WD=0.1\n",
            "  Ep1/20: train=78.1%  val=93.8%\n",
            "  Ep2/20: train=95.0%  val=95.7%\n",
            "  Ep3/20: train=96.7%  val=94.9%\n",
            "  Ep4/20: train=97.8%  val=96.4%\n",
            "  Ep5/20: train=98.2%  val=95.7%\n",
            "  Ep6/20: train=98.6%  val=96.3%\n",
            "  Ep7/20: train=98.9%  val=96.2%\n",
            "  Ep8/20: train=99.2%  val=97.2%\n",
            "  Ep9/20: train=99.4%  val=95.7%\n",
            "  Ep10/20: train=99.4%  val=96.7%\n",
            "  Ep11/20: train=99.5%  val=96.9%\n",
            "  Ep12/20: train=99.7%  val=97.1%\n",
            "  Ep13/20: train=99.8%  val=97.6%\n",
            "  Ep14/20: train=99.9%  val=97.8%\n",
            "  Ep15/20: train=99.9%  val=97.4%\n",
            "  Ep16/20: train=100.0%  val=97.5%\n",
            "  Ep17/20: train=100.0%  val=97.8%\n",
            "  Ep18/20: train=100.0%  val=97.9%\n",
            "  Ep19/20: train=100.0%  val=98.0%\n",
            "  Ep20/20: train=100.0%  val=98.0%\n",
            "\n",
            ">>> Testing BS=64, LR=1.9e-04, WD=0.5\n",
            "  Ep1/20: train=77.6%  val=93.6%\n",
            "  Ep2/20: train=95.0%  val=96.1%\n",
            "  Ep3/20: train=97.0%  val=94.7%\n",
            "  Ep4/20: train=97.8%  val=94.4%\n",
            "  Ep5/20: train=97.9%  val=96.4%\n",
            "  Ep6/20: train=98.6%  val=96.2%\n",
            "  Ep7/20: train=98.9%  val=96.2%\n",
            "  Ep8/20: train=99.2%  val=96.6%\n",
            "  Ep9/20: train=99.3%  val=96.9%\n",
            "  Ep10/20: train=99.4%  val=97.4%\n",
            "  Ep11/20: train=99.6%  val=96.3%\n",
            "  Ep12/20: train=99.7%  val=97.1%\n",
            "  Ep13/20: train=99.8%  val=97.3%\n",
            "  Ep14/20: train=99.9%  val=97.5%\n",
            "  Ep15/20: train=99.9%  val=97.7%\n",
            "  Ep16/20: train=100.0%  val=97.6%\n",
            "  Ep17/20: train=100.0%  val=97.6%\n",
            "  Ep18/20: train=100.0%  val=97.8%\n",
            "  Ep19/20: train=100.0%  val=97.6%\n",
            "  Ep20/20: train=100.0%  val=97.6%\n",
            "\n",
            ">>> Testing BS=64, LR=3.8e-04, WD=0.5\n",
            "  Ep1/20: train=81.7%  val=86.4%\n",
            "  Ep2/20: train=93.1%  val=93.2%\n",
            "  Ep3/20: train=95.3%  val=93.6%\n",
            "  Ep4/20: train=96.2%  val=94.0%\n",
            "  Ep5/20: train=96.9%  val=94.1%\n",
            "  Ep6/20: train=97.5%  val=95.8%\n",
            "  Ep7/20: train=98.1%  val=96.1%\n",
            "  Ep8/20: train=98.6%  val=94.2%\n",
            "  Ep9/20: train=98.8%  val=92.6%\n",
            "  Ep10/20: train=99.1%  val=96.5%\n",
            "  Ep11/20: train=99.4%  val=96.3%\n",
            "  Ep12/20: train=99.4%  val=96.9%\n",
            "  Ep13/20: train=99.7%  val=97.0%\n",
            "  Ep14/20: train=99.9%  val=97.4%\n",
            "  Ep15/20: train=99.9%  val=97.4%\n",
            "  Ep16/20: train=100.0%  val=97.1%\n",
            "  Ep17/20: train=99.9%  val=97.6%\n",
            "  Ep18/20: train=100.0%  val=97.5%\n",
            "  Ep19/20: train=100.0%  val=97.6%\n",
            "  Ep20/20: train=100.0%  val=97.7%\n",
            "\n",
            ">>> Best config: BS=64, LR=2.0e-04, WD=0.1 → val=98.0%\n",
            "\n",
            ">>> Running linear probe on frozen backbone\n",
            "  Probe Ep1/20: train=94.6%\n",
            "  Probe Ep2/20: train=98.9%\n",
            "  Probe Ep3/20: train=98.9%\n",
            "  Probe Ep4/20: train=98.9%\n",
            "  Probe Ep5/20: train=98.9%\n",
            "  Probe Ep6/20: train=98.9%\n",
            "  Probe Ep7/20: train=98.9%\n",
            "  Probe Ep8/20: train=98.9%\n",
            "  Probe Ep9/20: train=98.9%\n",
            "  Probe Ep10/20: train=99.0%\n",
            "  Probe Ep11/20: train=99.1%\n",
            "  Probe Ep12/20: train=99.0%\n",
            "  Probe Ep13/20: train=98.9%\n",
            "  Probe Ep14/20: train=98.8%\n",
            "  Probe Ep15/20: train=99.0%\n",
            "  Probe Ep16/20: train=98.9%\n",
            "  Probe Ep17/20: train=99.0%\n",
            "  Probe Ep18/20: train=98.8%\n",
            "  Probe Ep19/20: train=99.0%\n",
            "  Probe Ep20/20: train=99.0%\n",
            "→ Probe test acc: 99.1%\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B (train head on train+val):\n",
        "merged = torch.utils.data.ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "merged_dl = DataLoader(merged, bs, shuffle=True)\n",
        "probe_acc = linear_probe(best_model, merged_dl, te_dl, lr, wd)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:23:12.750347Z",
          "iopub.execute_input": "2025-04-22T13:23:12.750684Z",
          "iopub.status.idle": "2025-04-22T13:25:30.744259Z",
          "shell.execute_reply.started": "2025-04-22T13:23:12.750660Z",
          "shell.execute_reply": "2025-04-22T13:25:30.743546Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiAuFSdXsWrh",
        "outputId": "7e2d8dcf-9c75-4525-b644-2e09c7ec6aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Running linear probe on frozen backbone\n",
            "  Probe Ep1/20: train=95.4%\n",
            "  Probe Ep2/20: train=98.9%\n",
            "  Probe Ep3/20: train=98.9%\n",
            "  Probe Ep4/20: train=98.9%\n",
            "  Probe Ep5/20: train=98.9%\n",
            "  Probe Ep6/20: train=98.9%\n",
            "  Probe Ep7/20: train=98.9%\n",
            "  Probe Ep8/20: train=99.0%\n",
            "  Probe Ep9/20: train=98.9%\n",
            "  Probe Ep10/20: train=98.9%\n",
            "  Probe Ep11/20: train=98.9%\n",
            "  Probe Ep12/20: train=99.0%\n",
            "  Probe Ep13/20: train=98.9%\n",
            "  Probe Ep14/20: train=98.9%\n",
            "  Probe Ep15/20: train=99.0%\n",
            "  Probe Ep16/20: train=98.9%\n",
            "  Probe Ep17/20: train=99.0%\n",
            "  Probe Ep18/20: train=99.0%\n",
            "  Probe Ep19/20: train=98.9%\n",
            "  Probe Ep20/20: train=99.0%\n",
            "→ Probe test acc: 99.1%\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear probing with scikit learn"
      ],
      "metadata": {
        "id": "sOeyjqi5sWrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    # remove last classifier layer\n",
        "    backbone = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    backbone.to(device)\n",
        "    all_feats, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(loader, desc=\"Extracting\"):\n",
        "            xb = xb.to(device)\n",
        "            feats = backbone(xb)           # shape: (B, C, 1, 1)\n",
        "            feats = feats.view(feats.size(0), -1)  # (B, C)\n",
        "            all_feats.append(feats.cpu().numpy())\n",
        "            all_labels.append(yb.numpy())\n",
        "    return np.vstack(all_feats), np.concatenate(all_labels)\n",
        "\n",
        "# 1) Extract embeddings from frozen best_model\n",
        "X_train, y_train = extract_embeddings(best_model, tr_dl, DEVICE)\n",
        "X_test,  y_test  = extract_embeddings(best_model, te_dl, DEVICE)\n",
        "\n",
        "# 2) Fit a scikit‑learn “linear probe” (logistic regression)\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.preprocessing   import StandardScaler\n",
        "from sklearn.metrics         import accuracy_score, classification_report\n",
        "\n",
        "# scale features\n",
        "scaler  = StandardScaler().fit(X_train)\n",
        "X_tr_s  = scaler.transform(X_train)\n",
        "X_te_s  = scaler.transform(X_test)\n",
        "\n",
        "# C ≃ 1/weight_decay — try a small grid\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='saga',\n",
        "    multi_class='multinomial',\n",
        "    max_iter=200\n",
        ").fit(X_tr_s, y_train)\n",
        "\n",
        "# 3) Evaluate\n",
        "preds = clf.predict(X_te_s)\n",
        "acc   = accuracy_score(y_test, preds)\n",
        "print(f\"sklearn probe test accuracy: {acc*100:.2f}%\")\n",
        "print(classification_report(y_test, preds, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:33:59.313768Z",
          "iopub.execute_input": "2025-04-22T13:33:59.314333Z",
          "iopub.status.idle": "2025-04-22T13:36:03.813977Z",
          "shell.execute_reply.started": "2025-04-22T13:33:59.314310Z",
          "shell.execute_reply": "2025-04-22T13:36:03.813245Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_hOxDJqsWrq",
        "outputId": "bf710dfd-5780-4dce-cc41-c15d1c0e2e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting: 100%|██████████| 254/254 [00:11<00:00, 21.31it/s]\n",
            "Extracting: 100%|██████████| 85/85 [00:03<00:00, 22.55it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn probe test accuracy: 99.07%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9913    0.9879    0.9896       579\n",
            "           1     0.9965    0.9914    0.9939       580\n",
            "           2     0.9828    0.9828    0.9828       581\n",
            "           3     0.9899    0.9820    0.9859       500\n",
            "           4     0.9936    0.9979    0.9957       469\n",
            "           5     0.9861    0.9884    0.9872       430\n",
            "           6     0.9808    0.9884    0.9846       517\n",
            "           7     0.9968    0.9968    0.9968       617\n",
            "           8     0.9865    0.9942    0.9903       513\n",
            "           9     1.0000    0.9967    0.9984       614\n",
            "\n",
            "    accuracy                         0.9907      5400\n",
            "   macro avg     0.9904    0.9906    0.9905      5400\n",
            "weighted avg     0.9908    0.9907    0.9907      5400\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 8
    }
  ]
}