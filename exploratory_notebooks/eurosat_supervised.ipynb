{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torchvision.models import resnet18\nfrom itertools import product\nimport numpy as np\nimport random\nimport copy\nimport os, ssl, urllib.request, zipfile\n\n# ─── CONFIG ─────────────────────────────────────────────────────────────────────\nLOCAL_OR_COLAB = \"COLAB\"\nSEED           = 42\nNUM_EPOCHS     = 10\nDEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# split fractions\nTRAIN_FRAC = 0.6\nVAL_FRAC   = 0.2\nTEST_FRAC  = 0.2\n\n# hyperparameter grid\nBATCH_SIZES = [32]\nGRID        = [\n    (2e-4,    0.1  ),  # SimCLR\n    (1.875e-4,0.5  ),  # SatMIP\n    (3.75e-4, 0.5  ),  # SatMIPS\n]\n\n# ─── DATASET DOWNLOAD ────────────────────────────────────────────────────────────\nif LOCAL_OR_COLAB == \"LOCAL\":\n    DATA_DIR = \"/home/juliana/internship_LINUX/datasets/EuroSAT_RGB\"\nelse:\n    data_root = \"/content/EuroSAT_RGB\"\n    zip_path  = \"/content/EuroSAT.zip\"\n    if not os.path.exists(data_root):\n        ssl._create_default_https_context = ssl._create_unverified_context\n        urllib.request.urlretrieve(\n            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n        )\n        with zipfile.ZipFile(zip_path, \"r\") as z:\n            z.extractall(\"/content\")\n        os.rename(\"/content/2750\", data_root)\n    DATA_DIR = data_root\n\n# ─── HELPERS ─────────────────────────────────────────────────────────────────────\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark     = False\n\ndef get_data_loaders(data_dir, batch_size):\n    tf = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485,0.456,0.406],\n            std =[0.229,0.224,0.225]\n        )\n    ])\n    ds = datasets.ImageFolder(root=data_dir, transform=tf)\n    n   = len(ds)\n    n_train = int(TRAIN_FRAC * n)\n    n_val   = int(VAL_FRAC   * n)\n    n_test  = n - n_train - n_val\n    train_ds, val_ds, test_ds = random_split(ds, [n_train, n_val, n_test])\n    return (\n        DataLoader(train_ds, batch_size, shuffle=True),\n        DataLoader(val_ds,   batch_size, shuffle=False),\n        DataLoader(test_ds,  batch_size, shuffle=False),\n        len(ds.classes)\n    )\n\ndef build_model(n_cls, pretrained=False):\n    m = resnet18(weights=None if not pretrained else \"DEFAULT\")\n    m.fc = nn.Linear(m.fc.in_features, n_cls)\n    return m.to(DEVICE)\n\ndef train_one_epoch(model, loader, opt, crit, sched=None):\n    model.train()\n    tot_loss, corr, tot = 0.0, 0, 0\n    for xb, yb in loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        opt.zero_grad()\n        logits = model(xb)\n        loss   = crit(logits, yb)\n        loss.backward()\n        opt.step()\n        if sched: sched.step()\n        tot_loss += loss.item()\n        preds    = logits.argmax(dim=1)\n        corr    += (preds==yb).sum().item()\n        tot     += yb.size(0)\n    return tot_loss/len(loader), 100*corr/tot\n\ndef evaluate(model, loader):\n    model.eval()\n    corr, tot = 0,0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            preds = model(xb).argmax(dim=1)\n            corr += (preds==yb).sum().item()\n            tot  += yb.size(0)\n    return 100 * corr / tot\n\n# ─── PHASE 1: GRID SEARCH ────────────────────────────────────────────────────────\ndef hyperparam_search():\n    best_val = -1.0\n    best_cfg = None\n    best_model = None\n    # loop over all combos in one go\n    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd}\")\n        set_seed(SEED)\n        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n        model = build_model(n_cls, pretrained=True)\n\n        # optimizer + paper schedule\n        opt = optim.AdamW(model.parameters(),\n                          lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n        total_steps  = NUM_EPOCHS * len(tr_dl)\n        warmup_steps = len(tr_dl)\n        sched = SequentialLR(\n            opt,\n            schedulers=[\n                LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n                CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n            ],\n            milestones=[warmup_steps]\n        )\n        crit = nn.CrossEntropyLoss()\n\n        # train & validate\n        for ep in range(NUM_EPOCHS):\n            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n            val_acc          = evaluate(model, val_dl)\n            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train={tr_acc:.1f}%  val={val_acc:.1f}%\")\n\n        # pick best\n        if val_acc > best_val:\n            best_val = val_acc\n            best_cfg = (bs, lr, wd)\n            best_model = copy.deepcopy(model)   # store the weights\n\n    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]} \"\n          f\"→ val={best_val:.1f}%\")\n    return best_cfg, best_model\n\n# ─── PHASE 2: LINEAR PROBE ───────────────────────────────────────────────────────\ndef linear_probe(frozen_model, train_dl, test_dl, lr, wd):\n    # freeze backbone\n    for p in frozen_model.parameters():\n        p.requires_grad = False\n    # new head\n    n_in = frozen_model.fc.in_features\n    n_out = frozen_model.fc.out_features\n    frozen_model.fc = nn.Linear(n_in, n_out).to(DEVICE)\n\n    opt = optim.AdamW(frozen_model.fc.parameters(),\n                      lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n    crit = nn.CrossEntropyLoss()\n\n    print(\"\\n>>> Running linear probe on frozen backbone\")\n    for ep in range(NUM_EPOCHS):\n        loss, acc = train_one_epoch(frozen_model, train_dl, opt, crit, sched=None)\n        print(f\"  Probe Ep{ep+1}/{NUM_EPOCHS}: train={acc:.1f}%\")\n    test_acc = evaluate(frozen_model, test_dl)\n    print(f\"→ Probe test acc: {test_acc:.1f}%\")\n    return test_acc\n\n# ─── MAIN ───────────────────────────────────────────────────────────────────────\nif __name__ == \"__main__\":\n    best_cfg, best_model = hyperparam_search()\n    # rebuild loaders once more so we have the same splits\n    bs, lr, wd = best_cfg\n    tr_dl, val_dl, te_dl, _ = get_data_loaders(DATA_DIR, bs)\n\n    # Option A: probe on just the original training split\n    probe_acc = linear_probe(best_model, tr_dl, te_dl, lr, wd)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AD0e5irzQc0","outputId":"443d9973-313e-4506-8395-8af4a9a1dfed","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:09:05.901368Z","iopub.execute_input":"2025-04-22T13:09:05.901785Z","iopub.status.idle":"2025-04-22T13:20:26.682044Z","shell.execute_reply.started":"2025-04-22T13:09:05.901745Z","shell.execute_reply":"2025-04-22T13:20:26.681120Z"}},"outputs":[{"name":"stdout","text":"\n>>> Testing BS=32, LR=2.0e-04, WD=0.1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"  Ep1/10: train=79.1%  val=92.3%\n  Ep2/10: train=92.7%  val=95.2%\n  Ep3/10: train=95.2%  val=96.4%\n  Ep4/10: train=97.1%  val=96.1%\n  Ep5/10: train=98.2%  val=96.4%\n  Ep6/10: train=99.0%  val=97.3%\n  Ep7/10: train=99.4%  val=97.5%\n  Ep8/10: train=99.8%  val=97.9%\n  Ep9/10: train=99.9%  val=97.9%\n  Ep10/10: train=99.9%  val=98.0%\n\n>>> Testing BS=32, LR=1.9e-04, WD=0.5\n  Ep1/10: train=78.8%  val=92.9%\n  Ep2/10: train=92.7%  val=94.8%\n  Ep3/10: train=95.5%  val=95.7%\n  Ep4/10: train=97.1%  val=96.5%\n  Ep5/10: train=98.0%  val=96.8%\n  Ep6/10: train=98.8%  val=96.7%\n  Ep7/10: train=99.3%  val=97.2%\n  Ep8/10: train=99.7%  val=97.7%\n  Ep9/10: train=99.9%  val=97.7%\n  Ep10/10: train=99.9%  val=97.9%\n\n>>> Testing BS=32, LR=3.8e-04, WD=0.5\n  Ep1/10: train=81.1%  val=91.0%\n  Ep2/10: train=90.8%  val=94.8%\n  Ep3/10: train=93.7%  val=92.5%\n  Ep4/10: train=95.8%  val=95.3%\n  Ep5/10: train=96.8%  val=95.2%\n  Ep6/10: train=98.2%  val=96.2%\n  Ep7/10: train=99.0%  val=97.1%\n  Ep8/10: train=99.7%  val=97.4%\n  Ep9/10: train=99.8%  val=97.6%\n  Ep10/10: train=99.9%  val=97.6%\n\n>>> Best config: BS=32, LR=2.0e-04, WD=0.1 → val=98.0%\n\n>>> Running linear probe on frozen backbone\n  Probe Ep1/10: train=95.8%\n  Probe Ep2/10: train=98.5%\n  Probe Ep3/10: train=98.7%\n  Probe Ep4/10: train=98.6%\n  Probe Ep5/10: train=98.6%\n  Probe Ep6/10: train=98.7%\n  Probe Ep7/10: train=98.8%\n  Probe Ep8/10: train=98.7%\n  Probe Ep9/10: train=98.6%\n  Probe Ep10/10: train=98.8%\n→ Probe test acc: 99.2%\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Option B (train head on train+val):\nmerged = torch.utils.data.ConcatDataset([tr_dl.dataset, val_dl.dataset])\nmerged_dl = DataLoader(merged, bs, shuffle=True)\nprobe_acc = linear_probe(best_model, merged_dl, te_dl, lr, wd)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:23:12.750347Z","iopub.execute_input":"2025-04-22T13:23:12.750684Z","iopub.status.idle":"2025-04-22T13:25:30.744259Z","shell.execute_reply.started":"2025-04-22T13:23:12.750660Z","shell.execute_reply":"2025-04-22T13:25:30.743546Z"}},"outputs":[{"name":"stdout","text":"\n>>> Running linear probe on frozen backbone\n  Probe Ep1/10: train=96.7%\n  Probe Ep2/10: train=98.8%\n  Probe Ep3/10: train=98.7%\n  Probe Ep4/10: train=98.8%\n  Probe Ep5/10: train=98.8%\n  Probe Ep6/10: train=98.8%\n  Probe Ep7/10: train=98.9%\n  Probe Ep8/10: train=98.8%\n  Probe Ep9/10: train=98.8%\n  Probe Ep10/10: train=98.8%\n→ Probe test acc: 99.2%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Linear probing with scikit learn","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\n\ndef extract_embeddings(model, loader, device):\n    model.eval()\n    # remove last classifier layer\n    backbone = torch.nn.Sequential(*list(model.children())[:-1])\n    backbone.to(device)\n    all_feats, all_labels = [], []\n    with torch.no_grad():\n        for xb, yb in tqdm(loader, desc=\"Extracting\"):\n            xb = xb.to(device)\n            feats = backbone(xb)           # shape: (B, C, 1, 1)\n            feats = feats.view(feats.size(0), -1)  # (B, C)\n            all_feats.append(feats.cpu().numpy())\n            all_labels.append(yb.numpy())\n    return np.vstack(all_feats), np.concatenate(all_labels)\n\n# 1) Extract embeddings from frozen best_model\nX_train, y_train = extract_embeddings(best_model, tr_dl, DEVICE)\nX_test,  y_test  = extract_embeddings(best_model, te_dl, DEVICE)\n\n# 2) Fit a scikit‑learn “linear probe” (logistic regression)\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.preprocessing   import StandardScaler\nfrom sklearn.metrics         import accuracy_score, classification_report\n\n# scale features\nscaler  = StandardScaler().fit(X_train)\nX_tr_s  = scaler.transform(X_train)\nX_te_s  = scaler.transform(X_test)\n\n# C ≃ 1/weight_decay — try a small grid\nclf = LogisticRegression(\n    penalty='l2',\n    C=1.0,\n    solver='saga',\n    multi_class='multinomial',\n    max_iter=200\n).fit(X_tr_s, y_train)\n\n# 3) Evaluate\npreds = clf.predict(X_te_s)\nacc   = accuracy_score(y_test, preds)\nprint(f\"sklearn probe test accuracy: {acc*100:.2f}%\")\nprint(classification_report(y_test, preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:33:59.313768Z","iopub.execute_input":"2025-04-22T13:33:59.314333Z","iopub.status.idle":"2025-04-22T13:36:03.813977Z","shell.execute_reply.started":"2025-04-22T13:33:59.314310Z","shell.execute_reply":"2025-04-22T13:36:03.813245Z"}},"outputs":[{"name":"stderr","text":"Extracting: 100%|██████████| 507/507 [00:10<00:00, 50.00it/s]\nExtracting: 100%|██████████| 169/169 [00:03<00:00, 51.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"sklearn probe test accuracy: 99.28%\n              precision    recall  f1-score   support\n\n           0     0.9900    0.9967    0.9933       598\n           1     0.9950    0.9934    0.9942       602\n           2     0.9848    0.9864    0.9856       590\n           3     0.9897    0.9938    0.9917       482\n           4     0.9980    0.9960    0.9970       506\n           5     0.9868    0.9894    0.9881       379\n           6     0.9900    0.9841    0.9871       504\n           7     0.9951    0.9983    0.9967       605\n           8     0.9980    0.9884    0.9932       516\n           9     0.9984    0.9984    0.9984       618\n\n    accuracy                         0.9928      5400\n   macro avg     0.9926    0.9925    0.9925      5400\nweighted avg     0.9928    0.9928    0.9928      5400\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9}]}