{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, urllib.request, zipfile\n",
        "\n",
        "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
        "LOCAL_OR_COLAB = \"COLAB\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 20\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = 0.6\n",
        "VAL_FRAC   = 0.2\n",
        "TEST_FRAC  = 0.2\n",
        "\n",
        "# hyperparameter grid\n",
        "BATCH_SIZES = [32, 64]\n",
        "GRID        = [\n",
        "    (2e-4,    0.1  ),  # SimCLR\n",
        "    (1.875e-4,0.5  ),  # SatMIP\n",
        "    (3.75e-4, 0.5  ),  # SatMIPS\n",
        "]\n",
        "\n",
        "# ─── DATASET DOWNLOAD ────────────────────────────────────────────────────────────\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/home/juliana/internship_LINUX/datasets/EuroSAT_RGB\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "# ─── HELPERS ─────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485,0.456,0.406],\n",
        "            std =[0.229,0.224,0.225]\n",
        "        )\n",
        "    ])\n",
        "    ds = datasets.ImageFolder(root=data_dir, transform=tf)\n",
        "    n   = len(ds)\n",
        "    n_train = int(TRAIN_FRAC * n)\n",
        "    n_val   = int(VAL_FRAC   * n)\n",
        "    n_test  = n - n_train - n_val\n",
        "    train_ds, val_ds, test_ds = random_split(ds, [n_train, n_val, n_test])\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size, shuffle=True),\n",
        "        DataLoader(val_ds,   batch_size, shuffle=False),\n",
        "        DataLoader(test_ds,  batch_size, shuffle=False),\n",
        "        len(ds.classes)\n",
        "    )\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet18(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "    model.train()\n",
        "    tot_loss, corr, tot = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss   = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if sched: sched.step()\n",
        "        tot_loss += loss.item()\n",
        "        preds    = logits.argmax(dim=1)\n",
        "        corr    += (preds==yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "    return tot_loss/len(loader), 100*corr/tot\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    corr, tot = 0,0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            corr += (preds==yb).sum().item()\n",
        "            tot  += yb.size(0)\n",
        "    return 100 * corr / tot\n",
        "\n",
        "# ─── PHASE 1: GRID SEARCH ────────────────────────────────────────────────────────\n",
        "def hyperparam_search(pretrained = True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "    # loop over all combos in one go\n",
        "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd}\")\n",
        "        set_seed(SEED)\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "        model = build_model(n_cls, pretrained = pretrained)\n",
        "\n",
        "        # optimizer + paper schedule\n",
        "        opt = optim.AdamW(model.parameters(),\n",
        "                          lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "        total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "        warmup_steps = len(tr_dl)\n",
        "        sched = SequentialLR(\n",
        "            opt,\n",
        "            schedulers=[\n",
        "                LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "                CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "            ],\n",
        "            milestones=[warmup_steps]\n",
        "        )\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # train & validate\n",
        "        for ep in range(NUM_EPOCHS):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "            val_acc          = evaluate(model, val_dl)\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train={tr_acc:.1f}%  val={val_acc:.1f}%\")\n",
        "\n",
        "        # pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_cfg = (bs, lr, wd)\n",
        "            best_model = copy.deepcopy(model)   # store the weights\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]} \"\n",
        "          f\"→ val={best_val:.1f}%\")\n",
        "    return best_cfg, best_model\n",
        "\n",
        "# ─── PHASE 2: LINEAR PROBE ───────────────────────────────────────────────────────\n",
        "def linear_probe(frozen_model, train_dl, test_dl, lr, wd):\n",
        "    # freeze backbone\n",
        "    for p in frozen_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # new head\n",
        "    n_in = frozen_model.fc.in_features\n",
        "    n_out = frozen_model.fc.out_features\n",
        "    frozen_model.fc = nn.Linear(n_in, n_out).to(DEVICE)\n",
        "\n",
        "    opt = optim.AdamW(frozen_model.fc.parameters(),\n",
        "                      lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n>>> Running linear probe on frozen backbone\")\n",
        "    for ep in range(NUM_EPOCHS):\n",
        "        loss, acc = train_one_epoch(frozen_model, train_dl, opt, crit, sched=None)\n",
        "        print(f\"  Probe Ep{ep+1}/{NUM_EPOCHS}: train={acc:.1f}%\")\n",
        "    test_acc = evaluate(frozen_model, test_dl)\n",
        "    print(f\"→ Probe test acc: {test_acc:.1f}%\")\n",
        "    return test_acc\n",
        "\n",
        "# ─── MAIN ───────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    best_cfg, best_model = hyperparam_search(pretrained = False)\n",
        "    # rebuild loaders once more so we have the same splits\n",
        "    bs, lr, wd = best_cfg\n",
        "    tr_dl, val_dl, te_dl, _ = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "    # Option A: probe on just the original training split\n",
        "    probe_acc = linear_probe(best_model, tr_dl, te_dl, lr, wd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AD0e5irzQc0",
        "outputId": "9c1c4685-c888-4616-fbe6-e02fcd64b708",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:09:05.901368Z",
          "iopub.execute_input": "2025-04-22T13:09:05.901785Z",
          "iopub.status.idle": "2025-04-22T13:20:26.682044Z",
          "shell.execute_reply.started": "2025-04-22T13:09:05.901745Z",
          "shell.execute_reply": "2025-04-22T13:20:26.681120Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Testing BS=32, LR=2.0e-04, WD=0.1\n",
            "  Ep1/20: train=61.4%  val=68.3%\n",
            "  Ep2/20: train=74.3%  val=76.4%\n",
            "  Ep3/20: train=80.4%  val=76.4%\n",
            "  Ep4/20: train=85.9%  val=76.6%\n",
            "  Ep5/20: train=89.1%  val=79.1%\n",
            "  Ep6/20: train=91.8%  val=82.6%\n",
            "  Ep7/20: train=93.9%  val=81.3%\n",
            "  Ep8/20: train=95.3%  val=83.9%\n",
            "  Ep9/20: train=96.0%  val=81.2%\n",
            "  Ep10/20: train=97.0%  val=83.5%\n",
            "  Ep11/20: train=97.8%  val=86.0%\n",
            "  Ep12/20: train=98.4%  val=86.3%\n",
            "  Ep13/20: train=98.8%  val=87.1%\n",
            "  Ep14/20: train=99.2%  val=85.6%\n",
            "  Ep15/20: train=99.4%  val=87.8%\n",
            "  Ep16/20: train=99.7%  val=84.6%\n",
            "  Ep17/20: train=99.8%  val=87.4%\n",
            "  Ep18/20: train=99.9%  val=88.2%\n",
            "  Ep19/20: train=99.9%  val=88.3%\n",
            "  Ep20/20: train=99.9%  val=88.5%\n",
            "\n",
            ">>> Testing BS=32, LR=1.9e-04, WD=0.5\n",
            "  Ep1/20: train=60.7%  val=67.4%\n",
            "  Ep2/20: train=73.9%  val=79.4%\n",
            "  Ep3/20: train=80.3%  val=76.3%\n",
            "  Ep4/20: train=85.2%  val=77.5%\n",
            "  Ep5/20: train=88.2%  val=83.2%\n",
            "  Ep6/20: train=90.9%  val=78.5%\n",
            "  Ep7/20: train=92.7%  val=80.7%\n",
            "  Ep8/20: train=94.3%  val=85.6%\n",
            "  Ep9/20: train=95.6%  val=83.3%\n",
            "  Ep10/20: train=96.4%  val=86.8%\n",
            "  Ep11/20: train=97.3%  val=86.5%\n",
            "  Ep12/20: train=98.0%  val=87.8%\n",
            "  Ep13/20: train=98.6%  val=87.0%\n",
            "  Ep14/20: train=98.9%  val=87.1%\n",
            "  Ep15/20: train=99.4%  val=88.1%\n",
            "  Ep16/20: train=99.6%  val=84.5%\n",
            "  Ep17/20: train=99.8%  val=86.7%\n",
            "  Ep18/20: train=99.8%  val=88.7%\n",
            "  Ep19/20: train=99.9%  val=88.9%\n",
            "  Ep20/20: train=99.9%  val=88.9%\n",
            "\n",
            ">>> Testing BS=32, LR=3.8e-04, WD=0.5\n",
            "  Ep1/20: train=61.3%  val=67.4%\n",
            "  Ep2/20: train=72.5%  val=71.8%\n",
            "  Ep3/20: train=78.0%  val=80.5%\n",
            "  Ep4/20: train=83.2%  val=75.0%\n",
            "  Ep5/20: train=85.3%  val=79.4%\n",
            "  Ep6/20: train=88.2%  val=87.0%\n",
            "  Ep7/20: train=90.2%  val=85.1%\n",
            "  Ep8/20: train=92.3%  val=84.3%\n",
            "  Ep9/20: train=93.9%  val=86.9%\n",
            "  Ep10/20: train=95.0%  val=90.1%\n",
            "  Ep11/20: train=96.2%  val=82.9%\n",
            "  Ep12/20: train=97.5%  val=89.2%\n",
            "  Ep13/20: train=98.0%  val=89.9%\n",
            "  Ep14/20: train=98.8%  val=89.2%\n",
            "  Ep15/20: train=99.2%  val=90.9%\n",
            "  Ep16/20: train=99.6%  val=90.9%\n",
            "  Ep17/20: train=99.8%  val=91.5%\n",
            "  Ep18/20: train=99.9%  val=92.0%\n",
            "  Ep19/20: train=99.9%  val=92.1%\n",
            "  Ep20/20: train=99.9%  val=92.3%\n",
            "\n",
            ">>> Testing BS=64, LR=2.0e-04, WD=0.1\n",
            "  Ep1/20: train=61.4%  val=68.1%\n",
            "  Ep2/20: train=76.8%  val=74.8%\n",
            "  Ep3/20: train=82.9%  val=78.9%\n",
            "  Ep4/20: train=87.8%  val=80.9%\n",
            "  Ep5/20: train=90.9%  val=74.8%\n",
            "  Ep6/20: train=93.2%  val=82.3%\n",
            "  Ep7/20: train=94.8%  val=82.6%\n",
            "  Ep8/20: train=96.2%  val=81.3%\n",
            "  Ep9/20: train=96.6%  val=83.6%\n",
            "  Ep10/20: train=97.5%  val=81.8%\n",
            "  Ep11/20: train=98.1%  val=79.4%\n",
            "  Ep12/20: train=98.7%  val=83.9%\n",
            "  Ep13/20: train=99.1%  val=85.4%\n",
            "  Ep14/20: train=99.3%  val=81.5%\n",
            "  Ep15/20: train=99.6%  val=85.1%\n",
            "  Ep16/20: train=99.8%  val=86.3%\n",
            "  Ep17/20: train=99.9%  val=85.6%\n",
            "  Ep18/20: train=99.9%  val=86.1%\n",
            "  Ep19/20: train=99.9%  val=86.4%\n",
            "  Ep20/20: train=99.9%  val=86.0%\n",
            "\n",
            ">>> Testing BS=64, LR=1.9e-04, WD=0.5\n",
            "  Ep1/20: train=61.2%  val=70.8%\n",
            "  Ep2/20: train=77.1%  val=75.6%\n",
            "  Ep3/20: train=83.1%  val=71.9%\n",
            "  Ep4/20: train=87.5%  val=80.4%\n",
            "  Ep5/20: train=91.3%  val=78.8%\n",
            "  Ep6/20: train=93.1%  val=76.1%\n",
            "  Ep7/20: train=94.7%  val=78.5%\n",
            "  Ep8/20: train=95.7%  val=82.4%\n",
            "  Ep9/20: train=96.8%  val=78.7%\n",
            "  Ep10/20: train=97.4%  val=81.0%\n",
            "  Ep11/20: train=98.0%  val=74.2%\n",
            "  Ep12/20: train=98.5%  val=83.7%\n",
            "  Ep13/20: train=99.0%  val=85.0%\n",
            "  Ep14/20: train=99.3%  val=84.7%\n",
            "  Ep15/20: train=99.6%  val=84.5%\n",
            "  Ep16/20: train=99.7%  val=84.8%\n",
            "  Ep17/20: train=99.8%  val=85.6%\n",
            "  Ep18/20: train=99.9%  val=86.0%\n",
            "  Ep19/20: train=100.0%  val=85.9%\n",
            "  Ep20/20: train=99.9%  val=85.8%\n",
            "\n",
            ">>> Testing BS=64, LR=3.8e-04, WD=0.5\n",
            "  Ep1/20: train=63.3%  val=68.6%\n",
            "  Ep2/20: train=75.5%  val=72.5%\n",
            "  Ep3/20: train=80.3%  val=61.8%\n",
            "  Ep4/20: train=84.3%  val=71.8%\n",
            "  Ep5/20: train=86.9%  val=69.1%\n",
            "  Ep6/20: train=89.2%  val=84.0%\n",
            "  Ep7/20: train=92.4%  val=78.8%\n",
            "  Ep8/20: train=93.8%  val=81.2%\n",
            "  Ep9/20: train=95.4%  val=78.0%\n",
            "  Ep10/20: train=96.2%  val=88.0%\n",
            "  Ep11/20: train=97.8%  val=88.1%\n",
            "  Ep12/20: train=97.9%  val=78.6%\n",
            "  Ep13/20: train=98.6%  val=84.4%\n",
            "  Ep14/20: train=99.1%  val=89.7%\n",
            "  Ep15/20: train=99.4%  val=89.6%\n",
            "  Ep16/20: train=99.7%  val=90.5%\n",
            "  Ep17/20: train=99.8%  val=90.3%\n",
            "  Ep18/20: train=99.8%  val=91.2%\n",
            "  Ep19/20: train=99.9%  val=90.8%\n",
            "  Ep20/20: train=99.9%  val=90.8%\n",
            "\n",
            ">>> Best config: BS=32, LR=3.8e-04, WD=0.5 → val=92.3%\n",
            "\n",
            ">>> Running linear probe on frozen backbone\n",
            "  Probe Ep1/20: train=95.0%\n",
            "  Probe Ep2/20: train=96.4%\n",
            "  Probe Ep3/20: train=96.5%\n",
            "  Probe Ep4/20: train=96.4%\n",
            "  Probe Ep5/20: train=96.3%\n",
            "  Probe Ep6/20: train=96.3%\n",
            "  Probe Ep7/20: train=96.2%\n",
            "  Probe Ep8/20: train=96.5%\n",
            "  Probe Ep9/20: train=96.4%\n",
            "  Probe Ep10/20: train=96.5%\n",
            "  Probe Ep11/20: train=96.3%\n",
            "  Probe Ep12/20: train=96.5%\n",
            "  Probe Ep13/20: train=96.3%\n",
            "  Probe Ep14/20: train=96.3%\n",
            "  Probe Ep15/20: train=96.3%\n",
            "  Probe Ep16/20: train=96.4%\n",
            "  Probe Ep17/20: train=96.4%\n",
            "  Probe Ep18/20: train=96.4%\n",
            "  Probe Ep19/20: train=96.5%\n",
            "  Probe Ep20/20: train=96.5%\n",
            "→ Probe test acc: 96.9%\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B (train head on train+val):\n",
        "merged = torch.utils.data.ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "merged_dl = DataLoader(merged, bs, shuffle=True)\n",
        "probe_acc = linear_probe(best_model, merged_dl, te_dl, lr, wd)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:23:12.750347Z",
          "iopub.execute_input": "2025-04-22T13:23:12.750684Z",
          "iopub.status.idle": "2025-04-22T13:25:30.744259Z",
          "shell.execute_reply.started": "2025-04-22T13:23:12.750660Z",
          "shell.execute_reply": "2025-04-22T13:25:30.743546Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiAuFSdXsWrh",
        "outputId": "a3ad3a1f-eed3-4266-b34c-f66a888b3542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Running linear probe on frozen backbone\n",
            "  Probe Ep1/20: train=95.3%\n",
            "  Probe Ep2/20: train=96.2%\n",
            "  Probe Ep3/20: train=96.3%\n",
            "  Probe Ep4/20: train=96.4%\n",
            "  Probe Ep5/20: train=96.4%\n",
            "  Probe Ep6/20: train=96.2%\n",
            "  Probe Ep7/20: train=96.3%\n",
            "  Probe Ep8/20: train=96.3%\n",
            "  Probe Ep9/20: train=96.3%\n",
            "  Probe Ep10/20: train=96.3%\n",
            "  Probe Ep11/20: train=96.4%\n",
            "  Probe Ep12/20: train=96.3%\n",
            "  Probe Ep13/20: train=96.3%\n",
            "  Probe Ep14/20: train=96.4%\n",
            "  Probe Ep15/20: train=96.4%\n",
            "  Probe Ep16/20: train=96.3%\n",
            "  Probe Ep17/20: train=96.3%\n",
            "  Probe Ep18/20: train=96.4%\n",
            "  Probe Ep19/20: train=96.3%\n",
            "  Probe Ep20/20: train=96.4%\n",
            "→ Probe test acc: 97.0%\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear probing with scikit learn"
      ],
      "metadata": {
        "id": "sOeyjqi5sWrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    # remove last classifier layer\n",
        "    backbone = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    backbone.to(device)\n",
        "    all_feats, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(loader, desc=\"Extracting\"):\n",
        "            xb = xb.to(device)\n",
        "            feats = backbone(xb)           # shape: (B, C, 1, 1)\n",
        "            feats = feats.view(feats.size(0), -1)  # (B, C)\n",
        "            all_feats.append(feats.cpu().numpy())\n",
        "            all_labels.append(yb.numpy())\n",
        "    return np.vstack(all_feats), np.concatenate(all_labels)\n",
        "\n",
        "# 1) Extract embeddings from frozen best_model\n",
        "X_train, y_train = extract_embeddings(best_model, tr_dl, DEVICE)\n",
        "X_test,  y_test  = extract_embeddings(best_model, te_dl, DEVICE)\n",
        "\n",
        "# 2) Fit a scikit‑learn “linear probe” (logistic regression)\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.preprocessing   import StandardScaler\n",
        "from sklearn.metrics         import accuracy_score, classification_report\n",
        "\n",
        "# scale features\n",
        "scaler  = StandardScaler().fit(X_train)\n",
        "X_tr_s  = scaler.transform(X_train)\n",
        "X_te_s  = scaler.transform(X_test)\n",
        "\n",
        "# C ≃ 1/weight_decay — try a small grid\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='saga',\n",
        "    multi_class='multinomial',\n",
        "    max_iter=200\n",
        ").fit(X_tr_s, y_train)\n",
        "\n",
        "# 3) Evaluate\n",
        "preds = clf.predict(X_te_s)\n",
        "acc   = accuracy_score(y_test, preds)\n",
        "print(f\"sklearn probe test accuracy: {acc*100:.2f}%\")\n",
        "print(classification_report(y_test, preds, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:33:59.313768Z",
          "iopub.execute_input": "2025-04-22T13:33:59.314333Z",
          "iopub.status.idle": "2025-04-22T13:36:03.813977Z",
          "shell.execute_reply.started": "2025-04-22T13:33:59.314310Z",
          "shell.execute_reply": "2025-04-22T13:36:03.813245Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_hOxDJqsWrq",
        "outputId": "2bf78c3d-4067-4a4f-fd0e-a2ab4d4bfd59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting: 100%|██████████| 507/507 [00:12<00:00, 39.64it/s]\n",
            "Extracting: 100%|██████████| 169/169 [00:03<00:00, 42.36it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn probe test accuracy: 96.83%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9722    0.9655    0.9688       579\n",
            "           1     0.9914    0.9897    0.9905       580\n",
            "           2     0.9527    0.9363    0.9444       581\n",
            "           3     0.9572    0.9400    0.9485       500\n",
            "           4     0.9579    0.9701    0.9640       469\n",
            "           5     0.9523    0.9744    0.9632       430\n",
            "           6     0.9365    0.9420    0.9392       517\n",
            "           7     0.9871    0.9919    0.9895       617\n",
            "           8     0.9652    0.9727    0.9689       513\n",
            "           9     0.9967    0.9935    0.9951       614\n",
            "\n",
            "    accuracy                         0.9683      5400\n",
            "   macro avg     0.9669    0.9676    0.9672      5400\n",
            "weighted avg     0.9684    0.9683    0.9683      5400\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 4
    }
  ]
}