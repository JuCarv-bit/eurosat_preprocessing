wandb: Currently logged in as: analiju (analiju-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Conda version: 25.5.1
Python version: 3.10.16
PyTorch version: 2.5.1
CUDA available: True
CUDA device count: 2
Torchvision version: 0.20.1
Successfully set to use GPU: 0 (NVIDIA RTX A6000)
Final DEVICE variable is set to: cuda:0
Current PyTorch default device: 0
Current PyTorch default device (after set_device): 0
Dummy tensor is on device: cuda:0

=== Starting run with seed 42 ===
Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']
Stratified split sizes: train=21600, val=2700, test=2700
Computed mean: [0.3441457152366638, 0.3800986111164093, 0.40766361355781555]
Computed std:  [0.09299743920564651, 0.06464490294456482, 0.054139167070388794]
Mean and std saved to models/mean_std.txt
Train/Val/Test loaders: 84/11/11 batches

=== Starting run with seed 42 ===
Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']
Stratified split sizes: train=21600, val=2700, test=2700
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250625_145453-oahxte20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/analiju-paris/eurosat-contrastive-scratch
wandb: üöÄ View run at https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/oahxte20
Computed mean: [0.3441457152366638, 0.3800986111164093, 0.40766361355781555]
Computed std:  [0.09299743920564651, 0.06464490294456482, 0.054139167070388794]
Mean and std saved to models/mean_std.txt
Train/Val/Test loaders: 84/11/11 batches
Starting SimCLR training...
Traceback (most recent call last):
  File "/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/eurosat_contrastive-bs256_ep60-lrg-remov-gausblur.py", line 856, in <module>
    train_simclr(
  File "/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/eurosat_contrastive-bs256_ep60-lrg-remov-gausblur.py", line 532, in train_simclr
    _, z2 = model(x2)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/eurosat_contrastive-bs256_ep60-lrg-remov-gausblur.py", line 334, in forward
    feat = self.encoder(x)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 158, in forward
    identity = self.downsample(x)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 5.69 MiB is free. Process 74386 has 44.44 GiB memory in use. Including non-PyTorch memory, this process has 3.07 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 23.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mBS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS60[0m at: [34mhttps://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/oahxte20[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250625_145453-oahxte20/logs[0m
