{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, zipfile, urllib\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "LOCAL_OR_COLAB = \"LOCAL\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 34\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TRAIN_FRAC = 0.8\n",
        "VAL_FRAC   = 0.1\n",
        "TEST_FRAC  = 0.1\n",
        "\n",
        "# hyperparameter grid\n",
        "# BATCH_SIZES = [64, 128, 256]\n",
        "BATCH_SIZES = [128]  # Using a single batch size for simplicity\n",
        "LRS = [1e-4, 3e-4]\n",
        "\n",
        "GRID        = [\n",
        "    (3.75e-4, 0.5  ),\n",
        "]\n",
        "\n",
        "BETAS=(0.9,0.98)\n",
        "EPS = 1e-8\n",
        "\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "NUM_WORKERS = 4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    # This is a placeholder. You need to implement your actual splitting logic here.\n",
        "    # For demonstration, let's create a simple 80/10/10 split.\n",
        "    indices = np.arange(total_count)\n",
        "    np.random.seed(SEED) # for reproducibility\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_split = int(0.8 * total_count)\n",
        "    val_split = int(0.9 * total_count)\n",
        "\n",
        "    train_idx = indices[:train_split]\n",
        "    val_idx = indices[train_split:val_split]\n",
        "    test_idx = indices[val_split:]\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "    # Base transform to compute mean and std (only ToTensor)\n",
        "    base_tf = transforms.ToTensor()\n",
        "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "    labels = np.array(ds_all.targets)\n",
        "    num_classes = len(ds_all.classes)\n",
        "    total_count = len(ds_all)\n",
        "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
        "\n",
        "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
        "\n",
        "    # Dataset for computing mean and std should *not* have the random augmentations\n",
        "    # as these statistics should ideally represent the original data distribution.\n",
        "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
        "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
        "    print(f\"Computed mean: {mean}\")\n",
        "    print(f\"Computed std:  {std}\")\n",
        "\n",
        "\n",
        "    # Let's refine the \"one of 8 random rotations\" part.\n",
        "    # It typically means 0, 90, 180, 270 degree rotations, and for each of these,\n",
        "    # a horizontal flip can also be applied.\n",
        "    # This leads to 4 rotations * 2 (flip/no flip) = 8.\n",
        "    # A more standard way to represent this is:\n",
        "    train_transform_augmented = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomApply([transforms.RandomRotation(angle) for angle in [0, 90, 180, 270]], p=1.0), # Apply one of 0, 90, 180, 270 rotations\n",
        "        transforms.RandomHorizontalFlip(p=0.5), # Randomly apply horizontal flip (50% chance)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Transformations for EVALUATION (validation and test): resize, central crop, ToTensor, Normalize\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.Resize(256), # Resize to 256x256\n",
        "        transforms.CenterCrop(224), # Perform a central crop of 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "    # Create datasets with the respective transformations\n",
        "    train_ds = datasets.ImageFolder(root=data_dir, transform=train_transform_augmented)\n",
        "    val_ds = datasets.ImageFolder(root=data_dir, transform=eval_transform)\n",
        "    test_ds = datasets.ImageFolder(root=data_dir, transform=eval_transform)\n",
        "\n",
        "    # Apply subsets to the transformed datasets\n",
        "    train_ds_subset = Subset(train_ds, train_idx)\n",
        "    val_ds_subset = Subset(val_ds, val_idx)\n",
        "    test_ds_subset = Subset(test_ds, test_idx)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_ds_subset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    val_loader   = DataLoader(val_ds_subset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    test_loader  = DataLoader(test_ds_subset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "    print(f\"Train/Val/Test splits: {len(train_ds_subset)}/{len(val_ds_subset)}/{len(test_ds_subset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# def compute_mean_std(dataset, batch_size):\n",
        "#     loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
        "#     mean = 0.0\n",
        "#     std = 0.0\n",
        "#     n_samples = 0\n",
        "\n",
        "#     for data, _ in loader:\n",
        "#         batch_samples = data.size(0)\n",
        "#         data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "#         mean += data.mean(2).sum(0)\n",
        "#         std += data.std(2).sum(0)\n",
        "#         n_samples += batch_samples\n",
        "\n",
        "#     mean /= n_samples\n",
        "#     std /= n_samples\n",
        "#     return mean.tolist(), std.tolist()\n",
        "\n",
        "# def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "#     base_tf = transforms.ToTensor()\n",
        "#     ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "#     labels = np.array(ds_all.targets)   # numpy array of shape (N,)\n",
        "#     num_classes = len(ds_all.classes)\n",
        "#     total_count = len(ds_all)\n",
        "#     print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
        "\n",
        "#     train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
        "\n",
        "#     train_subset_for_stats = Subset(ds_all, train_idx)\n",
        "#     mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
        "#     print(f\"Computed mean: {mean}\")\n",
        "#     print(f\"Computed std:  {std}\")\n",
        "\n",
        "#     tf_final = transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=mean, std=std)\n",
        "#     ])\n",
        "\n",
        "#     #  full ImageFolder but now with normalization baked in\n",
        "#     ds_all_norm = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
        "\n",
        "#     train_ds = Subset(ds_all_norm, train_idx)\n",
        "#     val_ds   = Subset(ds_all_norm, val_idx)\n",
        "#     test_ds  = Subset(ds_all_norm, test_idx)\n",
        "\n",
        "#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "#     val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "#     test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "#     print(f\"Train/Val/Test splits: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
        "\n",
        "#     return train_loader, val_loader, test_loader, num_classes\n",
        "\n",
        "def get_proportion(num_classes, dataset):\n",
        "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
        "    n_temp = total_count - n_train   # this is val + test\n",
        "\n",
        "    sss1 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_train,\n",
        "        test_size=n_temp,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    # Train and temp(val+test) indices\n",
        "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
        "\n",
        "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
        "    n_test = total_count - n_train - n_val\n",
        "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
        "\n",
        "    labels_temp = labels[temp_idx]\n",
        "\n",
        "    sss2 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_val,\n",
        "        test_size=n_test,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
        "\n",
        "    val_idx = temp_idx[val_idx_in_temp]\n",
        "    test_idx = temp_idx[test_idx_in_temp]\n",
        "\n",
        "    assert len(train_idx) == n_train\n",
        "    assert len(val_idx) == n_val\n",
        "    assert len(test_idx) == n_test\n",
        "\n",
        "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
        "    return train_idx,val_idx,test_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet50(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "    model.train()\n",
        "    tot_loss, corr, tot = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "\n",
        "        loss   = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if sched: sched.step()\n",
        "        tot_loss += loss.item()\n",
        "        preds    = logits.argmax(dim=1)\n",
        "        corr    += (preds==yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "        avg_loss = tot_loss / len(loader)\n",
        "\n",
        "    avg_loss = tot_loss / len(loader)\n",
        "    acc = 100.0 * corr / tot\n",
        "    return avg_loss, acc\n",
        "\n",
        "def evaluate(model, loader, num_classes):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    correct_per_class = torch.zeros(num_classes, dtype=torch.int64)\n",
        "    total_per_class   = torch.zeros(num_classes, dtype=torch.int64)\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            preds  = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "            total_correct += (preds == yb).sum().item()\n",
        "            total_samples += yb.size(0)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                # mask of samples in this batch whose true label == c\n",
        "                class_mask = (yb == c)\n",
        "                if class_mask.sum().item() == 0:\n",
        "                    continue\n",
        "\n",
        "                total_per_class[c] += class_mask.sum().item()\n",
        "\n",
        "                correct_per_class[c] += ((preds == yb) & class_mask).sum().item()\n",
        "\n",
        "    overall_acc = 100.0 * total_correct / total_samples\n",
        "\n",
        "    acc_per_class = {}\n",
        "    for c in range(num_classes):\n",
        "        if total_per_class[c].item() > 0:\n",
        "            acc = 100.0 * correct_per_class[c].item() / total_per_class[c].item()\n",
        "        else:\n",
        "            acc = 0.0\n",
        "        acc_per_class[c] = acc\n",
        "\n",
        "    return overall_acc, acc_per_class, all_labels, all_preds\n",
        "\n",
        "def plot_confusion_matrix_from_preds(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # normalize by true-label counts (row‐wise) to get percentages\n",
        "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    \n",
        "    ticks = np.arange(len(class_names))\n",
        "    plt.xticks(ticks, class_names, rotation=90)\n",
        "    plt.yticks(ticks, class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    # threshold for text color\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            pct = cm_norm[i, j] * 100\n",
        "            plt.text(\n",
        "                j, i,\n",
        "                f\"{cm[i, j]}\\n{pct:.1f}%\",\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
        "            )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_class_acc_prop(te_dl, acc_vals, class_proportions_test):\n",
        "    classes = te_dl.dataset.dataset.classes\n",
        "    x = np.arange(len(classes))\n",
        "\n",
        "    acc   = acc_vals\n",
        "    prop  = class_proportions_test * 100\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "    bars = ax1.bar(x, acc, color='C0', alpha=0.7)\n",
        "    ax1.set_ylabel('Accuracy (%)', color='C0')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.tick_params(axis='y', labelcolor='C0')\n",
        "\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}%', ha='center', va='bottom', color='C0')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    line = ax2.plot(x, prop, color='C1', marker='o', linewidth=2)\n",
        "    ax2.set_ylabel('Test Proportion (%)', color='C1')\n",
        "    ax2.set_ylim(0, max(prop)*1.2)\n",
        "    ax2.tick_params(axis='y', labelcolor='C1')\n",
        "\n",
        "    for xi, yi in zip(x, prop):\n",
        "        ax2.text(xi, yi + max(prop)*0.02, f'{yi:.1f}%', ha='center', va='bottom', color='C1')\n",
        "\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
        "    plt.title('Per-class Accuracy vs. Test Proportion')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def hyperparam_search(pretrained=True):\n",
        "    print(f\"\\n>>> Starting hyperparameter search with pretrained={pretrained}...\")\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "\n",
        "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}\")\n",
        "        \n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "        model = build_model(n_cls, pretrained=pretrained)\n",
        "        \n",
        "        total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "        warmup_steps = len(tr_dl)\n",
        "        opt = optim.AdamW(model.parameters(), lr=lr, betas=BETAS, eps=float(EPS), weight_decay=wd)\n",
        "        sched = SequentialLR(\n",
        "            opt,\n",
        "            schedulers=[\n",
        "                LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "                CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "            ],\n",
        "            milestones=[warmup_steps]\n",
        "        )\n",
        "        crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Start a W&B run\n",
        "        wandb_run = wandb.init(\n",
        "            project=\"eurosat-supervised-scratch-grid-search-augmented\",\n",
        "            name=f\"BS{bs}_LR{lr:.0e}_TR{TRAIN_FRAC}\",\n",
        "            config={\n",
        "                \"batch_size\": bs,\n",
        "                \"learning_rate\": lr,\n",
        "                \"epochs\": NUM_EPOCHS,\n",
        "                \"pretrained\": pretrained,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        for ep in range(NUM_EPOCHS):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "            # Compute validation loss & accuracy in one pass\n",
        "            model.eval()\n",
        "            val_loss, corr, tot = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_dl:\n",
        "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                    logits = model(xb)\n",
        "                    loss = crit(logits, yb)\n",
        "                    val_loss += loss.item()\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                    corr += (preds == yb).sum().item()\n",
        "                    tot  += yb.size(0)\n",
        "            val_loss /= len(val_dl)\n",
        "            val_acc = 100.0 * corr / tot\n",
        "        \n",
        "\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train_acc={tr_acc:.1f}%  train_loss={tr_loss:.4f}, \"\n",
        "                  f\"val_acc={val_acc:.1f}%, val_loss={val_loss:.4f}\")\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\":       ep + 1,\n",
        "                \"train_loss\":  tr_loss,\n",
        "                \"train_acc\":   tr_acc,\n",
        "                \"val_loss\":    val_loss,\n",
        "                \"val_acc\":     val_acc\n",
        "            })\n",
        "\n",
        "        wandb_run.finish()\n",
        "\n",
        "        # Only use val_acc to pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val   = val_acc\n",
        "            best_cfg   = (bs, lr, wd)\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, val_acc={best_val:.1f}%\")\n",
        "    \n",
        "    return best_cfg, best_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform Hyperparameter Search, Retrain on Train + Validation Set, Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_optimizer_scheduler(params, lr, wd, steps_per_epoch, epochs):\n",
        "    total_steps  = epochs * steps_per_epoch\n",
        "    warmup_steps = steps_per_epoch\n",
        "    opt = optim.AdamW(params, lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "    sched = SequentialLR(\n",
        "        opt,\n",
        "        schedulers=[\n",
        "            LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "            CosineAnnealingLR(opt, T_max=total_steps - warmup_steps)\n",
        "        ],\n",
        "        milestones=[warmup_steps]\n",
        "    )\n",
        "    return opt, sched\n",
        "\n",
        "def retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, num_epochs):\n",
        "\n",
        "    combined_ds = ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "    combined_dl = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=4)\n",
        "\n",
        "    model = build_model(n_cls, pretrained=False)\n",
        "    optimizer, scheduler = make_optimizer_scheduler(\n",
        "        model.parameters(), lr, wd, len(combined_dl), num_epochs\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs):\n",
        "        loss, acc = train_one_epoch(model, combined_dl, optimizer, criterion, sched=scheduler)\n",
        "        print(f\"  Ep {ep+1}/{num_epochs}: train_acc={acc:.1f}%\")\n",
        "    return model, combined_ds\n",
        "\n",
        "#  Evaluate & log to wandb\n",
        "def evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr):\n",
        "    \"\"\"\n",
        "    Evaluate on test set, print per-class stats, log to wandb, and plot.\n",
        "    \"\"\"\n",
        "    final_test_acc, acc_per_class, y_true, y_pred = evaluate(final_model, te_dl, n_cls)\n",
        "    plot_confusion_matrix_from_preds(y_true, y_pred, te_dl.dataset.dataset.classes)\n",
        "\n",
        "    test_targs = np.array(te_dl.dataset.dataset.targets)[te_dl.dataset.indices]\n",
        "    prop_test = np.bincount(test_targs, minlength=n_cls) / len(test_targs)\n",
        "\n",
        "    combined_targs = np.concatenate([\n",
        "        np.array(ds.dataset.targets)[ds.indices] for ds in combined_ds.datasets\n",
        "    ])\n",
        "    prop_trainval = np.bincount(combined_targs, minlength=n_cls) / len(combined_targs)\n",
        "\n",
        "    acc_vals = np.array([acc_per_class[c] for c in range(n_cls)])\n",
        "    weighted_acc = (acc_vals * prop_test).sum()\n",
        "\n",
        "    print(\"\\n>>> Final Test Accuracy:\")\n",
        "    print(f\"  Overall:             {final_test_acc:5.1f}%\")\n",
        "    print(f\"  Weighted class acc.: {weighted_acc:5.1f}%\\n\")\n",
        "    hdr = f\"{'Class':20s}  {'Acc':>6s}   {'Train+Val':>9s}   {'Test':>6s}\"\n",
        "    print(hdr); print(\"-\"*len(hdr))\n",
        "    for c, name in enumerate(te_dl.dataset.dataset.classes):\n",
        "        print(f\"{name:20s}  {acc_vals[c]:6.1f}%   {prop_trainval[c]*100:8.0f}%   {prop_test[c]*100:6.0f}%\")\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"eurosat-supervised-scratch-final-augmented\",\n",
        "        name=f\"BS{bs}_LR{lr:.0e}_final\",\n",
        "        config={\n",
        "            \"batch_size\": bs, \"learning_rate\": lr, \"epochs\": NUM_EPOCHS,\n",
        "            \"pretrained\": False, \"final_retrain\": True\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"final_test_acc\":     final_test_acc,\n",
        "        \"weighted_class_acc\": weighted_acc,\n",
        "        \"per_class_acc\":      acc_vals\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    plot_class_acc_prop(te_dl, acc_vals, prop_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Starting hyperparameter search with pretrained=False...\n",
            "\n",
            ">>> Testing BS=128, LR=3.8e-04\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed mean: [0.3441457748413086, 0.38009852170944214, 0.40766361355781555]\n",
            "Computed std:  [0.09299740195274353, 0.06464490294456482, 0.054139167070388794]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250610_161150-xtx6woca</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-augmented/runs/xtx6woca' target=\"_blank\">BS128_LR4e-04_TR0.8</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-augmented' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-augmented' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-augmented</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-augmented/runs/xtx6woca' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-augmented/runs/xtx6woca</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 15.56 GiB of which 16.38 MiB is free. Process 35125 has 906.00 MiB memory in use. Process 46635 has 904.00 MiB memory in use. Process 20179 has 4.14 GiB memory in use. Including non-PyTorch memory, this process has 9.62 GiB memory in use. Of the allocated memory 9.40 GiB is allocated by PyTorch, and 93.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[1;32m      2\u001b[0m set_seed(SEED)\n\u001b[0;32m----> 4\u001b[0m best_cfg, _    \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m bs, lr, wd     \u001b[38;5;241m=\u001b[39m best_cfg\n\u001b[1;32m      6\u001b[0m tr_dl, val_dl, te_dl, n_cls \u001b[38;5;241m=\u001b[39m get_data_loaders(DATA_DIR, bs)\n",
            "Cell \u001b[0;32mIn[5], line 183\u001b[0m, in \u001b[0;36mhyperparam_search\u001b[0;34m(pretrained)\u001b[0m\n\u001b[1;32m    171\u001b[0m wandb_run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m    172\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meurosat-supervised-scratch-grid-search-augmented\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    173\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_LR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_TR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_FRAC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     }\n\u001b[1;32m    180\u001b[0m )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m--> 183\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Compute validation loss & accuracy in one pass\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
            "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, opt, crit, sched)\u001b[0m\n\u001b[1;32m     18\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(DEVICE), yb\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     19\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss   \u001b[38;5;241m=\u001b[39m crit(logits, yb)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[0;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 15.56 GiB of which 16.38 MiB is free. Process 35125 has 906.00 MiB memory in use. Process 46635 has 904.00 MiB memory in use. Process 20179 has 4.14 GiB memory in use. Including non-PyTorch memory, this process has 9.62 GiB memory in use. Of the allocated memory 9.40 GiB is allocated by PyTorch, and 93.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Main\n",
        "set_seed(SEED)\n",
        "\n",
        "best_cfg, _    = hyperparam_search(pretrained=False)\n",
        "bs, lr, wd     = best_cfg\n",
        "tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "# Retrain on TRAIN+VAL\n",
        "final_model, combined_ds = retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, NUM_EPOCHS)\n",
        "\n",
        "evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr)\n",
        "\n",
        "final_path = f\"models/eurosat_supervised_final_bs{bs}_lr{lr:.0e}_epcs{NUM_EPOCHS}.pth\"\n",
        "torch.save(final_model.state_dict(), final_path)\n",
        "print(f\"Final model saved to {final_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
