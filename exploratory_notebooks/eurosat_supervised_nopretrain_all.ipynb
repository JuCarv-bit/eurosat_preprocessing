{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1\n",
            "0.20.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, zipfile, urllib\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, SequentialLR, MultiStepLR\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "LOCAL_OR_COLAB = \"LOCAL\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 34\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TRAIN_FRAC = 0.8\n",
        "VAL_FRAC   = 0.1\n",
        "TEST_FRAC  = 0.1\n",
        "\n",
        "BATCH_SIZES = [128]  \n",
        "LRS = [1e-4, 3e-4]\n",
        "\n",
        "GRID = product(\n",
        "    [0.1, 0.01],    # learning rate\n",
        "    [0.01, 0.0001]  # weight decay\n",
        ")\n",
        "\n",
        "TRAINING_SCHEDULES = {\n",
        "    \"short\": {\"p\": [750, 1500, 2500], \"w\": 200, \"unit\": \"steps\"},\n",
        "    \"medium\": {\"p\": [3000, 6000], \"w\": 500, \"unit\": \"steps\"},\n",
        "    \"long\": {\"p\": [30, 60], \"w\": 5, \"unit\": \"epochs\"}\n",
        "}\n",
        "\n",
        "\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB/\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "NUM_WORKERS = 4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully set to use GPU: 1 (Quadro RTX 6000)\n",
            "Final DEVICE variable is set to: cuda:1\n",
            "Current PyTorch default device: 0\n",
            "Current PyTorch default device (after set_device): 1\n",
            "Dummy tensor is on device: cuda:1\n"
          ]
        }
      ],
      "source": [
        "# Use GPU1\n",
        "TARGET_GPU_INDEX = 1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    if TARGET_GPU_INDEX < torch.cuda.device_count():\n",
        "        DEVICE = torch.device(f\"cuda:{TARGET_GPU_INDEX}\")\n",
        "        print(f\"Successfully set to use GPU: {TARGET_GPU_INDEX} ({torch.cuda.get_device_name(TARGET_GPU_INDEX)})\")\n",
        "    else:\n",
        "        print(f\"Error: Physical GPU {TARGET_GPU_INDEX} is not available. There are only {torch.cuda.device_count()} GPUs (0 to {torch.cuda.device_count() - 1}).\")\n",
        "        print(\"Falling back to CPU.\")\n",
        "        DEVICE = torch.device(\"CPU\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Falling back to CPU.\")\n",
        "    DEVICE = torch.device(\"CPU\")\n",
        "\n",
        "print(f\"Final DEVICE variable is set to: {DEVICE}\")\n",
        "if DEVICE.type == 'cuda':\n",
        "    print(f\"Current PyTorch default device: {torch.cuda.current_device()}\")\n",
        "\n",
        "    torch.cuda.set_device(TARGET_GPU_INDEX)\n",
        "    print(f\"Current PyTorch default device (after set_device): {torch.cuda.current_device()}\")\n",
        "\n",
        "dummy_tensor = torch.randn(2, 2)\n",
        "dummy_tensor_on_gpu = dummy_tensor.to(DEVICE)\n",
        "print(f\"Dummy tensor is on device: {dummy_tensor_on_gpu.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    indices = np.arange(total_count)\n",
        "    np.random.seed(SEED) # for reproducibility\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_split = int(0.8 * total_count)\n",
        "    val_split = int(0.9 * total_count)\n",
        "\n",
        "    train_idx = indices[:train_split]\n",
        "    val_idx = indices[train_split:val_split]\n",
        "    test_idx = indices[val_split:]\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "    base_tf = transforms.ToTensor()\n",
        "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "    labels = np.array(ds_all.targets)\n",
        "    num_classes = len(ds_all.classes)\n",
        "    total_count = len(ds_all)\n",
        "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
        "\n",
        "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
        "\n",
        "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
        "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
        "    print(f\"Computed mean: {mean}\")\n",
        "    print(f\"Computed std:  {std}\")\n",
        "\n",
        "\n",
        "\n",
        "    train_transform_augmented = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomApply([transforms.RandomRotation(angle) for angle in [0, 90, 180, 270]], p=1.0), # Apply one of 0, 90, 180, 270 rotations\n",
        "        transforms.RandomHorizontalFlip(p=0.5), # Randomly apply horizontal flip (50% chance)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.Resize(256), # Resize to 256x256\n",
        "        transforms.CenterCrop(224), # Perform a central crop of 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "    # Create datasets with the respective transformations\n",
        "    train_ds = datasets.ImageFolder(root=data_dir, transform=train_transform_augmented)\n",
        "    val_ds = datasets.ImageFolder(root=data_dir, transform=eval_transform)\n",
        "    test_ds = datasets.ImageFolder(root=data_dir, transform=eval_transform)\n",
        "\n",
        "    # Apply subsets to the transformed datasets\n",
        "    train_ds_subset = Subset(train_ds, train_idx)\n",
        "    val_ds_subset = Subset(val_ds, val_idx)\n",
        "    test_ds_subset = Subset(test_ds, test_idx)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_ds_subset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    val_loader   = DataLoader(val_ds_subset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    test_loader  = DataLoader(test_ds_subset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "    print(f\"Train/Val/Test splits: {len(train_ds_subset)}/{len(val_ds_subset)}/{len(test_ds_subset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lr_scheduler(optimizer, total_training_steps, schedule_cfg, steps_per_epoch):\n",
        "\n",
        "    warmup_iters = schedule_cfg[\"w\"]\n",
        "    milestones = [] # Points at which LR drops\n",
        "\n",
        "    if schedule_cfg[\"unit\"] == \"steps\":\n",
        "        milestones = schedule_cfg[\"p\"]\n",
        "    elif schedule_cfg[\"unit\"] == \"epochs\":\n",
        "        # Convert epoch milestones to step milestones\n",
        "        milestones = [m * steps_per_epoch for m in schedule_cfg[\"p\"]]\n",
        "        warmup_iters = schedule_cfg[\"w\"] * steps_per_epoch # Convert warmup epochs to steps\n",
        "\n",
        "    # Linear warm-up scheduler\n",
        "    warmup_scheduler = LinearLR(optimizer, start_factor=1e-6, end_factor=1.0, total_iters=warmup_iters)\n",
        "\n",
        "    decay_scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        schedulers=[warmup_scheduler, decay_scheduler],\n",
        "        milestones=[warmup_iters]\n",
        "    )\n",
        "    return scheduler\n",
        "\n",
        "def hyperparam_search(pretrained=True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "\n",
        "    for bs, (lr, wd), schedule_name in product(BATCH_SIZES, GRID, TRAINING_SCHEDULES.keys()):\n",
        "\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd:.1e}, Schedule={schedule_name}\")\n",
        "\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs) \n",
        "\n",
        "\n",
        "        steps_per_epoch = len(tr_dl)\n",
        "\n",
        "        schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "        if schedule_cfg[\"unit\"] == \"steps\":\n",
        "\n",
        "            total_steps = max(schedule_cfg[\"p\"]) \n",
        "            NUM_EPOCHS_FOR_RUN = int(np.ceil(total_steps / steps_per_epoch)) + 1 # Add a buffer epoch\n",
        "        else: # epochs\n",
        "            total_epochs_from_schedule = max(schedule_cfg[\"p\"]) + schedule_cfg[\"w\"] # max 'p' + warmup epochs\n",
        "            NUM_EPOCHS_FOR_RUN = total_epochs_from_schedule # Total epochs to run\n",
        "            total_steps = NUM_EPOCHS_FOR_RUN * steps_per_epoch\n",
        "\n",
        "\n",
        "        model = build_model(n_cls, pretrained=pretrained)\n",
        "        model.to(DEVICE) \n",
        "\n",
        "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        sched = build_lr_scheduler(opt, total_steps, schedule_cfg, steps_per_epoch)\n",
        "\n",
        "        wandb_run = wandb.init(\n",
        "            project=\"eurosat-supervised-scratch-grid-search-lrsched\",\n",
        "            name=f\"BS{bs}_LR{lr:.0e}_WD{wd:.0e}_Sched_{schedule_name}\",\n",
        "            config={\n",
        "                \"batch_size\": bs,\n",
        "                \"learning_rate\": lr,\n",
        "                \"weight_decay\": wd,\n",
        "                \"schedule_name\": schedule_name,\n",
        "                \"total_epochs_for_run\": NUM_EPOCHS_FOR_RUN,\n",
        "                \"pretrained\": pretrained,\n",
        "                \"optimizer\": \"SGD_momentum_0.9\",\n",
        "                \"scheduler_type\": \"LinearWarmup_MultiStepLR\",\n",
        "                \"warmup_steps_or_epochs\": schedule_cfg[\"w\"],\n",
        "                \"decay_milestones\": schedule_cfg[\"p\"],\n",
        "                \"decay_unit\": schedule_cfg[\"unit\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        for ep in range(NUM_EPOCHS_FOR_RUN):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched, DEVICE) \n",
        "            model.eval()\n",
        "            val_loss, corr, tot = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_dl:\n",
        "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                    logits = model(xb)\n",
        "                    loss = crit(logits, yb)\n",
        "                    val_loss += loss.item()\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                    corr += (preds == yb).sum().item()\n",
        "                    tot  += yb.size(0)\n",
        "            val_loss /= len(val_dl)\n",
        "            val_acc = 100.0 * corr / tot\n",
        "\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS_FOR_RUN}: train_acc={tr_acc:.1f}%  train_loss={tr_loss:.4f}, \"\n",
        "                  f\"val_acc={val_acc:.1f}%, val_loss={val_loss:.4f}\")\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\":       ep + 1,\n",
        "                \"train_loss\":  tr_loss,\n",
        "                \"train_acc\":   tr_acc,\n",
        "                \"val_loss\":    val_loss,\n",
        "                \"val_acc\":     val_acc,\n",
        "                \"learning_rate\": opt.param_groups[0]['lr'] \n",
        "            })\n",
        "\n",
        "        wandb_run.finish()\n",
        "\n",
        "        # Only use val_acc to pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val   = val_acc\n",
        "            best_cfg   = (bs, lr, wd, schedule_name)\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]:.1e}, Schedule={best_cfg[3]}, val_acc={best_val:.1f}%\")\n",
        "\n",
        "    return best_cfg, best_model\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0) # Accumulate weighted by batch size\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = 100 * correct_predictions / total_samples\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_proportion(num_classes, dataset):\n",
        "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
        "    n_temp = total_count - n_train   \n",
        "\n",
        "    sss1 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_train,\n",
        "        test_size=n_temp,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    # Train and temp(val+test) indices\n",
        "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
        "\n",
        "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
        "    n_test = total_count - n_train - n_val\n",
        "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
        "\n",
        "    labels_temp = labels[temp_idx]\n",
        "\n",
        "    sss2 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_val,\n",
        "        test_size=n_test,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
        "\n",
        "    val_idx = temp_idx[val_idx_in_temp]\n",
        "    test_idx = temp_idx[test_idx_in_temp]\n",
        "\n",
        "    assert len(train_idx) == n_train\n",
        "    assert len(val_idx) == n_val\n",
        "    assert len(test_idx) == n_test\n",
        "\n",
        "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
        "    return train_idx,val_idx,test_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet50(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def evaluate(model, loader, num_classes):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    correct_per_class = torch.zeros(num_classes, dtype=torch.int64)\n",
        "    total_per_class   = torch.zeros(num_classes, dtype=torch.int64)\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            preds  = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "            total_correct += (preds == yb).sum().item()\n",
        "            total_samples += yb.size(0)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                # mask of samples in this batch whose true label == c\n",
        "                class_mask = (yb == c)\n",
        "                if class_mask.sum().item() == 0:\n",
        "                    continue\n",
        "\n",
        "                total_per_class[c] += class_mask.sum().item()\n",
        "\n",
        "                correct_per_class[c] += ((preds == yb) & class_mask).sum().item()\n",
        "\n",
        "    overall_acc = 100.0 * total_correct / total_samples\n",
        "\n",
        "    acc_per_class = {}\n",
        "    for c in range(num_classes):\n",
        "        if total_per_class[c].item() > 0:\n",
        "            acc = 100.0 * correct_per_class[c].item() / total_per_class[c].item()\n",
        "        else:\n",
        "            acc = 0.0\n",
        "        acc_per_class[c] = acc\n",
        "\n",
        "    return overall_acc, acc_per_class, all_labels, all_preds\n",
        "\n",
        "def plot_confusion_matrix_from_preds(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    \n",
        "    ticks = np.arange(len(class_names))\n",
        "    plt.xticks(ticks, class_names, rotation=90)\n",
        "    plt.yticks(ticks, class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    # threshold for text color\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            pct = cm_norm[i, j] * 100\n",
        "            plt.text(\n",
        "                j, i,\n",
        "                f\"{cm[i, j]}\\n{pct:.1f}%\",\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
        "            )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_class_acc_prop(te_dl, acc_vals, class_proportions_test):\n",
        "    classes = te_dl.dataset.dataset.classes\n",
        "    x = np.arange(len(classes))\n",
        "\n",
        "    acc   = acc_vals\n",
        "    prop  = class_proportions_test * 100\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "    bars = ax1.bar(x, acc, color='C0', alpha=0.7)\n",
        "    ax1.set_ylabel('Accuracy (%)', color='C0')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.tick_params(axis='y', labelcolor='C0')\n",
        "\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}%', ha='center', va='bottom', color='C0')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    line = ax2.plot(x, prop, color='C1', marker='o', linewidth=2)\n",
        "    ax2.set_ylabel('Test Proportion (%)', color='C1')\n",
        "    ax2.set_ylim(0, max(prop)*1.2)\n",
        "    ax2.tick_params(axis='y', labelcolor='C1')\n",
        "\n",
        "    for xi, yi in zip(x, prop):\n",
        "        ax2.text(xi, yi + max(prop)*0.02, f'{yi:.1f}%', ha='center', va='bottom', color='C1')\n",
        "\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
        "    plt.title('Per-class Accuracy vs. Test Proportion')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform Hyperparameter Search, Retrain on Train + Validation Set, Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def make_optimizer_scheduler_reused(params, lr, wd, schedule_name, steps_per_epoch):\n",
        "\n",
        "    opt = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "\n",
        "    total_steps_for_scheduler_config = max(schedule_cfg['p']) if schedule_cfg['unit'] == 'steps' else max(schedule_cfg['p']) * steps_per_epoch\n",
        "\n",
        "    scheduler = build_lr_scheduler(opt, total_steps_for_scheduler_config, schedule_cfg, steps_per_epoch)\n",
        "    return opt, scheduler\n",
        "\n",
        "\n",
        "\n",
        "def retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, schedule_name): \n",
        "\n",
        "    print(\"\\n>>> Retraining final model on TRAIN+VAL combined with best hyperparameters\")\n",
        "    combined_ds = ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "\n",
        "\n",
        "    combined_dl = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=4) \n",
        "\n",
        "    model = build_model(n_cls, pretrained=False)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    steps_per_epoch = len(combined_dl)\n",
        "    schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "    if schedule_cfg[\"unit\"] == \"steps\":\n",
        "        total_steps_for_run = max(schedule_cfg[\"p\"]) \n",
        "        num_epochs_for_run = int(np.ceil(total_steps_for_run / steps_per_epoch)) + 1\n",
        "    else: # epochs\n",
        "        num_epochs_for_run = max(schedule_cfg[\"p\"]) + schedule_cfg[\"w\"]\n",
        "\n",
        "\n",
        "    optimizer, scheduler = make_optimizer_scheduler_reused( \n",
        "        model.parameters(), lr, wd, schedule_name, steps_per_epoch\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs_for_run): \n",
        "        loss, acc = train_one_epoch(model, combined_dl, optimizer, criterion, scheduler, DEVICE) \n",
        "        print(f\"  Ep {ep+1}/{num_epochs_for_run}: train_acc={acc:.1f}%\")\n",
        "    return model, combined_ds\n",
        "\n",
        "def evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr):\n",
        "\n",
        "    final_test_acc, acc_per_class, y_true, y_pred = evaluate(final_model, te_dl, n_cls)\n",
        "    plot_confusion_matrix_from_preds(y_true, y_pred, te_dl.dataset.dataset.classes)\n",
        "\n",
        "    test_targs = np.array(te_dl.dataset.dataset.targets)[te_dl.dataset.indices]\n",
        "    prop_test = np.bincount(test_targs, minlength=n_cls) / len(test_targs)\n",
        "\n",
        "    combined_targs = np.concatenate([\n",
        "        np.array(ds.dataset.targets)[ds.indices] for ds in combined_ds.datasets\n",
        "    ])\n",
        "    prop_trainval = np.bincount(combined_targs, minlength=n_cls) / len(combined_targs)\n",
        "\n",
        "    acc_vals = np.array([acc_per_class[c] for c in range(n_cls)])\n",
        "    weighted_acc = (acc_vals * prop_test).sum()\n",
        "\n",
        "    print(\"\\n>>> Final Test Accuracy:\")\n",
        "    print(f\"  Overall:             {final_test_acc:5.1f}%\")\n",
        "    print(f\"  Weighted class acc.: {weighted_acc:5.1f}%\\n\")\n",
        "    hdr = f\"{'Class':20s}  {'Acc':>6s}   {'Train+Val':>9s}   {'Test':>6s}\"\n",
        "    print(hdr); print(\"-\"*len(hdr))\n",
        "    for c, name in enumerate(te_dl.dataset.dataset.classes):\n",
        "        print(f\"{name:20s}  {acc_vals[c]:6.1f}%   {prop_trainval[c]*100:8.0f}%   {prop_test[c]*100:6.0f}%\")\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"eurosat-supervised-scratch-final-lrsched\",\n",
        "        name=f\"BS{bs}_LR{lr:.0e}_final\",\n",
        "        config={\n",
        "            \"batch_size\": bs, \"learning_rate\": lr, \"epochs\": NUM_EPOCHS,\n",
        "            \"pretrained\": False, \"final_retrain\": True\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"final_test_acc\":     final_test_acc,\n",
        "        \"weighted_class_acc\": weighted_acc,\n",
        "        \"per_class_acc\":      acc_vals\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    plot_class_acc_prop(te_dl, acc_vals, prop_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-02, Schedule=short\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_142034-u5adzz5n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/u5adzz5n' target=\"_blank\">BS128_LR1e-01_WD1e-02_Sched_short</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/u5adzz5n' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/u5adzz5n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/16: train_acc=43.8%  train_loss=1.6966, val_acc=15.7%, val_loss=3.6202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate   use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep2/16: train_acc=54.1%  train_loss=1.2968, val_acc=12.7%, val_loss=2.7988\n",
            "  Ep3/16: train_acc=57.0%  train_loss=1.1932, val_acc=21.9%, val_loss=4.1103\n",
            "  Ep4/16: train_acc=58.3%  train_loss=1.1581, val_acc=21.4%, val_loss=4.5286\n",
            "  Ep5/16: train_acc=60.4%  train_loss=1.1229, val_acc=40.9%, val_loss=1.9583\n",
            "  Ep6/16: train_acc=65.4%  train_loss=0.9754, val_acc=62.2%, val_loss=0.9772\n",
            "  Ep7/16: train_acc=74.5%  train_loss=0.7195, val_acc=71.5%, val_loss=0.7592\n",
            "  Ep8/16: train_acc=76.5%  train_loss=0.6662, val_acc=72.2%, val_loss=0.7984\n",
            "  Ep9/16: train_acc=78.1%  train_loss=0.6280, val_acc=70.8%, val_loss=0.8392\n",
            "  Ep10/16: train_acc=79.3%  train_loss=0.6079, val_acc=62.9%, val_loss=1.0469\n",
            "  Ep11/16: train_acc=83.8%  train_loss=0.4910, val_acc=84.7%, val_loss=0.4467\n",
            "  Ep12/16: train_acc=85.1%  train_loss=0.4571, val_acc=86.3%, val_loss=0.4231\n",
            "  Ep13/16: train_acc=85.5%  train_loss=0.4421, val_acc=84.9%, val_loss=0.4422\n",
            "  Ep14/16: train_acc=86.4%  train_loss=0.4238, val_acc=87.5%, val_loss=0.3933\n",
            "  Ep15/16: train_acc=87.0%  train_loss=0.4121, val_acc=87.6%, val_loss=0.3834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep16/16: train_acc=87.3%  train_loss=0.4001, val_acc=89.2%, val_loss=0.3520\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate</td><td>▇████▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▄▄▆▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▅▅▅▄▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▂▄▆▆▆▆▆██████</td></tr><tr><td>val_loss</td><td>▆▅▇█▄▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_acc</td><td>87.31481</td></tr><tr><td>train_loss</td><td>0.40008</td></tr><tr><td>val_acc</td><td>89.18519</td></tr><tr><td>val_loss</td><td>0.35203</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS128_LR1e-01_WD1e-02_Sched_short</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/u5adzz5n' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/u5adzz5n</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250611_142034-u5adzz5n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-02, Schedule=medium\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_145420-kzdbqkbv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/kzdbqkbv' target=\"_blank\">BS128_LR1e-01_WD1e-02_Sched_medium</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/kzdbqkbv' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/kzdbqkbv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/37: train_acc=44.2%  train_loss=1.6066, val_acc=22.0%, val_loss=6.7116\n",
            "  Ep2/37: train_acc=59.4%  train_loss=1.1561, val_acc=31.5%, val_loss=2.0007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate   use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep3/37: train_acc=60.9%  train_loss=1.1019, val_acc=37.6%, val_loss=1.8959\n",
            "  Ep4/37: train_acc=59.9%  train_loss=1.1444, val_acc=32.2%, val_loss=2.1462\n",
            "  Ep5/37: train_acc=65.0%  train_loss=0.9894, val_acc=21.8%, val_loss=4.4077\n",
            "  Ep6/37: train_acc=69.1%  train_loss=0.8718, val_acc=33.6%, val_loss=2.3775\n",
            "  Ep7/37: train_acc=66.3%  train_loss=0.9604, val_acc=9.4%, val_loss=7.1151\n",
            "  Ep8/37: train_acc=65.4%  train_loss=0.9762, val_acc=36.7%, val_loss=2.0710\n",
            "  Ep9/37: train_acc=59.9%  train_loss=1.1172, val_acc=40.0%, val_loss=1.6335\n",
            "  Ep10/37: train_acc=58.4%  train_loss=1.1240, val_acc=25.1%, val_loss=3.2034\n",
            "  Ep11/37: train_acc=63.9%  train_loss=1.0058, val_acc=30.4%, val_loss=3.4481\n",
            "  Ep12/37: train_acc=63.2%  train_loss=1.0188, val_acc=19.7%, val_loss=6.1237\n",
            "  Ep13/37: train_acc=58.6%  train_loss=1.1573, val_acc=12.0%, val_loss=4.2154\n",
            "  Ep14/37: train_acc=56.8%  train_loss=1.1661, val_acc=11.1%, val_loss=6.8875\n",
            "  Ep15/37: train_acc=50.7%  train_loss=1.2893, val_acc=13.7%, val_loss=4.8938\n",
            "  Ep16/37: train_acc=53.4%  train_loss=1.2154, val_acc=22.3%, val_loss=2.9909\n",
            "  Ep17/37: train_acc=57.8%  train_loss=1.1463, val_acc=11.4%, val_loss=6.4455\n",
            "  Ep18/37: train_acc=64.1%  train_loss=1.0214, val_acc=21.7%, val_loss=2.8331\n",
            "  Ep19/37: train_acc=59.8%  train_loss=1.1047, val_acc=26.6%, val_loss=2.5026\n",
            "  Ep20/37: train_acc=56.6%  train_loss=1.2072, val_acc=18.7%, val_loss=25.7072\n",
            "  Ep21/37: train_acc=57.6%  train_loss=1.1392, val_acc=57.1%, val_loss=1.2065\n",
            "  Ep22/37: train_acc=68.0%  train_loss=0.8970, val_acc=64.7%, val_loss=0.9573\n",
            "  Ep23/37: train_acc=70.5%  train_loss=0.8302, val_acc=68.7%, val_loss=0.8987\n",
            "  Ep24/37: train_acc=72.8%  train_loss=0.7663, val_acc=62.2%, val_loss=1.0326\n",
            "  Ep25/37: train_acc=76.4%  train_loss=0.6809, val_acc=72.9%, val_loss=0.7609\n",
            "  Ep26/37: train_acc=77.7%  train_loss=0.6421, val_acc=52.9%, val_loss=1.5920\n",
            "  Ep27/37: train_acc=78.3%  train_loss=0.6279, val_acc=72.6%, val_loss=0.8148\n",
            "  Ep28/37: train_acc=79.0%  train_loss=0.6056, val_acc=74.6%, val_loss=0.7290\n",
            "  Ep29/37: train_acc=79.4%  train_loss=0.6005, val_acc=67.1%, val_loss=0.9081\n",
            "  Ep30/37: train_acc=79.7%  train_loss=0.5862, val_acc=79.0%, val_loss=0.6242\n",
            "  Ep31/37: train_acc=80.8%  train_loss=0.5562, val_acc=65.2%, val_loss=1.0886\n",
            "  Ep32/37: train_acc=81.6%  train_loss=0.5444, val_acc=74.9%, val_loss=0.7241\n",
            "  Ep33/37: train_acc=82.0%  train_loss=0.5383, val_acc=74.3%, val_loss=0.7697\n",
            "  Ep34/37: train_acc=82.9%  train_loss=0.5174, val_acc=58.1%, val_loss=1.2519\n",
            "  Ep35/37: train_acc=83.5%  train_loss=0.5071, val_acc=61.7%, val_loss=1.2379\n",
            "  Ep36/37: train_acc=83.8%  train_loss=0.4947, val_acc=71.3%, val_loss=0.8414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep37/37: train_acc=84.8%  train_loss=0.4711, val_acc=61.2%, val_loss=1.2107\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▃▅██████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▅▅▅▅▄▃▄▄▃▃▂▃▃▄▄▃▃▅▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▅▅▅▄▃▄▄▅▅▄▄▅▅▆▆▅▄▅▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▃▄▃▂▃▁▄▄▃▃▂▁▁▁▂▁▂▃▂▆▇▇▆▇▅▇█▇█▇██▆▆▇▆</td></tr><tr><td>val_loss</td><td>▃▁▁▁▂▁▃▁▁▂▂▃▂▃▂▂▃▂▂█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>learning_rate</td><td>0.01</td></tr><tr><td>train_acc</td><td>84.81944</td></tr><tr><td>train_loss</td><td>0.47109</td></tr><tr><td>val_acc</td><td>61.18519</td></tr><tr><td>val_loss</td><td>1.2107</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS128_LR1e-01_WD1e-02_Sched_medium</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/kzdbqkbv' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/kzdbqkbv</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250611_145420-kzdbqkbv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-02, Schedule=long\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_161219-5s4ngm0g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/5s4ngm0g' target=\"_blank\">BS128_LR1e-01_WD1e-02_Sched_long</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/5s4ngm0g' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/5s4ngm0g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/65: train_acc=42.7%  train_loss=1.6083, val_acc=51.0%, val_loss=1.3646\n",
            "  Ep2/65: train_acc=59.5%  train_loss=1.1921, val_acc=30.9%, val_loss=2.2272\n",
            "  Ep3/65: train_acc=64.4%  train_loss=1.0205, val_acc=50.2%, val_loss=1.5006\n",
            "  Ep4/65: train_acc=64.6%  train_loss=1.0106, val_acc=25.6%, val_loss=3.0075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate   use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep5/65: train_acc=66.4%  train_loss=0.9625, val_acc=20.1%, val_loss=3.6842\n",
            "  Ep6/65: train_acc=67.5%  train_loss=0.9321, val_acc=34.0%, val_loss=2.1947\n",
            "  Ep7/65: train_acc=67.2%  train_loss=0.9362, val_acc=25.5%, val_loss=4.3641\n",
            "  Ep8/65: train_acc=66.2%  train_loss=0.9685, val_acc=31.7%, val_loss=2.6308\n",
            "  Ep9/65: train_acc=58.2%  train_loss=1.1181, val_acc=16.6%, val_loss=5.5139\n",
            "  Ep10/65: train_acc=62.9%  train_loss=1.0355, val_acc=18.1%, val_loss=6.7130\n",
            "  Ep11/65: train_acc=59.1%  train_loss=1.1391, val_acc=39.2%, val_loss=1.6384\n",
            "  Ep12/65: train_acc=61.2%  train_loss=1.0826, val_acc=28.0%, val_loss=2.9579\n",
            "  Ep13/65: train_acc=60.4%  train_loss=1.0905, val_acc=11.9%, val_loss=14.2652\n",
            "  Ep14/65: train_acc=54.3%  train_loss=1.1970, val_acc=25.4%, val_loss=1.9681\n",
            "  Ep15/65: train_acc=58.1%  train_loss=1.1376, val_acc=22.3%, val_loss=3.2624\n",
            "  Ep16/65: train_acc=51.5%  train_loss=1.2640, val_acc=19.1%, val_loss=4.7405\n",
            "  Ep17/65: train_acc=54.8%  train_loss=1.2473, val_acc=22.3%, val_loss=3.6928\n",
            "  Ep18/65: train_acc=51.8%  train_loss=1.3034, val_acc=21.8%, val_loss=3.2605\n",
            "  Ep19/65: train_acc=61.7%  train_loss=1.0747, val_acc=11.1%, val_loss=15.9194\n",
            "  Ep20/65: train_acc=47.9%  train_loss=1.3768, val_acc=25.8%, val_loss=2.3192\n",
            "  Ep21/65: train_acc=48.8%  train_loss=1.3235, val_acc=25.9%, val_loss=2.2379\n",
            "  Ep22/65: train_acc=48.6%  train_loss=1.3354, val_acc=14.9%, val_loss=4.6666\n",
            "  Ep23/65: train_acc=44.7%  train_loss=1.4265, val_acc=16.3%, val_loss=2.9263\n",
            "  Ep24/65: train_acc=40.2%  train_loss=1.5627, val_acc=11.8%, val_loss=2.5171\n",
            "  Ep25/65: train_acc=42.5%  train_loss=1.4689, val_acc=15.3%, val_loss=2.7763\n",
            "  Ep26/65: train_acc=41.2%  train_loss=1.5035, val_acc=13.1%, val_loss=2.6247\n",
            "  Ep27/65: train_acc=34.8%  train_loss=1.6637, val_acc=15.5%, val_loss=2.3060\n",
            "  Ep28/65: train_acc=34.6%  train_loss=1.6634, val_acc=9.3%, val_loss=42.2491\n",
            "  Ep29/65: train_acc=41.4%  train_loss=1.5062, val_acc=21.0%, val_loss=2.6573\n",
            "  Ep30/65: train_acc=42.8%  train_loss=1.4727, val_acc=11.3%, val_loss=4.0298\n",
            "  Ep31/65: train_acc=44.9%  train_loss=1.4205, val_acc=19.3%, val_loss=2.2595\n",
            "  Ep32/65: train_acc=45.5%  train_loss=1.4132, val_acc=19.7%, val_loss=2.6208\n",
            "  Ep33/65: train_acc=45.4%  train_loss=1.4059, val_acc=22.4%, val_loss=2.7617\n",
            "  Ep34/65: train_acc=45.6%  train_loss=1.3960, val_acc=15.4%, val_loss=8.5026\n",
            "  Ep35/65: train_acc=47.9%  train_loss=1.3934, val_acc=18.0%, val_loss=4.5462\n",
            "  Ep36/65: train_acc=56.8%  train_loss=1.2028, val_acc=45.0%, val_loss=1.4158\n",
            "  Ep37/65: train_acc=59.5%  train_loss=1.1304, val_acc=43.7%, val_loss=1.4669\n",
            "  Ep38/65: train_acc=60.6%  train_loss=1.1055, val_acc=46.6%, val_loss=1.4672\n",
            "  Ep39/65: train_acc=60.3%  train_loss=1.0977, val_acc=47.8%, val_loss=1.4107\n",
            "  Ep40/65: train_acc=61.6%  train_loss=1.0748, val_acc=60.7%, val_loss=1.1138\n",
            "  Ep41/65: train_acc=62.8%  train_loss=1.0506, val_acc=60.4%, val_loss=1.1512\n",
            "  Ep42/65: train_acc=64.0%  train_loss=1.0089, val_acc=44.6%, val_loss=1.7577\n",
            "  Ep43/65: train_acc=64.2%  train_loss=1.0085, val_acc=39.6%, val_loss=1.8613\n",
            "  Ep44/65: train_acc=65.1%  train_loss=0.9899, val_acc=53.7%, val_loss=1.3787\n",
            "  Ep45/65: train_acc=65.1%  train_loss=0.9769, val_acc=59.0%, val_loss=1.0895\n",
            "  Ep46/65: train_acc=65.0%  train_loss=0.9766, val_acc=49.4%, val_loss=1.5430\n",
            "  Ep47/65: train_acc=66.1%  train_loss=0.9540, val_acc=60.5%, val_loss=1.1060\n",
            "  Ep48/65: train_acc=66.2%  train_loss=0.9483, val_acc=48.6%, val_loss=1.5924\n",
            "  Ep49/65: train_acc=67.2%  train_loss=0.9301, val_acc=25.7%, val_loss=2.1782\n",
            "  Ep50/65: train_acc=67.0%  train_loss=0.9273, val_acc=26.1%, val_loss=2.5492\n",
            "  Ep51/65: train_acc=67.4%  train_loss=0.9230, val_acc=51.4%, val_loss=1.4077\n",
            "  Ep52/65: train_acc=67.7%  train_loss=0.9145, val_acc=53.1%, val_loss=1.5065\n",
            "  Ep53/65: train_acc=67.7%  train_loss=0.9191, val_acc=44.9%, val_loss=1.6138\n",
            "  Ep54/65: train_acc=66.9%  train_loss=0.9225, val_acc=42.1%, val_loss=1.8165\n",
            "  Ep55/65: train_acc=67.4%  train_loss=0.9168, val_acc=40.3%, val_loss=1.9971\n",
            "  Ep56/65: train_acc=68.1%  train_loss=0.9101, val_acc=29.5%, val_loss=2.7967\n",
            "  Ep57/65: train_acc=67.6%  train_loss=0.9180, val_acc=32.9%, val_loss=2.4368\n",
            "  Ep58/65: train_acc=68.0%  train_loss=0.9051, val_acc=64.1%, val_loss=0.9952\n",
            "  Ep59/65: train_acc=68.5%  train_loss=0.8884, val_acc=57.7%, val_loss=1.1706\n",
            "  Ep60/65: train_acc=68.8%  train_loss=0.8880, val_acc=29.9%, val_loss=2.7954\n",
            "  Ep61/65: train_acc=68.7%  train_loss=0.8929, val_acc=18.6%, val_loss=4.6316\n",
            "  Ep62/65: train_acc=68.3%  train_loss=0.9044, val_acc=46.2%, val_loss=1.5179\n",
            "  Ep63/65: train_acc=69.4%  train_loss=0.8715, val_acc=55.9%, val_loss=1.2645\n",
            "  Ep64/65: train_acc=68.7%  train_loss=0.8923, val_acc=53.8%, val_loss=1.2820\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep65/65: train_acc=69.2%  train_loss=0.8709, val_acc=14.2%, val_loss=3.6229\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>▃▅▆████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▇▇▇█▆▆▆▅▆▄▄▄▃▂▂▁▂▃▃▃▄▆▆▆▇▇▇██▇█████████</td></tr><tr><td>train_loss</td><td>█▄▂▃▂▃▄▃▄▄▃▅▅▆▇█▇▆▆▆▆▆▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▇▄▇▃▄▄▂▂▄▁▃▃▃▁▃▂▂▁▃▂▃▂▆▆▆▇█▆█▆▇▇▆▄▄▄▂▆▇▂</td></tr><tr><td>val_loss</td><td>▁▁▁▂▁▁▁▃▁▁▁▁▂▁▁▁█▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>65</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_acc</td><td>69.25</td></tr><tr><td>train_loss</td><td>0.87086</td></tr><tr><td>val_acc</td><td>14.22222</td></tr><tr><td>val_loss</td><td>3.62286</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS128_LR1e-01_WD1e-02_Sched_long</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/5s4ngm0g' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/5s4ngm0g</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250611_161219-5s4ngm0g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-04, Schedule=short\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_182744-4rrto0ge</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/4rrto0ge' target=\"_blank\">BS128_LR1e-01_WD1e-04_Sched_short</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/4rrto0ge' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/4rrto0ge</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/16: train_acc=38.1%  train_loss=2.1847, val_acc=11.1%, val_loss=10448.7089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate   use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep2/16: train_acc=45.9%  train_loss=1.8155, val_acc=56.2%, val_loss=1.2262\n",
            "  Ep3/16: train_acc=59.6%  train_loss=1.1339, val_acc=64.7%, val_loss=1.0732\n",
            "  Ep4/16: train_acc=67.2%  train_loss=0.9207, val_acc=67.3%, val_loss=0.9351\n",
            "  Ep5/16: train_acc=72.0%  train_loss=0.7965, val_acc=56.3%, val_loss=1.5961\n",
            "  Ep6/16: train_acc=77.6%  train_loss=0.6454, val_acc=82.9%, val_loss=0.5079\n",
            "  Ep7/16: train_acc=80.9%  train_loss=0.5317, val_acc=80.5%, val_loss=0.5841\n",
            "  Ep8/16: train_acc=81.9%  train_loss=0.5151, val_acc=81.9%, val_loss=0.5437\n",
            "  Ep9/16: train_acc=82.3%  train_loss=0.4960, val_acc=84.4%, val_loss=0.4567\n",
            "  Ep10/16: train_acc=82.8%  train_loss=0.4812, val_acc=74.2%, val_loss=1.0151\n",
            "  Ep11/16: train_acc=83.5%  train_loss=0.4573, val_acc=85.3%, val_loss=0.4324\n",
            "  Ep12/16: train_acc=84.0%  train_loss=0.4483, val_acc=84.9%, val_loss=0.4347\n",
            "  Ep13/16: train_acc=84.2%  train_loss=0.4412, val_acc=85.4%, val_loss=0.4299\n",
            "  Ep14/16: train_acc=84.5%  train_loss=0.4345, val_acc=85.5%, val_loss=0.4285\n",
            "  Ep15/16: train_acc=84.7%  train_loss=0.4355, val_acc=85.3%, val_loss=0.4323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep16/16: train_acc=84.4%  train_loss=0.4346, val_acc=85.7%, val_loss=0.4224\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate</td><td>▇████▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▅▆▇▇█████████</td></tr><tr><td>train_loss</td><td>█▇▄▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▅████▇██████</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_acc</td><td>84.40741</td></tr><tr><td>train_loss</td><td>0.43463</td></tr><tr><td>val_acc</td><td>85.66667</td></tr><tr><td>val_loss</td><td>0.4224</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS128_LR1e-01_WD1e-04_Sched_short</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/4rrto0ge' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/4rrto0ge</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250611_182744-4rrto0ge/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-04, Schedule=medium\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_190232-eqy9fjvs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/eqy9fjvs' target=\"_blank\">BS128_LR1e-01_WD1e-04_Sched_medium</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/eqy9fjvs' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/eqy9fjvs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/37: train_acc=44.0%  train_loss=1.7020, val_acc=36.6%, val_loss=9.1951\n",
            "  Ep2/37: train_acc=61.4%  train_loss=1.2114, val_acc=43.3%, val_loss=2.0115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate   use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep3/37: train_acc=66.2%  train_loss=1.0039, val_acc=45.9%, val_loss=1.6876\n",
            "  Ep4/37: train_acc=73.6%  train_loss=0.7864, val_acc=69.5%, val_loss=1.1702\n",
            "  Ep5/37: train_acc=76.8%  train_loss=0.6768, val_acc=73.3%, val_loss=0.9381\n",
            "  Ep6/37: train_acc=80.4%  train_loss=0.5743, val_acc=76.5%, val_loss=0.7067\n",
            "  Ep7/37: train_acc=82.3%  train_loss=0.5254, val_acc=82.1%, val_loss=0.5694\n",
            "  Ep8/37: train_acc=83.6%  train_loss=0.4794, val_acc=77.6%, val_loss=0.7421\n",
            "  Ep9/37: train_acc=84.9%  train_loss=0.4460, val_acc=73.3%, val_loss=1.0088\n",
            "  Ep10/37: train_acc=85.9%  train_loss=0.4078, val_acc=85.8%, val_loss=0.4161\n",
            "  Ep11/37: train_acc=87.1%  train_loss=0.3703, val_acc=90.2%, val_loss=0.3453\n",
            "  Ep12/37: train_acc=87.4%  train_loss=0.3682, val_acc=88.9%, val_loss=0.3472\n",
            "  Ep13/37: train_acc=88.8%  train_loss=0.3297, val_acc=87.0%, val_loss=0.4397\n",
            "  Ep14/37: train_acc=90.0%  train_loss=0.2982, val_acc=90.4%, val_loss=0.2903\n",
            "  Ep15/37: train_acc=90.5%  train_loss=0.2860, val_acc=68.9%, val_loss=2.2262\n",
            "  Ep16/37: train_acc=90.6%  train_loss=0.2816, val_acc=85.9%, val_loss=0.4667\n",
            "  Ep17/37: train_acc=91.3%  train_loss=0.2575, val_acc=90.1%, val_loss=0.2985\n",
            "  Ep18/37: train_acc=91.3%  train_loss=0.2592, val_acc=85.4%, val_loss=0.6136\n",
            "  Ep19/37: train_acc=91.8%  train_loss=0.2422, val_acc=86.2%, val_loss=0.4893\n",
            "  Ep20/37: train_acc=92.4%  train_loss=0.2233, val_acc=88.7%, val_loss=0.4145\n",
            "  Ep21/37: train_acc=93.3%  train_loss=0.1984, val_acc=95.6%, val_loss=0.1277\n",
            "  Ep22/37: train_acc=95.4%  train_loss=0.1356, val_acc=96.3%, val_loss=0.1254\n",
            "  Ep23/37: train_acc=95.9%  train_loss=0.1229, val_acc=95.9%, val_loss=0.1271\n",
            "  Ep24/37: train_acc=96.0%  train_loss=0.1198, val_acc=96.3%, val_loss=0.1326\n",
            "  Ep25/37: train_acc=96.1%  train_loss=0.1187, val_acc=96.4%, val_loss=0.1171\n",
            "  Ep26/37: train_acc=96.0%  train_loss=0.1139, val_acc=96.3%, val_loss=0.1582\n",
            "  Ep27/37: train_acc=96.4%  train_loss=0.1080, val_acc=96.3%, val_loss=0.1466\n",
            "  Ep28/37: train_acc=96.4%  train_loss=0.1043, val_acc=96.5%, val_loss=0.1103\n",
            "  Ep29/37: train_acc=96.5%  train_loss=0.1029, val_acc=96.6%, val_loss=0.1163\n",
            "  Ep30/37: train_acc=96.6%  train_loss=0.1003, val_acc=96.5%, val_loss=0.1220\n",
            "  Ep31/37: train_acc=96.7%  train_loss=0.0985, val_acc=96.8%, val_loss=0.1356\n",
            "  Ep32/37: train_acc=96.6%  train_loss=0.0966, val_acc=96.7%, val_loss=0.1096\n",
            "  Ep33/37: train_acc=96.6%  train_loss=0.0966, val_acc=96.9%, val_loss=0.1106\n",
            "  Ep34/37: train_acc=96.7%  train_loss=0.0943, val_acc=96.4%, val_loss=0.1103\n",
            "  Ep35/37: train_acc=96.7%  train_loss=0.0962, val_acc=96.7%, val_loss=0.1027\n",
            "  Ep36/37: train_acc=96.9%  train_loss=0.0926, val_acc=96.8%, val_loss=0.1003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep37/37: train_acc=96.9%  train_loss=0.0880, val_acc=96.7%, val_loss=0.1019\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▃▅██████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▂▅▅▆▆▆▅▇▇▇▇▇▅▇▇▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▂▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>learning_rate</td><td>0.01</td></tr><tr><td>train_acc</td><td>96.92593</td></tr><tr><td>train_loss</td><td>0.08799</td></tr><tr><td>val_acc</td><td>96.66667</td></tr><tr><td>val_loss</td><td>0.10193</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS128_LR1e-01_WD1e-04_Sched_medium</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/eqy9fjvs' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/eqy9fjvs</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250611_190232-eqy9fjvs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-04, Schedule=long\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_202253-i27gdmli</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/i27gdmli' target=\"_blank\">BS128_LR1e-01_WD1e-04_Sched_long</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/i27gdmli' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/i27gdmli</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/65: train_acc=43.3%  train_loss=1.5899, val_acc=51.4%, val_loss=1.6902\n",
            "  Ep2/65: train_acc=63.0%  train_loss=1.1428, val_acc=52.0%, val_loss=2.1884\n",
            "  Ep3/65: train_acc=62.8%  train_loss=1.1126, val_acc=60.8%, val_loss=1.1820\n",
            "  Ep4/65: train_acc=73.4%  train_loss=0.7824, val_acc=68.2%, val_loss=1.4160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate   use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep5/65: train_acc=73.1%  train_loss=0.7932, val_acc=63.3%, val_loss=1.3238\n",
            "  Ep6/65: train_acc=79.1%  train_loss=0.6106, val_acc=75.3%, val_loss=0.7291\n",
            "  Ep7/65: train_acc=79.8%  train_loss=0.5911, val_acc=81.2%, val_loss=0.5652\n",
            "  Ep8/65: train_acc=83.5%  train_loss=0.4828, val_acc=66.0%, val_loss=1.8607\n",
            "  Ep9/65: train_acc=85.6%  train_loss=0.4251, val_acc=82.4%, val_loss=0.5446\n",
            "  Ep10/65: train_acc=86.6%  train_loss=0.3972, val_acc=85.6%, val_loss=0.4184\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[32m      2\u001b[39m set_seed(SEED)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m best_cfg, _    = hyperparam_search(pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m bs, lr, wd     = best_cfg\n\u001b[32m      6\u001b[39m tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mhyperparam_search\u001b[39m\u001b[34m(pretrained)\u001b[39m\n\u001b[32m     59\u001b[39m wandb_run = wandb.init(\n\u001b[32m     60\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33meurosat-supervised-scratch-grid-search-lrsched\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_LR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_WD\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_Sched_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     }\n\u001b[32m     75\u001b[39m )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS_FOR_RUN):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched, DEVICE) \u001b[38;5;66;03m# Pass DEVICE to train_one_epoch\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Compute validation loss & accuracy\u001b[39;00m\n\u001b[32m     80\u001b[39m     model.eval()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, scheduler, device)\u001b[39m\n\u001b[32m    131\u001b[39m optimizer.step()\n\u001b[32m    132\u001b[39m scheduler.step()\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m total_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# Accumulate weighted by batch size\u001b[39;00m\n\u001b[32m    135\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m    136\u001b[39m total_samples += labels.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Main\n",
        "set_seed(SEED)\n",
        "\n",
        "best_cfg, _    = hyperparam_search(pretrained=False)\n",
        "bs, lr, wd     = best_cfg\n",
        "tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "# Retrain on TRAIN+VAL\n",
        "final_model, combined_ds = retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, NUM_EPOCHS)\n",
        "\n",
        "evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr)\n",
        "\n",
        "final_path = f\"models/eurosat_supervised_final_bs{bs}_lr{lr:.0e}_epcs{NUM_EPOCHS}.pth\"\n",
        "torch.save(final_model.state_dict(), final_path)\n",
        "print(f\"Final model saved to {final_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
