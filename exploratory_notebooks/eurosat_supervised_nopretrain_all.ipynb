{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1\n",
            "0.20.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, zipfile, urllib\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, SequentialLR, MultiStepLR\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "LOCAL_OR_COLAB = \"LOCAL\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 34\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TRAIN_FRAC = 0.8\n",
        "VAL_FRAC   = 0.1\n",
        "TEST_FRAC  = 0.1\n",
        "\n",
        "BATCH_SIZES = [128]  # Using a single batch size for simplicity\n",
        "LRS = [1e-4, 3e-4]\n",
        "\n",
        "GRID = product(\n",
        "    [0.1, 0.01],    # learning rate\n",
        "    [0.01, 0.0001]  # weight decay\n",
        ")\n",
        "\n",
        "TRAINING_SCHEDULES = {\n",
        "    \"short\": {\"p\": [750, 1500, 2250, 2500], \"w\": 200, \"unit\": \"steps\"},\n",
        "    \"medium\": {\"p\": [3000, 6000, 9000, 10000], \"w\": 500, \"unit\": \"steps\"},\n",
        "    \"long\": {\"p\": [30, 60, 80, 90], \"w\": 5, \"unit\": \"epochs\"}\n",
        "}\n",
        "\n",
        "\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "NUM_WORKERS = 4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully set to use GPU: 1 (Quadro RTX 6000)\n",
            "Final DEVICE variable is set to: cuda:1\n",
            "Current PyTorch default device: 0\n",
            "Current PyTorch default device (after set_device): 1\n",
            "Dummy tensor is on device: cuda:1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "TARGET_GPU_INDEX = 1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    if TARGET_GPU_INDEX < torch.cuda.device_count():\n",
        "        DEVICE = torch.device(f\"cuda:{TARGET_GPU_INDEX}\")\n",
        "        print(f\"Successfully set to use GPU: {TARGET_GPU_INDEX} ({torch.cuda.get_device_name(TARGET_GPU_INDEX)})\")\n",
        "    else:\n",
        "        print(f\"Error: Physical GPU {TARGET_GPU_INDEX} is not available. There are only {torch.cuda.device_count()} GPUs (0 to {torch.cuda.device_count() - 1}).\")\n",
        "        print(\"Falling back to CPU.\")\n",
        "        DEVICE = torch.device(\"CPU\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Falling back to CPU.\")\n",
        "    DEVICE = torch.device(\"CPU\")\n",
        "\n",
        "# --- Verification (Optional, but good to run to confirm) ---\n",
        "print(f\"Final DEVICE variable is set to: {DEVICE}\")\n",
        "if DEVICE.type == 'cuda':\n",
        "    print(f\"Current PyTorch default device: {torch.cuda.current_device()}\")\n",
        "\n",
        "    torch.cuda.set_device(TARGET_GPU_INDEX)\n",
        "    print(f\"Current PyTorch default device (after set_device): {torch.cuda.current_device()}\")\n",
        "\n",
        "\n",
        "dummy_tensor = torch.randn(2, 2)\n",
        "dummy_tensor_on_gpu = dummy_tensor.to(DEVICE)\n",
        "print(f\"Dummy tensor is on device: {dummy_tensor_on_gpu.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    indices = np.arange(total_count)\n",
        "    np.random.seed(SEED) # for reproducibility\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_split = int(0.8 * total_count)\n",
        "    val_split = int(0.9 * total_count)\n",
        "\n",
        "    train_idx = indices[:train_split]\n",
        "    val_idx = indices[train_split:val_split]\n",
        "    test_idx = indices[val_split:]\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "    base_tf = transforms.ToTensor()\n",
        "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "    labels = np.array(ds_all.targets)\n",
        "    num_classes = len(ds_all.classes)\n",
        "    total_count = len(ds_all)\n",
        "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
        "\n",
        "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
        "\n",
        "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
        "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
        "    print(f\"Computed mean: {mean}\")\n",
        "    print(f\"Computed std:  {std}\")\n",
        "\n",
        "\n",
        "\n",
        "    train_transform_augmented = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomApply([transforms.RandomRotation(angle) for angle in [0, 90, 180, 270]], p=1.0), # Apply one of 0, 90, 180, 270 rotations\n",
        "        transforms.RandomHorizontalFlip(p=0.5), # Randomly apply horizontal flip (50% chance)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.Resize(256), # Resize to 256x256\n",
        "        transforms.CenterCrop(224), # Perform a central crop of 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "    # Create datasets with the respective transformations\n",
        "    train_ds = datasets.ImageFolder(root=data_dir, transform=train_transform_augmented)\n",
        "    val_ds = datasets.ImageFolder(root=data_dir, transform=eval_transform)\n",
        "    test_ds = datasets.ImageFolder(root=data_dir, transform=eval_transform)\n",
        "\n",
        "    # Apply subsets to the transformed datasets\n",
        "    train_ds_subset = Subset(train_ds, train_idx)\n",
        "    val_ds_subset = Subset(val_ds, val_idx)\n",
        "    test_ds_subset = Subset(test_ds, test_idx)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_ds_subset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    val_loader   = DataLoader(val_ds_subset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    test_loader  = DataLoader(test_ds_subset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "    print(f\"Train/Val/Test splits: {len(train_ds_subset)}/{len(val_ds_subset)}/{len(test_ds_subset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lr_scheduler(optimizer, total_training_steps, schedule_cfg, steps_per_epoch):\n",
        "    \"\"\"\n",
        "    Builds the learning rate scheduler based on the specified schedule configuration.\n",
        "\n",
        "    Args:\n",
        "        optimizer: The PyTorch optimizer.\n",
        "        total_training_steps: Total number of optimization steps for the entire training.\n",
        "        schedule_cfg: Dictionary containing 'p', 'w', and 'unit' for the schedule.\n",
        "        steps_per_epoch: Number of optimization steps in one epoch.\n",
        "    \"\"\"\n",
        "    warmup_iters = schedule_cfg[\"w\"]\n",
        "    milestones = [] # Points at which LR drops\n",
        "\n",
        "    if schedule_cfg[\"unit\"] == \"steps\":\n",
        "        milestones = schedule_cfg[\"p\"]\n",
        "    elif schedule_cfg[\"unit\"] == \"epochs\":\n",
        "        # Convert epoch milestones to step milestones\n",
        "        milestones = [m * steps_per_epoch for m in schedule_cfg[\"p\"]]\n",
        "        warmup_iters = schedule_cfg[\"w\"] * steps_per_epoch # Convert warmup epochs to steps\n",
        "\n",
        "    # Linear warm-up scheduler\n",
        "    warmup_scheduler = LinearLR(optimizer, start_factor=1e-6, end_factor=1.0, total_iters=warmup_iters)\n",
        "\n",
        "    decay_scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        schedulers=[warmup_scheduler, decay_scheduler],\n",
        "        milestones=[warmup_iters]\n",
        "    )\n",
        "    return scheduler\n",
        "\n",
        "def hyperparam_search(pretrained=True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "\n",
        "    for bs, (lr, wd), schedule_name in product(BATCH_SIZES, GRID, TRAINING_SCHEDULES.keys()):\n",
        "\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd:.1e}, Schedule={schedule_name}\")\n",
        "\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs) # Assuming get_data_loaders is adapted for preprocessing\n",
        "\n",
        "\n",
        "        steps_per_epoch = len(tr_dl)\n",
        "\n",
        "        schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "        if schedule_cfg[\"unit\"] == \"steps\":\n",
        "\n",
        "            total_steps = max(schedule_cfg[\"p\"]) # This is the total number of steps for the scheduler's milestones.\n",
        "            NUM_EPOCHS_FOR_RUN = int(np.ceil(total_steps / steps_per_epoch)) + 1 # Add a buffer epoch\n",
        "        else: # schedule_cfg[\"unit\"] == \"epochs\"\n",
        "            total_epochs_from_schedule = max(schedule_cfg[\"p\"]) + schedule_cfg[\"w\"] # max 'p' + warmup epochs\n",
        "            NUM_EPOCHS_FOR_RUN = total_epochs_from_schedule # Total epochs to run\n",
        "            total_steps = NUM_EPOCHS_FOR_RUN * steps_per_epoch\n",
        "\n",
        "\n",
        "        model = build_model(n_cls, pretrained=pretrained)\n",
        "        model.to(DEVICE) # Move model to device\n",
        "\n",
        "        # Optimizer: SGD with momentum set to 0.9\n",
        "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        sched = build_lr_scheduler(opt, total_steps, schedule_cfg, steps_per_epoch)\n",
        "\n",
        "        wandb_run = wandb.init(\n",
        "            project=\"eurosat-supervised-scratch-grid-search-lrsched\",\n",
        "            name=f\"BS{bs}_LR{lr:.0e}_WD{wd:.0e}_Sched_{schedule_name}\",\n",
        "            config={\n",
        "                \"batch_size\": bs,\n",
        "                \"learning_rate\": lr,\n",
        "                \"weight_decay\": wd,\n",
        "                \"schedule_name\": schedule_name,\n",
        "                \"total_epochs_for_run\": NUM_EPOCHS_FOR_RUN,\n",
        "                \"pretrained\": pretrained,\n",
        "                \"optimizer\": \"SGD_momentum_0.9\",\n",
        "                \"scheduler_type\": \"LinearWarmup_MultiStepLR\",\n",
        "                \"warmup_steps_or_epochs\": schedule_cfg[\"w\"],\n",
        "                \"decay_milestones\": schedule_cfg[\"p\"],\n",
        "                \"decay_unit\": schedule_cfg[\"unit\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        for ep in range(NUM_EPOCHS_FOR_RUN):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched, DEVICE) # Pass DEVICE to train_one_epoch\n",
        "            # Compute validation loss & accuracy\n",
        "            model.eval()\n",
        "            val_loss, corr, tot = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_dl:\n",
        "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                    logits = model(xb)\n",
        "                    loss = crit(logits, yb)\n",
        "                    val_loss += loss.item()\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                    corr += (preds == yb).sum().item()\n",
        "                    tot  += yb.size(0)\n",
        "            val_loss /= len(val_dl)\n",
        "            val_acc = 100.0 * corr / tot\n",
        "\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS_FOR_RUN}: train_acc={tr_acc:.1f}%  train_loss={tr_loss:.4f}, \"\n",
        "                  f\"val_acc={val_acc:.1f}%, val_loss={val_loss:.4f}\")\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\":       ep + 1,\n",
        "                \"train_loss\":  tr_loss,\n",
        "                \"train_acc\":   tr_acc,\n",
        "                \"val_loss\":    val_loss,\n",
        "                \"val_acc\":     val_acc,\n",
        "                \"learning_rate\": opt.param_groups[0]['lr'] # Log current LR\n",
        "            })\n",
        "\n",
        "        wandb_run.finish()\n",
        "\n",
        "        # Only use val_acc to pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val   = val_acc\n",
        "            best_cfg   = (bs, lr, wd, schedule_name)\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]:.1e}, Schedule={best_cfg[3]}, val_acc={best_val:.1f}%\")\n",
        "\n",
        "    return best_cfg, best_model\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step() # <--- IMPORTANT: Step the scheduler after each batch\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0) # Accumulate weighted by batch size\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = 100 * correct_predictions / total_samples\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# def compute_mean_std(dataset, batch_size):\n",
        "#     loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
        "#     mean = 0.0\n",
        "#     std = 0.0\n",
        "#     n_samples = 0\n",
        "\n",
        "#     for data, _ in loader:\n",
        "#         batch_samples = data.size(0)\n",
        "#         data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "#         mean += data.mean(2).sum(0)\n",
        "#         std += data.std(2).sum(0)\n",
        "#         n_samples += batch_samples\n",
        "\n",
        "#     mean /= n_samples\n",
        "#     std /= n_samples\n",
        "#     return mean.tolist(), std.tolist()\n",
        "\n",
        "# def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "#     base_tf = transforms.ToTensor()\n",
        "#     ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "#     labels = np.array(ds_all.targets)   # numpy array of shape (N,)\n",
        "#     num_classes = len(ds_all.classes)\n",
        "#     total_count = len(ds_all)\n",
        "#     print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
        "\n",
        "#     train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
        "\n",
        "#     train_subset_for_stats = Subset(ds_all, train_idx)\n",
        "#     mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
        "#     print(f\"Computed mean: {mean}\")\n",
        "#     print(f\"Computed std:  {std}\")\n",
        "\n",
        "#     tf_final = transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=mean, std=std)\n",
        "#     ])\n",
        "\n",
        "#     #  full ImageFolder but now with normalization baked in\n",
        "#     ds_all_norm = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
        "\n",
        "#     train_ds = Subset(ds_all_norm, train_idx)\n",
        "#     val_ds   = Subset(ds_all_norm, val_idx)\n",
        "#     test_ds  = Subset(ds_all_norm, test_idx)\n",
        "\n",
        "#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "#     val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "#     test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "#     print(f\"Train/Val/Test splits: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
        "\n",
        "#     return train_loader, val_loader, test_loader, num_classes\n",
        "\n",
        "def get_proportion(num_classes, dataset):\n",
        "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
        "    n_temp = total_count - n_train   # this is val + test\n",
        "\n",
        "    sss1 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_train,\n",
        "        test_size=n_temp,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    # Train and temp(val+test) indices\n",
        "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
        "\n",
        "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
        "    n_test = total_count - n_train - n_val\n",
        "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
        "\n",
        "    labels_temp = labels[temp_idx]\n",
        "\n",
        "    sss2 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_val,\n",
        "        test_size=n_test,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
        "\n",
        "    val_idx = temp_idx[val_idx_in_temp]\n",
        "    test_idx = temp_idx[test_idx_in_temp]\n",
        "\n",
        "    assert len(train_idx) == n_train\n",
        "    assert len(val_idx) == n_val\n",
        "    assert len(test_idx) == n_test\n",
        "\n",
        "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
        "    return train_idx,val_idx,test_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet50(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "# def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "#     model.train()\n",
        "#     tot_loss, corr, tot = 0.0, 0, 0\n",
        "#     for xb, yb in loader:\n",
        "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "#         opt.zero_grad()\n",
        "#         logits = model(xb)\n",
        "\n",
        "#         loss   = crit(logits, yb)\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#         if sched: sched.step()\n",
        "#         tot_loss += loss.item()\n",
        "#         preds    = logits.argmax(dim=1)\n",
        "#         corr    += (preds==yb).sum().item()\n",
        "#         tot     += yb.size(0)\n",
        "#         avg_loss = tot_loss / len(loader)\n",
        "\n",
        "#     avg_loss = tot_loss / len(loader)\n",
        "#     acc = 100.0 * corr / tot\n",
        "#     return avg_loss, acc\n",
        "\n",
        "def evaluate(model, loader, num_classes):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    correct_per_class = torch.zeros(num_classes, dtype=torch.int64)\n",
        "    total_per_class   = torch.zeros(num_classes, dtype=torch.int64)\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            preds  = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "            total_correct += (preds == yb).sum().item()\n",
        "            total_samples += yb.size(0)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                # mask of samples in this batch whose true label == c\n",
        "                class_mask = (yb == c)\n",
        "                if class_mask.sum().item() == 0:\n",
        "                    continue\n",
        "\n",
        "                total_per_class[c] += class_mask.sum().item()\n",
        "\n",
        "                correct_per_class[c] += ((preds == yb) & class_mask).sum().item()\n",
        "\n",
        "    overall_acc = 100.0 * total_correct / total_samples\n",
        "\n",
        "    acc_per_class = {}\n",
        "    for c in range(num_classes):\n",
        "        if total_per_class[c].item() > 0:\n",
        "            acc = 100.0 * correct_per_class[c].item() / total_per_class[c].item()\n",
        "        else:\n",
        "            acc = 0.0\n",
        "        acc_per_class[c] = acc\n",
        "\n",
        "    return overall_acc, acc_per_class, all_labels, all_preds\n",
        "\n",
        "def plot_confusion_matrix_from_preds(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # normalize by true-label counts (row‐wise) to get percentages\n",
        "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    \n",
        "    ticks = np.arange(len(class_names))\n",
        "    plt.xticks(ticks, class_names, rotation=90)\n",
        "    plt.yticks(ticks, class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    # threshold for text color\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            pct = cm_norm[i, j] * 100\n",
        "            plt.text(\n",
        "                j, i,\n",
        "                f\"{cm[i, j]}\\n{pct:.1f}%\",\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
        "            )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_class_acc_prop(te_dl, acc_vals, class_proportions_test):\n",
        "    classes = te_dl.dataset.dataset.classes\n",
        "    x = np.arange(len(classes))\n",
        "\n",
        "    acc   = acc_vals\n",
        "    prop  = class_proportions_test * 100\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "    bars = ax1.bar(x, acc, color='C0', alpha=0.7)\n",
        "    ax1.set_ylabel('Accuracy (%)', color='C0')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.tick_params(axis='y', labelcolor='C0')\n",
        "\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}%', ha='center', va='bottom', color='C0')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    line = ax2.plot(x, prop, color='C1', marker='o', linewidth=2)\n",
        "    ax2.set_ylabel('Test Proportion (%)', color='C1')\n",
        "    ax2.set_ylim(0, max(prop)*1.2)\n",
        "    ax2.tick_params(axis='y', labelcolor='C1')\n",
        "\n",
        "    for xi, yi in zip(x, prop):\n",
        "        ax2.text(xi, yi + max(prop)*0.02, f'{yi:.1f}%', ha='center', va='bottom', color='C1')\n",
        "\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
        "    plt.title('Per-class Accuracy vs. Test Proportion')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# def hyperparam_search(pretrained=False):\n",
        "#     best_val = -1.0\n",
        "#     best_cfg = None\n",
        "#     best_model = None\n",
        "\n",
        "#     for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "\n",
        "#         print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}\")\n",
        "        \n",
        "#         tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "#         model = build_model(n_cls, pretrained=pretrained)\n",
        "        \n",
        "#         total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "#         warmup_steps = len(tr_dl)\n",
        "#         opt = optim.AdamW(model.parameters(), lr=lr, betas=BETAS, eps=float(EPS), weight_decay=wd)\n",
        "#         sched = SequentialLR(\n",
        "#             opt,\n",
        "#             schedulers=[\n",
        "#                 LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "#                 CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "#             ],\n",
        "#             milestones=[warmup_steps]\n",
        "#         )\n",
        "#         crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Start a W&B run\n",
        "#         wandb_run = wandb.init(\n",
        "#             project=\"eurosat-supervised-scratch-grid-search\",\n",
        "#             name=f\"BS{bs}_LR{lr:.0e}_TR{TRAIN_FRAC}\",\n",
        "#             config={\n",
        "#                 \"batch_size\": bs,\n",
        "#                 \"learning_rate\": lr,\n",
        "#                 \"epochs\": NUM_EPOCHS,\n",
        "#                 \"pretrained\": pretrained,\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         for ep in range(NUM_EPOCHS):\n",
        "#             tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "#             # Compute validation loss & accuracy in one pass\n",
        "#             model.eval()\n",
        "#             val_loss, corr, tot = 0.0, 0, 0\n",
        "#             with torch.no_grad():\n",
        "#                 for xb, yb in val_dl:\n",
        "#                     xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "#                     logits = model(xb)\n",
        "#                     loss = crit(logits, yb)\n",
        "#                     val_loss += loss.item()\n",
        "#                     preds = logits.argmax(dim=1)\n",
        "#                     corr += (preds == yb).sum().item()\n",
        "#                     tot  += yb.size(0)\n",
        "#             val_loss /= len(val_dl)\n",
        "#             val_acc = 100.0 * corr / tot\n",
        "\n",
        "#             print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train_acc={tr_acc:.1f}%  train_loss={tr_loss:.4f}, \"\n",
        "#                   f\"val_acc={val_acc:.1f}%, val_loss={val_loss:.4f}\")\n",
        "\n",
        "#             wandb.log({\n",
        "#                 \"epoch\":       ep + 1,\n",
        "#                 \"train_loss\":  tr_loss,\n",
        "#                 \"train_acc\":   tr_acc,\n",
        "#                 \"val_loss\":    val_loss,\n",
        "#                 \"val_acc\":     val_acc\n",
        "#             })\n",
        "\n",
        "#         wandb_run.finish()\n",
        "\n",
        "#         # Only use val_acc to pick best\n",
        "#         if val_acc > best_val:\n",
        "#             best_val   = val_acc\n",
        "#             best_cfg   = (bs, lr, wd)\n",
        "#             best_model = copy.deepcopy(model)\n",
        "\n",
        "#     print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, val_acc={best_val:.1f}%\")\n",
        "    \n",
        "#     return best_cfg, best_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform Hyperparameter Search, Retrain on Train + Validation Set, Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Assuming build_lr_scheduler and TRAINING_SCHEDULES are defined as before\n",
        "\n",
        "# Alternative (better) make_optimizer_scheduler using the helper\n",
        "def make_optimizer_scheduler_reused(params, lr, wd, schedule_name, steps_per_epoch):\n",
        "    \"\"\"\n",
        "    Builds the SGD optimizer and the specific learning rate scheduler\n",
        "    by reusing the build_lr_scheduler function.\n",
        "\n",
        "    Args:\n",
        "        params: Model parameters.\n",
        "        lr: Learning rate.\n",
        "        wd: Weight decay.\n",
        "        schedule_name: Name of the training schedule ('short', 'medium', 'long').\n",
        "        steps_per_epoch: Number of optimization steps in one epoch.\n",
        "    \"\"\"\n",
        "    opt = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "    # We need to provide a `total_training_steps` to build_lr_scheduler,\n",
        "    # though MultiStepLR doesn't strictly use it beyond its milestones.\n",
        "    # For consistency, we can pass the max step from milestones or a very large number.\n",
        "    # Let's pass the max of the milestones as an effective 'total_steps' for the schedule.\n",
        "    # The actual NUM_EPOCHS for training will be determined by the schedule logic.\n",
        "    total_steps_for_scheduler_config = max(schedule_cfg['p']) if schedule_cfg['unit'] == 'steps' else max(schedule_cfg['p']) * steps_per_epoch\n",
        "\n",
        "    scheduler = build_lr_scheduler(opt, total_steps_for_scheduler_config, schedule_cfg, steps_per_epoch)\n",
        "    return opt, scheduler\n",
        "\n",
        "\n",
        "# Assuming build_model, train_one_epoch, DEVICE, and TRAINING_SCHEDULES are defined\n",
        "\n",
        "def retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, schedule_name): # Added schedule_name\n",
        "\n",
        "    print(\"\\n>>> Retraining final model on TRAIN+VAL combined with best hyperparameters\")\n",
        "    combined_ds = ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "\n",
        "    # Important: The combined_dl needs to use the same training transforms as tr_dl.\n",
        "    # If tr_dl.dataset is already a Subset, and the base dataset had the transform, this is fine.\n",
        "    # If the Subset itself wraps a dataset with a transform, it's correct.\n",
        "    # Ensure the combined_ds has the correct (training) transformations applied implicitly or explicitly.\n",
        "    # In your previous `get_data_loaders`, `train_ds_subset` was created from `train_ds` which had `train_transform_augmented`.\n",
        "    # So `tr_dl.dataset` and `val_dl.dataset` already have their respective transforms.\n",
        "    # When combining, you might need to re-apply the *training* transforms if the validation set was not augmented.\n",
        "    # However, for retraining on combined data, you typically use the TRAINING augmentations.\n",
        "    # The current `tr_dl.dataset` and `val_dl.dataset` come with their *original* transforms.\n",
        "    # If `val_dl.dataset` used `eval_transform`, it won't have random augmentations.\n",
        "    # For retraining, it's common to use the TRAINING transform on ALL data.\n",
        "    # This might require creating a new dataset from raw data with `train_transform_augmented`\n",
        "    # and then subsetting it.\n",
        "\n",
        "    # Let's assume for simplicity that the datasets wrapped by tr_dl.dataset and val_dl.dataset\n",
        "    # are suitable for concatenation directly and will effectively be augmented\n",
        "    # as per the training augmentation during loading. This is usually handled if the\n",
        "    # original ImageFolder was created with the proper transform before subsetting.\n",
        "\n",
        "    combined_dl = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=4) # Assuming 4 workers\n",
        "\n",
        "    model = build_model(n_cls, pretrained=False)\n",
        "    model.to(DEVICE) # Move model to device\n",
        "\n",
        "    # Determine total epochs for this specific schedule\n",
        "    steps_per_epoch = len(combined_dl)\n",
        "    schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "    if schedule_cfg[\"unit\"] == \"steps\":\n",
        "        total_steps_for_run = max(schedule_cfg[\"p\"]) # Run for at least the last milestone\n",
        "        num_epochs_for_run = int(np.ceil(total_steps_for_run / steps_per_epoch)) + 1 # Add buffer\n",
        "    else: # schedule_cfg[\"unit\"] == \"epochs\"\n",
        "        num_epochs_for_run = max(schedule_cfg[\"p\"]) + schedule_cfg[\"w\"]\n",
        "\n",
        "\n",
        "    # Use the new make_optimizer_scheduler\n",
        "    optimizer, scheduler = make_optimizer_scheduler_reused( # Changed function name\n",
        "        model.parameters(), lr, wd, schedule_name, steps_per_epoch\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs_for_run): # Changed num_epochs to num_epochs_for_run\n",
        "        loss, acc = train_one_epoch(model, combined_dl, optimizer, criterion, scheduler, DEVICE) # Pass DEVICE\n",
        "        print(f\"  Ep {ep+1}/{num_epochs_for_run}: train_acc={acc:.1f}%\")\n",
        "    return model, combined_ds\n",
        "\n",
        "#  Evaluate & log to wandb\n",
        "def evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr):\n",
        "    \"\"\"\n",
        "    Evaluate on test set, print per-class stats, log to wandb, and plot.\n",
        "    \"\"\"\n",
        "    final_test_acc, acc_per_class, y_true, y_pred = evaluate(final_model, te_dl, n_cls)\n",
        "    plot_confusion_matrix_from_preds(y_true, y_pred, te_dl.dataset.dataset.classes)\n",
        "\n",
        "    test_targs = np.array(te_dl.dataset.dataset.targets)[te_dl.dataset.indices]\n",
        "    prop_test = np.bincount(test_targs, minlength=n_cls) / len(test_targs)\n",
        "\n",
        "    combined_targs = np.concatenate([\n",
        "        np.array(ds.dataset.targets)[ds.indices] for ds in combined_ds.datasets\n",
        "    ])\n",
        "    prop_trainval = np.bincount(combined_targs, minlength=n_cls) / len(combined_targs)\n",
        "\n",
        "    acc_vals = np.array([acc_per_class[c] for c in range(n_cls)])\n",
        "    weighted_acc = (acc_vals * prop_test).sum()\n",
        "\n",
        "    print(\"\\n>>> Final Test Accuracy:\")\n",
        "    print(f\"  Overall:             {final_test_acc:5.1f}%\")\n",
        "    print(f\"  Weighted class acc.: {weighted_acc:5.1f}%\\n\")\n",
        "    hdr = f\"{'Class':20s}  {'Acc':>6s}   {'Train+Val':>9s}   {'Test':>6s}\"\n",
        "    print(hdr); print(\"-\"*len(hdr))\n",
        "    for c, name in enumerate(te_dl.dataset.dataset.classes):\n",
        "        print(f\"{name:20s}  {acc_vals[c]:6.1f}%   {prop_trainval[c]*100:8.0f}%   {prop_test[c]*100:6.0f}%\")\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"eurosat-supervised-scratch-final-lrsched\",\n",
        "        name=f\"BS{bs}_LR{lr:.0e}_final\",\n",
        "        config={\n",
        "            \"batch_size\": bs, \"learning_rate\": lr, \"epochs\": NUM_EPOCHS,\n",
        "            \"pretrained\": False, \"final_retrain\": True\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"final_test_acc\":     final_test_acc,\n",
        "        \"weighted_class_acc\": weighted_acc,\n",
        "        \"per_class_acc\":      acc_vals\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    plot_class_acc_prop(te_dl, acc_vals, prop_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=128, LR=1.0e-01, WD=1.0e-02, Schedule=short\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed mean: [0.3441525995731354, 0.3800968527793884, 0.407656192779541]\n",
            "Computed std:  [0.09124630689620972, 0.06498812139034271, 0.055154334753751755]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_133949-l8x9nqr4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/l8x9nqr4' target=\"_blank\">BS128_LR1e-01_WD1e-02_Sched_short</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/l8x9nqr4' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/l8x9nqr4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[32m      2\u001b[39m set_seed(SEED)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m best_cfg, _    = hyperparam_search(pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m bs, lr, wd     = best_cfg\n\u001b[32m      6\u001b[39m tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mhyperparam_search\u001b[39m\u001b[34m(pretrained)\u001b[39m\n\u001b[32m     68\u001b[39m wandb_run = wandb.init(\n\u001b[32m     69\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33meurosat-supervised-scratch-grid-search-lrsched\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     70\u001b[39m     name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_LR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_WD\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_Sched_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     }\n\u001b[32m     84\u001b[39m )\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS_FOR_RUN):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched, DEVICE) \u001b[38;5;66;03m# Pass DEVICE to train_one_epoch\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Compute validation loss & accuracy\u001b[39;00m\n\u001b[32m     89\u001b[39m     model.eval()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, scheduler, device)\u001b[39m\n\u001b[32m    140\u001b[39m optimizer.step()\n\u001b[32m    141\u001b[39m scheduler.step() \u001b[38;5;66;03m# <--- IMPORTANT: Step the scheduler after each batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m total_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# Accumulate weighted by batch size\u001b[39;00m\n\u001b[32m    144\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m    145\u001b[39m total_samples += labels.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Main\n",
        "set_seed(SEED)\n",
        "\n",
        "best_cfg, _    = hyperparam_search(pretrained=False)\n",
        "bs, lr, wd     = best_cfg\n",
        "tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "# Retrain on TRAIN+VAL\n",
        "final_model, combined_ds = retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, NUM_EPOCHS)\n",
        "\n",
        "evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr)\n",
        "\n",
        "final_path = f\"models/eurosat_supervised_final_bs{bs}_lr{lr:.0e}_epcs{NUM_EPOCHS}.pth\"\n",
        "torch.save(final_model.state_dict(), final_path)\n",
        "print(f\"Final model saved to {final_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
