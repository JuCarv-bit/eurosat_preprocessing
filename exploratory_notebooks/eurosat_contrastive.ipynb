{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementation of the SIMCLR with resnet50 backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBlEnEB7Rzfu",
        "outputId": "f0106383-237e-475e-d660-715dcc591a87"
      },
      "outputs": [],
      "source": [
        "# =========== GLOBAL CONFIGURATION ===========\n",
        "import os\n",
        "import ssl\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
        "from torchvision.models import resnet50\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Prevent nondeterminism\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "# torch.backends.cudnn.enabled = False\n",
        "\n",
        "CONFIG = {\n",
        "    \"LOCAL_OR_COLAB\": \"LOCAL\",\n",
        "    \"DATA_DIR_LOCAL\": \"/users/c/carvalhj/datasets/EuroSAT_RGB/\",\n",
        "    \"DATA_DIR_COLAB\": \"/content/EuroSAT_RGB\",\n",
        "    \"ZIP_PATH\": \"/content/EuroSAT.zip\",\n",
        "    \"EUROSAT_URL\": \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\",\n",
        "    \"SEED\": 42,  # Default seed (will be overridden per run)\n",
        "    \"BATCH_SIZE\": 256,\n",
        "    \"LR\": 1e-4,\n",
        "    \"LR_LINEAR\": 1e-4,\n",
        "    \"EPOCHS_SIMCLR\": 100,\n",
        "    \"EPOCHS_LINEAR\": 100,\n",
        "    \"PROJ_DIM\": 128,\n",
        "    \"FEATURE_DIM\": 2048, # ResNet50 feature dimension = 2048\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = 0.6\n",
        "VAL_FRAC   = 0.2\n",
        "TEST_FRAC  = 0.2\n",
        "\n",
        "PRETRAINED = False\n",
        "\n",
        "TEMPERATURE = 0.1\n",
        "\n",
        "LINEAR_PROB_TRAIN_SPLIT = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def prepare_data():\n",
        "    if CONFIG[\"LOCAL_OR_COLAB\"] == \"LOCAL\":\n",
        "        return CONFIG[\"DATA_DIR_LOCAL\"]\n",
        "\n",
        "    if not os.path.exists(CONFIG[\"DATA_DIR_COLAB\"]):\n",
        "        print(\"Downloading EuroSAT RGB...\")\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(CONFIG[\"EUROSAT_URL\"], CONFIG[\"ZIP_PATH\"])\n",
        "        with zipfile.ZipFile(CONFIG[\"ZIP_PATH\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", CONFIG[\"DATA_DIR_COLAB\"])\n",
        "        print(\"EuroSAT RGB dataset downloaded and extracted.\")\n",
        "    return CONFIG[\"DATA_DIR_COLAB\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def prepare_data():\n",
        "    if CONFIG[\"LOCAL_OR_COLAB\"] == \"LOCAL\":\n",
        "        return CONFIG[\"DATA_DIR_LOCAL\"]\n",
        "\n",
        "    if not os.path.exists(CONFIG[\"DATA_DIR_COLAB\"]):\n",
        "        print(\"Downloading EuroSAT RGB...\")\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(CONFIG[\"EUROSAT_URL\"], CONFIG[\"ZIP_PATH\"])\n",
        "        with zipfile.ZipFile(CONFIG[\"ZIP_PATH\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", CONFIG[\"DATA_DIR_COLAB\"])\n",
        "        print(\"EuroSAT RGB dataset downloaded and extracted.\")\n",
        "    return CONFIG[\"DATA_DIR_COLAB\"]\n",
        "\n",
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transform(x), self.base_transform(x)]\n",
        "    \n",
        "class SimCLRDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.dataset[idx]\n",
        "        x1, x2 = self.transform(x)\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - train_loader (SimCLR-style: yields (x1, x2) pairs)\n",
        "      - val_loader  (standard: yields (image, label))\n",
        "      - test_loader (standard: yields (image, label))\n",
        "      - num_classes (int)\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute channel‐wise mean/std on the training split (unaugmented).\n",
        "    # build a separate ImageFolder with `transform=ToTensor()`.\n",
        "    dataset_for_stats = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=transforms.ToTensor()\n",
        "    )\n",
        "    total_len = len(dataset_for_stats)\n",
        "    n_train = int(TRAIN_FRAC * total_len)\n",
        "    n_val   = int(VAL_FRAC   * total_len)\n",
        "    n_test  = total_len - n_train - n_val\n",
        "\n",
        "    train_for_stats, val_for_stats, test_for_stats = random_split(\n",
        "        dataset_for_stats,\n",
        "        [n_train, n_val, n_test]\n",
        "    )\n",
        "    # Compute mean/std on train_for_stats\n",
        "    mean, std = compute_mean_std(train_for_stats, batch_size)\n",
        "\n",
        "    # IMAGEFOLDER FOR TRAIN (NO TRANSFORM HERE)\n",
        "    # We want the SimCLR augmentations applied on‐the‐fly, so we leave transform=None.\n",
        "    dataset_train_no_transform = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_indices, val_indices, test_indices = random_split(\n",
        "        list(range(total_len)),\n",
        "        [n_train, n_val, n_test],\n",
        "        generator=generator\n",
        "    )\n",
        "    \n",
        "    all_indices = list(range(total_len))\n",
        "    rnd = random.Random(seed)\n",
        "    rnd.shuffle(all_indices)\n",
        "    train_indices = all_indices[:n_train]\n",
        "    val_indices   = all_indices[n_train : n_train + n_val]\n",
        "    test_indices  = all_indices[n_train + n_val : ]\n",
        "\n",
        "    # Now build Subsets pointing to dataset_train_no_transform:\n",
        "    train_subset_no_transform = Subset(dataset_train_no_transform, train_indices)\n",
        "\n",
        "    # 3) IMAGEFOLDER FOR VALIDATION/TEST (WITH EVAL TRANSFORM)\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.Resize(72),\n",
        "        transforms.CenterCrop(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "    dataset_eval = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=eval_transform\n",
        "    )\n",
        "    val_subset = Subset(dataset_eval, val_indices)\n",
        "    test_subset = Subset(dataset_eval, test_indices)\n",
        "\n",
        "    # DEFINE SIMCLR TRANSFORMS FOR TRAIN\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(64, scale=(0.5, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    simclr_transform = TwoCropsTransform(augment_transform)\n",
        "    train_ds_simclr = SimCLRDataset(train_subset_no_transform, simclr_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds_simclr,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    num_classes = len(dataset_for_stats.classes)\n",
        "    return train_loader, val_loader, test_loader, num_classes\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, proj_dim=128, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class SimCLRModel(nn.Module):\n",
        "    def __init__(self, base_encoder, proj_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = base_encoder\n",
        "        self.encoder.fc = nn.Identity()\n",
        "        self.projection_head = ProjectionHead(input_dim=CONFIG[\"FEATURE_DIM\"], proj_dim=proj_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.encoder(x)\n",
        "        proj = self.projection_head(feat)\n",
        "        return feat, proj\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature=0.5, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        N = zis.size(0)\n",
        "        z = F.normalize(torch.cat([zis, zjs], dim=0), dim=1)\n",
        "        sim = torch.matmul(z, z.T) / self.temperature\n",
        "        mask = torch.eye(2 * N, dtype=torch.bool).to(self.device)\n",
        "        sim = sim.masked_fill(mask, -1e9)\n",
        "        labels = torch.cat([torch.arange(N, 2 * N), torch.arange(0, N)]).to(self.device)\n",
        "        # return self.x(sim, labels)\n",
        "        return self.criterion(sim, labels)\n",
        "\n",
        "def train_simclr(model, loader, optimizer, criterion, device, epochs):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for (x1, x2) in loader:\n",
        "            x1, x2 = x1.to(device), x2.to(device)\n",
        "            _, z1 = model(x1)\n",
        "            _, z2 = model(x2)\n",
        "            loss = criterion(z1, z2)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg = total_loss / len(loader)\n",
        "        print(f\"[SimCLR] Epoch {epoch+1}/{epochs} - Loss: {avg:.4f}\")\n",
        "    print(\"Finished SimCLR pretraining.\")\n",
        "\n",
        "def evaluate(classifier, backbone, loader, device):\n",
        "    classifier.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = backbone(images)\n",
        "            outputs = classifier(features)\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return correct / total * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_linear_probe(backbone, train_loader, val_loader, device, epochs, lr, run_id):\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    base_ds = train_loader.dataset\n",
        "    while isinstance(base_ds, Subset):\n",
        "        base_ds = base_ds.dataset\n",
        "    num_classes = len(base_ds.classes)\n",
        "\n",
        "    # ── Build a single‐layer classifier on top of the frozen features ──\n",
        "    classifier = nn.Linear(CONFIG[\"FEATURE_DIM\"], num_classes).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        classifier.train()\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = backbone(images)\n",
        "            outputs = classifier(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_acc = correct / total * 100\n",
        "        val_acc = evaluate(classifier, backbone, val_loader, device)\n",
        "        print(f\"[Linear] Epoch {epoch+1}/{epochs} - \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    torch.save(classifier.state_dict(), f\"linear_probe_seed{run_id}.pth\")\n",
        "    return val_acc\n",
        "\n",
        "def load_evaluate_model(model_path, device, data_dir, seed):\n",
        "    results = []\n",
        "\n",
        "    backbone = resnet50(weights=None if not PRETRAINED else \"DEFAULT\")\n",
        "    backbone.fc = nn.Identity()  # same as in SimCLRModel\n",
        "    backbone.load_state_dict(torch.load(model_path), strict=False)\n",
        "    backbone.to(device)    \n",
        "    backbone.eval()\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # (Make sure 'data_dir' and 'seed' are in scope; if not, pass them in.)\n",
        "    _, _, test_loader, _ = get_data_loaders(data_dir, CONFIG[\"BATCH_SIZE\"])\n",
        "    print(f\"Starting linear probe on EuroSAT test split (seed={seed})...\")\n",
        "\n",
        "    # Split EuroSAT test‐subset into 80%/20% for probe‐train vs. probe‐val\n",
        "    full_test_ds = test_loader.dataset  # this is a Subset of dataset_eval\n",
        "    train_size = int(LINEAR_PROB_TRAIN_SPLIT * len(full_test_ds))\n",
        "    val_size = len(full_test_ds) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_test_ds, [train_size, val_size])\n",
        "\n",
        "    train_loader_from_test = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    val_loader_from_test = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    linear_probe_val_acc = train_linear_probe(\n",
        "        backbone,\n",
        "        train_loader_from_test,\n",
        "        val_loader_from_test,\n",
        "        DEVICE,\n",
        "        epochs=CONFIG[\"EPOCHS_LINEAR\"],\n",
        "        lr=CONFIG[\"LR_LINEAR\"],\n",
        "        run_id=seed\n",
        "    )\n",
        "    print(f\"[Linear‐Probe on EuroSAT test] Final Val Acc = {linear_probe_val_acc:.2f}%\\n\")\n",
        "\n",
        "    # ─── 4) Save & return ───────────────────────────────────────────\n",
        "    results.append({\n",
        "        \"seed\": seed,\n",
        "        \"val_acc\": linear_probe_val_acc\n",
        "    })\n",
        "    with open(\"linear_probe_results.txt\", \"a\") as f:\n",
        "        f.write(f\"Seed: {seed}, Val Acc: {linear_probe_val_acc:.2f}%\\n\")\n",
        "    print(\"Results saved to linear_probe_results.txt\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Starting run with seed 42 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/100 - Loss: 5.8148\n",
            "[SimCLR] Epoch 2/100 - Loss: 5.4270\n",
            "[SimCLR] Epoch 3/100 - Loss: 5.1051\n",
            "[SimCLR] Epoch 4/100 - Loss: 4.8714\n",
            "[SimCLR] Epoch 5/100 - Loss: 4.6871\n",
            "[SimCLR] Epoch 6/100 - Loss: 4.4835\n",
            "[SimCLR] Epoch 7/100 - Loss: 4.2467\n",
            "[SimCLR] Epoch 8/100 - Loss: 4.0137\n",
            "[SimCLR] Epoch 9/100 - Loss: 3.7987\n",
            "[SimCLR] Epoch 10/100 - Loss: 3.5824\n",
            "[SimCLR] Epoch 11/100 - Loss: 3.3657\n",
            "[SimCLR] Epoch 12/100 - Loss: 3.1135\n",
            "[SimCLR] Epoch 13/100 - Loss: 2.9641\n",
            "[SimCLR] Epoch 14/100 - Loss: 2.8231\n",
            "[SimCLR] Epoch 15/100 - Loss: 2.6822\n",
            "[SimCLR] Epoch 16/100 - Loss: 2.5939\n",
            "[SimCLR] Epoch 17/100 - Loss: 2.5121\n",
            "[SimCLR] Epoch 18/100 - Loss: 2.4416\n",
            "[SimCLR] Epoch 19/100 - Loss: 2.3567\n",
            "[SimCLR] Epoch 20/100 - Loss: 2.2716\n",
            "[SimCLR] Epoch 21/100 - Loss: 2.2032\n",
            "[SimCLR] Epoch 22/100 - Loss: 2.1246\n",
            "[SimCLR] Epoch 23/100 - Loss: 2.0770\n",
            "[SimCLR] Epoch 24/100 - Loss: 2.0192\n",
            "[SimCLR] Epoch 25/100 - Loss: 1.9564\n",
            "[SimCLR] Epoch 26/100 - Loss: 1.9347\n",
            "[SimCLR] Epoch 27/100 - Loss: 1.8626\n",
            "[SimCLR] Epoch 28/100 - Loss: 1.8335\n",
            "[SimCLR] Epoch 29/100 - Loss: 1.7793\n",
            "[SimCLR] Epoch 30/100 - Loss: 1.7017\n",
            "[SimCLR] Epoch 31/100 - Loss: 1.6820\n",
            "[SimCLR] Epoch 32/100 - Loss: 1.6702\n",
            "[SimCLR] Epoch 33/100 - Loss: 1.6320\n",
            "[SimCLR] Epoch 34/100 - Loss: 1.5995\n",
            "[SimCLR] Epoch 35/100 - Loss: 1.5573\n",
            "[SimCLR] Epoch 36/100 - Loss: 1.5265\n",
            "[SimCLR] Epoch 37/100 - Loss: 1.5149\n",
            "[SimCLR] Epoch 38/100 - Loss: 1.4857\n",
            "[SimCLR] Epoch 39/100 - Loss: 1.4657\n",
            "[SimCLR] Epoch 40/100 - Loss: 1.4254\n",
            "[SimCLR] Epoch 41/100 - Loss: 1.4426\n",
            "[SimCLR] Epoch 42/100 - Loss: 1.4043\n",
            "[SimCLR] Epoch 43/100 - Loss: 1.3428\n",
            "[SimCLR] Epoch 44/100 - Loss: 1.3437\n",
            "[SimCLR] Epoch 45/100 - Loss: 1.3608\n",
            "[SimCLR] Epoch 46/100 - Loss: 1.3292\n",
            "[SimCLR] Epoch 47/100 - Loss: 1.3030\n",
            "[SimCLR] Epoch 48/100 - Loss: 1.2840\n",
            "[SimCLR] Epoch 49/100 - Loss: 1.2588\n",
            "[SimCLR] Epoch 50/100 - Loss: 1.2529\n",
            "[SimCLR] Epoch 51/100 - Loss: 1.2319\n",
            "[SimCLR] Epoch 52/100 - Loss: 1.2077\n",
            "[SimCLR] Epoch 53/100 - Loss: 1.2005\n",
            "[SimCLR] Epoch 54/100 - Loss: 1.1912\n",
            "[SimCLR] Epoch 55/100 - Loss: 1.1649\n",
            "[SimCLR] Epoch 56/100 - Loss: 1.1511\n",
            "[SimCLR] Epoch 57/100 - Loss: 1.1397\n",
            "[SimCLR] Epoch 58/100 - Loss: 1.1355\n",
            "[SimCLR] Epoch 59/100 - Loss: 1.1148\n",
            "[SimCLR] Epoch 60/100 - Loss: 1.1145\n",
            "[SimCLR] Epoch 61/100 - Loss: 1.0992\n",
            "[SimCLR] Epoch 62/100 - Loss: 1.0852\n",
            "[SimCLR] Epoch 63/100 - Loss: 1.0743\n",
            "[SimCLR] Epoch 64/100 - Loss: 1.0648\n",
            "[SimCLR] Epoch 65/100 - Loss: 1.0538\n",
            "[SimCLR] Epoch 66/100 - Loss: 1.0245\n",
            "[SimCLR] Epoch 67/100 - Loss: 1.0205\n",
            "[SimCLR] Epoch 68/100 - Loss: 1.0072\n",
            "[SimCLR] Epoch 69/100 - Loss: 1.0256\n",
            "[SimCLR] Epoch 70/100 - Loss: 1.0137\n",
            "[SimCLR] Epoch 71/100 - Loss: 1.0043\n",
            "[SimCLR] Epoch 72/100 - Loss: 1.0000\n",
            "[SimCLR] Epoch 73/100 - Loss: 0.9726\n",
            "[SimCLR] Epoch 74/100 - Loss: 0.9716\n",
            "[SimCLR] Epoch 75/100 - Loss: 0.9640\n",
            "[SimCLR] Epoch 76/100 - Loss: 0.9612\n",
            "[SimCLR] Epoch 77/100 - Loss: 0.9632\n",
            "[SimCLR] Epoch 78/100 - Loss: 0.9304\n",
            "[SimCLR] Epoch 79/100 - Loss: 0.9281\n",
            "[SimCLR] Epoch 80/100 - Loss: 0.9247\n",
            "[SimCLR] Epoch 81/100 - Loss: 0.9209\n",
            "[SimCLR] Epoch 82/100 - Loss: 0.9133\n",
            "[SimCLR] Epoch 83/100 - Loss: 0.9085\n",
            "[SimCLR] Epoch 84/100 - Loss: 0.8917\n",
            "[SimCLR] Epoch 85/100 - Loss: 0.8842\n",
            "[SimCLR] Epoch 86/100 - Loss: 0.8788\n",
            "[SimCLR] Epoch 87/100 - Loss: 0.8710\n",
            "[SimCLR] Epoch 88/100 - Loss: 0.8535\n",
            "[SimCLR] Epoch 89/100 - Loss: 0.8646\n",
            "[SimCLR] Epoch 90/100 - Loss: 0.8530\n",
            "[SimCLR] Epoch 91/100 - Loss: 0.8633\n",
            "[SimCLR] Epoch 92/100 - Loss: 0.8483\n",
            "[SimCLR] Epoch 93/100 - Loss: 0.8314\n",
            "[SimCLR] Epoch 94/100 - Loss: 0.8362\n",
            "[SimCLR] Epoch 95/100 - Loss: 0.8406\n",
            "[SimCLR] Epoch 96/100 - Loss: 0.8301\n",
            "[SimCLR] Epoch 97/100 - Loss: 0.8200\n",
            "[SimCLR] Epoch 98/100 - Loss: 0.8181\n",
            "[SimCLR] Epoch 99/100 - Loss: 0.8064\n",
            "[SimCLR] Epoch 100/100 - Loss: 0.8057\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n"
          ]
        }
      ],
      "source": [
        "# =========== RUN EVERYTHING ===========\n",
        "\n",
        "# Define the list of seeds for each run\n",
        "seeds = [42]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
        "    set_seed(seed)\n",
        "    \n",
        "    data_dir = prepare_data()\n",
        "    train_loader, val_loader, test_loader, num_classes = get_data_loaders(data_dir, CONFIG[\"BATCH_SIZE\"])\n",
        "\n",
        "    # Initialize base encoder and SimCLR model\n",
        "    base_encoder = resnet50(weights=None if not PRETRAINED else \"DEFAULT\")\n",
        "    simclr_model = SimCLRModel(base_encoder, proj_dim=CONFIG[\"PROJ_DIM\"])\n",
        "    optimizer = optim.Adam(simclr_model.parameters(), lr=CONFIG[\"LR\"])\n",
        "    bs = CONFIG[\"BATCH_SIZE\"]\n",
        "    loss_fn = NTXentLoss(bs, temperature=TEMPERATURE, device=DEVICE)\n",
        "\n",
        "    print(\"Starting SimCLR training...\")\n",
        "    train_simclr(simclr_model, train_loader, optimizer, loss_fn, DEVICE, CONFIG[\"EPOCHS_SIMCLR\"])\n",
        "\n",
        "    print(\"Saving encoder...\")\n",
        "    torch.save(simclr_model.state_dict(), f\"simclr_model_seed{seed}_temperature{TEMPERATURE}_bs{bs}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_49968/1937333664.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  backbone.load_state_dict(torch.load(model_path), strict=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting linear probe on EuroSAT test split (seed=42)...\n",
            "[Linear] Epoch 1/100 - Train Acc: 9.54%, Val Acc: 7.13%\n",
            "[Linear] Epoch 2/100 - Train Acc: 9.51%, Val Acc: 13.15%\n",
            "[Linear] Epoch 3/100 - Train Acc: 11.44%, Val Acc: 12.22%\n",
            "[Linear] Epoch 4/100 - Train Acc: 15.05%, Val Acc: 14.91%\n",
            "[Linear] Epoch 5/100 - Train Acc: 18.89%, Val Acc: 14.91%\n",
            "[Linear] Epoch 6/100 - Train Acc: 19.88%, Val Acc: 17.87%\n",
            "[Linear] Epoch 7/100 - Train Acc: 21.78%, Val Acc: 19.72%\n",
            "[Linear] Epoch 8/100 - Train Acc: 23.56%, Val Acc: 25.83%\n",
            "[Linear] Epoch 9/100 - Train Acc: 24.44%, Val Acc: 19.54%\n",
            "[Linear] Epoch 10/100 - Train Acc: 25.14%, Val Acc: 20.83%\n",
            "[Linear] Epoch 11/100 - Train Acc: 24.86%, Val Acc: 27.78%\n",
            "[Linear] Epoch 12/100 - Train Acc: 26.46%, Val Acc: 26.76%\n",
            "[Linear] Epoch 13/100 - Train Acc: 26.11%, Val Acc: 21.20%\n",
            "[Linear] Epoch 14/100 - Train Acc: 26.97%, Val Acc: 27.13%\n",
            "[Linear] Epoch 15/100 - Train Acc: 27.78%, Val Acc: 25.19%\n",
            "[Linear] Epoch 16/100 - Train Acc: 28.73%, Val Acc: 24.35%\n",
            "[Linear] Epoch 17/100 - Train Acc: 28.61%, Val Acc: 25.00%\n",
            "[Linear] Epoch 18/100 - Train Acc: 29.79%, Val Acc: 29.44%\n",
            "[Linear] Epoch 19/100 - Train Acc: 31.16%, Val Acc: 31.11%\n",
            "[Linear] Epoch 20/100 - Train Acc: 30.67%, Val Acc: 34.07%\n",
            "[Linear] Epoch 21/100 - Train Acc: 32.78%, Val Acc: 31.20%\n",
            "[Linear] Epoch 22/100 - Train Acc: 32.96%, Val Acc: 30.65%\n",
            "[Linear] Epoch 23/100 - Train Acc: 32.06%, Val Acc: 28.61%\n",
            "[Linear] Epoch 24/100 - Train Acc: 32.22%, Val Acc: 28.80%\n",
            "[Linear] Epoch 25/100 - Train Acc: 33.70%, Val Acc: 33.15%\n",
            "[Linear] Epoch 26/100 - Train Acc: 34.61%, Val Acc: 31.39%\n",
            "[Linear] Epoch 27/100 - Train Acc: 34.26%, Val Acc: 31.94%\n",
            "[Linear] Epoch 28/100 - Train Acc: 33.43%, Val Acc: 31.30%\n",
            "[Linear] Epoch 29/100 - Train Acc: 34.61%, Val Acc: 31.94%\n",
            "[Linear] Epoch 30/100 - Train Acc: 35.49%, Val Acc: 34.63%\n",
            "[Linear] Epoch 31/100 - Train Acc: 35.49%, Val Acc: 35.74%\n",
            "[Linear] Epoch 32/100 - Train Acc: 35.14%, Val Acc: 35.19%\n",
            "[Linear] Epoch 33/100 - Train Acc: 37.20%, Val Acc: 36.11%\n",
            "[Linear] Epoch 34/100 - Train Acc: 36.44%, Val Acc: 39.35%\n",
            "[Linear] Epoch 35/100 - Train Acc: 36.32%, Val Acc: 29.44%\n",
            "[Linear] Epoch 36/100 - Train Acc: 37.36%, Val Acc: 28.80%\n",
            "[Linear] Epoch 37/100 - Train Acc: 36.02%, Val Acc: 35.56%\n",
            "[Linear] Epoch 38/100 - Train Acc: 38.24%, Val Acc: 35.37%\n",
            "[Linear] Epoch 39/100 - Train Acc: 37.57%, Val Acc: 37.22%\n",
            "[Linear] Epoch 40/100 - Train Acc: 39.63%, Val Acc: 37.41%\n",
            "[Linear] Epoch 41/100 - Train Acc: 39.81%, Val Acc: 35.93%\n",
            "[Linear] Epoch 42/100 - Train Acc: 38.98%, Val Acc: 38.24%\n",
            "[Linear] Epoch 43/100 - Train Acc: 40.09%, Val Acc: 40.28%\n",
            "[Linear] Epoch 44/100 - Train Acc: 40.79%, Val Acc: 38.33%\n",
            "[Linear] Epoch 45/100 - Train Acc: 39.91%, Val Acc: 38.61%\n",
            "[Linear] Epoch 46/100 - Train Acc: 40.67%, Val Acc: 39.26%\n",
            "[Linear] Epoch 47/100 - Train Acc: 40.25%, Val Acc: 40.56%\n",
            "[Linear] Epoch 48/100 - Train Acc: 42.25%, Val Acc: 41.57%\n",
            "[Linear] Epoch 49/100 - Train Acc: 41.55%, Val Acc: 38.70%\n",
            "[Linear] Epoch 50/100 - Train Acc: 42.27%, Val Acc: 38.61%\n",
            "[Linear] Epoch 51/100 - Train Acc: 41.50%, Val Acc: 40.46%\n",
            "[Linear] Epoch 52/100 - Train Acc: 42.04%, Val Acc: 40.37%\n",
            "[Linear] Epoch 53/100 - Train Acc: 42.25%, Val Acc: 41.76%\n",
            "[Linear] Epoch 54/100 - Train Acc: 43.45%, Val Acc: 42.31%\n",
            "[Linear] Epoch 55/100 - Train Acc: 43.59%, Val Acc: 43.61%\n",
            "[Linear] Epoch 56/100 - Train Acc: 44.72%, Val Acc: 43.89%\n",
            "[Linear] Epoch 57/100 - Train Acc: 44.42%, Val Acc: 41.48%\n",
            "[Linear] Epoch 58/100 - Train Acc: 44.51%, Val Acc: 40.56%\n",
            "[Linear] Epoch 59/100 - Train Acc: 44.12%, Val Acc: 43.15%\n",
            "[Linear] Epoch 60/100 - Train Acc: 44.38%, Val Acc: 42.69%\n",
            "[Linear] Epoch 61/100 - Train Acc: 45.07%, Val Acc: 40.65%\n",
            "[Linear] Epoch 62/100 - Train Acc: 44.40%, Val Acc: 40.74%\n",
            "[Linear] Epoch 63/100 - Train Acc: 44.93%, Val Acc: 39.63%\n",
            "[Linear] Epoch 64/100 - Train Acc: 44.77%, Val Acc: 43.98%\n",
            "[Linear] Epoch 65/100 - Train Acc: 44.65%, Val Acc: 41.48%\n",
            "[Linear] Epoch 66/100 - Train Acc: 44.79%, Val Acc: 41.94%\n",
            "[Linear] Epoch 67/100 - Train Acc: 43.96%, Val Acc: 42.96%\n",
            "[Linear] Epoch 68/100 - Train Acc: 45.44%, Val Acc: 39.54%\n",
            "[Linear] Epoch 69/100 - Train Acc: 44.68%, Val Acc: 38.52%\n",
            "[Linear] Epoch 70/100 - Train Acc: 47.38%, Val Acc: 45.00%\n",
            "[Linear] Epoch 71/100 - Train Acc: 47.15%, Val Acc: 41.76%\n",
            "[Linear] Epoch 72/100 - Train Acc: 46.60%, Val Acc: 42.31%\n",
            "[Linear] Epoch 73/100 - Train Acc: 47.31%, Val Acc: 41.20%\n",
            "[Linear] Epoch 74/100 - Train Acc: 47.31%, Val Acc: 43.98%\n",
            "[Linear] Epoch 75/100 - Train Acc: 47.43%, Val Acc: 46.48%\n",
            "[Linear] Epoch 76/100 - Train Acc: 47.34%, Val Acc: 43.61%\n",
            "[Linear] Epoch 77/100 - Train Acc: 47.89%, Val Acc: 44.81%\n",
            "[Linear] Epoch 78/100 - Train Acc: 49.24%, Val Acc: 42.87%\n",
            "[Linear] Epoch 79/100 - Train Acc: 47.18%, Val Acc: 44.35%\n",
            "[Linear] Epoch 80/100 - Train Acc: 48.31%, Val Acc: 46.02%\n",
            "[Linear] Epoch 81/100 - Train Acc: 48.38%, Val Acc: 43.61%\n",
            "[Linear] Epoch 82/100 - Train Acc: 48.31%, Val Acc: 43.43%\n",
            "[Linear] Epoch 83/100 - Train Acc: 48.63%, Val Acc: 44.07%\n",
            "[Linear] Epoch 84/100 - Train Acc: 47.18%, Val Acc: 45.83%\n",
            "[Linear] Epoch 85/100 - Train Acc: 47.20%, Val Acc: 42.31%\n",
            "[Linear] Epoch 86/100 - Train Acc: 49.35%, Val Acc: 46.30%\n",
            "[Linear] Epoch 87/100 - Train Acc: 49.86%, Val Acc: 46.20%\n",
            "[Linear] Epoch 88/100 - Train Acc: 48.56%, Val Acc: 45.37%\n",
            "[Linear] Epoch 89/100 - Train Acc: 49.28%, Val Acc: 43.61%\n",
            "[Linear] Epoch 90/100 - Train Acc: 48.56%, Val Acc: 45.93%\n",
            "[Linear] Epoch 91/100 - Train Acc: 48.70%, Val Acc: 44.81%\n",
            "[Linear] Epoch 92/100 - Train Acc: 50.28%, Val Acc: 41.11%\n",
            "[Linear] Epoch 93/100 - Train Acc: 48.19%, Val Acc: 40.93%\n",
            "[Linear] Epoch 94/100 - Train Acc: 48.80%, Val Acc: 45.56%\n",
            "[Linear] Epoch 95/100 - Train Acc: 50.56%, Val Acc: 48.33%\n",
            "[Linear] Epoch 96/100 - Train Acc: 50.74%, Val Acc: 44.72%\n",
            "[Linear] Epoch 97/100 - Train Acc: 49.68%, Val Acc: 46.76%\n",
            "[Linear] Epoch 98/100 - Train Acc: 51.44%, Val Acc: 45.74%\n",
            "[Linear] Epoch 99/100 - Train Acc: 50.07%, Val Acc: 48.43%\n",
            "[Linear] Epoch 100/100 - Train Acc: 50.95%, Val Acc: 47.69%\n",
            "[Linear‐Probe on EuroSAT test] Final Val Acc = 47.69%\n",
            "\n",
            "Results saved to linear_probe_results.txt\n",
            "Results for seed 42: [{'seed': 42, 'val_acc': 47.68518518518518}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run the evaluation\n",
        "for seed in seeds:\n",
        "    results = load_evaluate_model(f\"simclr_model_seed{seed}.pth\", DEVICE, data_dir, seed)\n",
        "    print(f\"Results for seed {seed}: {results}\")\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
