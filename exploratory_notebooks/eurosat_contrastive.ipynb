{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementation of the SIMCLR with resnet50 backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBlEnEB7Rzfu",
        "outputId": "f0106383-237e-475e-d660-715dcc591a87"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ssl\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
        "from torchvision.models import resnet50\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Prevent nondeterminism\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "torch.backends.cudnn.enabled = False\n",
        "\n",
        "CONFIG = {\n",
        "    \"LOCAL_OR_COLAB\": \"LOCAL\",\n",
        "    \"DATA_DIR_LOCAL\": \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB/\",\n",
        "    \"DATA_DIR_COLAB\": \"/content/EuroSAT_RGB\",\n",
        "    \"ZIP_PATH\": \"/content/EuroSAT.zip\",\n",
        "    \"EUROSAT_URL\": \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\",\n",
        "    \"SEED\": 42,  \n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"LR\": 1e-4,\n",
        "    \"WD\": 0.5,\n",
        "    \"LR_LINEAR\": 1e-4,\n",
        "    \"EPOCHS_SIMCLR\": 100,\n",
        "    \"EPOCHS_LINEAR\": 100,\n",
        "    \"PROJ_DIM\": 128,\n",
        "    \"FEATURE_DIM\": 2048, # ResNet50 feature dimension = 2048\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = 0.8\n",
        "VAL_FRAC   = 0.1\n",
        "TEST_FRAC  = 0.1\n",
        "\n",
        "SEED = CONFIG[\"SEED\"]\n",
        "\n",
        "PRETRAINED = False\n",
        "\n",
        "TEMPERATURE = 0.2\n",
        "\n",
        "BETAS=(0.9,0.98)\n",
        "EPS = 1e-8\n",
        "\n",
        "LINEAR_PROB_TRAIN_SPLIT = 0.75\n",
        "\n",
        "GLOBAL_SEED = CONFIG[\"SEED\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def prepare_data():\n",
        "    if CONFIG[\"LOCAL_OR_COLAB\"] == \"LOCAL\":\n",
        "        return CONFIG[\"DATA_DIR_LOCAL\"]\n",
        "\n",
        "    if not os.path.exists(CONFIG[\"DATA_DIR_COLAB\"]):\n",
        "        print(\"Downloading EuroSAT RGB...\")\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(CONFIG[\"EUROSAT_URL\"], CONFIG[\"ZIP_PATH\"])\n",
        "        with zipfile.ZipFile(CONFIG[\"ZIP_PATH\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", CONFIG[\"DATA_DIR_COLAB\"])\n",
        "        print(\"EuroSAT RGB dataset downloaded and extracted.\")\n",
        "    return CONFIG[\"DATA_DIR_COLAB\"]\n",
        "\n",
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transform(x), self.base_transform(x)]\n",
        "    \n",
        "class SimCLRDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.dataset[idx]\n",
        "        x1, x2 = self.transform(x)\n",
        "        return x1, x2\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    indices = np.arange(total_count)\n",
        "    np.random.seed(SEED) # for reproducibility\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_split = int(0.8 * total_count)\n",
        "    val_split = int(0.9 * total_count)\n",
        "\n",
        "    train_idx = indices[:train_split]\n",
        "    val_idx = indices[train_split:val_split]\n",
        "    test_idx = indices[val_split:]\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "    dataset_for_stats = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=transforms.ToTensor()\n",
        "    )\n",
        "    total_len = len(dataset_for_stats)\n",
        "    n_train = int(TRAIN_FRAC * total_len)\n",
        "    n_val   = int(VAL_FRAC   * total_len)\n",
        "    n_test  = total_len - n_train - n_val\n",
        "\n",
        "    train_for_stats, val_for_stats, test_for_stats = random_split(\n",
        "        dataset_for_stats,\n",
        "        [n_train, n_val, n_test]\n",
        "    )\n",
        "    \n",
        "    mean, std = compute_mean_std(train_for_stats, batch_size)\n",
        "\n",
        "    dataset_train_no_transform = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_indices, val_indices, test_indices = random_split(\n",
        "        list(range(total_len)),\n",
        "        [n_train, n_val, n_test],\n",
        "        generator=generator\n",
        "    )\n",
        "    \n",
        "    all_indices = list(range(total_len))\n",
        "    rnd = random.Random(seed)\n",
        "    rnd.shuffle(all_indices)\n",
        "    train_indices = all_indices[:n_train]\n",
        "    val_indices   = all_indices[n_train : n_train + n_val]\n",
        "    test_indices  = all_indices[n_train + n_val : ]\n",
        "\n",
        "    # Now build Subsets pointing to dataset_train_no_transform:\n",
        "    train_subset_no_transform = Subset(dataset_train_no_transform, train_indices)\n",
        "\n",
        "    # 3) IMAGEFOLDER FOR VALIDATION/TEST (WITH EVAL TRANSFORM)\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.Resize(72),\n",
        "        transforms.CenterCrop(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "    dataset_eval = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=eval_transform\n",
        "    )\n",
        "    val_subset = Subset(dataset_eval, val_indices)\n",
        "    test_subset = Subset(dataset_eval, test_indices)\n",
        "\n",
        "    # DEFINE SIMCLR TRANSFORMS FOR TRAIN\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(64, scale=(0.5, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    simclr_transform = TwoCropsTransform(augment_transform)\n",
        "    train_ds_simclr = SimCLRDataset(train_subset_no_transform, simclr_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds_simclr,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    num_classes = len(dataset_for_stats.classes)\n",
        "    return train_loader, val_loader, test_loader, num_classes\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, proj_dim=128, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class SimCLRModel(nn.Module):\n",
        "    def __init__(self, base_encoder, proj_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = base_encoder\n",
        "        self.encoder.fc = nn.Identity()\n",
        "        self.projection_head = ProjectionHead(input_dim=CONFIG[\"FEATURE_DIM\"], proj_dim=proj_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.encoder(x)\n",
        "        proj = self.projection_head(feat)\n",
        "        return feat, proj\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature=0.5, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        N = zis.size(0)\n",
        "        z = F.normalize(torch.cat([zis, zjs], dim=0), dim=1)\n",
        "        sim = torch.matmul(z, z.T) / self.temperature\n",
        "        mask = torch.eye(2 * N, dtype=torch.bool).to(self.device)\n",
        "        sim = sim.masked_fill(mask, -1e9)\n",
        "        labels = torch.cat([torch.arange(N, 2 * N), torch.arange(0, N)]).to(self.device)\n",
        "        # return self.x(sim, labels)\n",
        "        return self.criterion(sim, labels)\n",
        "\n",
        "def train_simclr(model, loader, optimizer, criterion, device, epochs, scheduler=None):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    # Start a W&B run here\n",
        "   \n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for (x1, x2) in loader:\n",
        "            x1, x2 = x1.to(device), x2.to(device)\n",
        "            _, z1 = model(x1)\n",
        "            _, z2 = model(x2)\n",
        "            loss = criterion(z1, z2)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "        avg = total_loss / len(loader)\n",
        "        print(f\"[SimCLR] Epoch {epoch+1}/{epochs} - Loss: {avg:.4f}\")\n",
        "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg})\n",
        "        \n",
        "    print(\"Finished SimCLR pretraining.\")\n",
        "\n",
        "def evaluate(classifier, backbone, loader, device):\n",
        "    classifier.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = backbone(images)\n",
        "            outputs = classifier(features)\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return correct / total * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_linear_probe(backbone, train_loader, val_loader, device, epochs, lr, run_id):\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    base_ds = train_loader.dataset\n",
        "    while isinstance(base_ds, Subset):\n",
        "        base_ds = base_ds.dataset\n",
        "    num_classes = len(base_ds.classes)\n",
        "\n",
        "    classifier = nn.Linear(CONFIG[\"FEATURE_DIM\"], num_classes).to(device)\n",
        "\n",
        "    # sched = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.9)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        classifier.train()\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = backbone(images)\n",
        "            outputs = classifier(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # sched.step()\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_acc = correct / total * 100\n",
        "        val_acc = evaluate(classifier, backbone, val_loader, device)\n",
        "        print(f\"[Linear] Epoch {epoch+1}/{epochs} - \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    torch.save(classifier.state_dict(), f\"linear_probe_seed{run_id}.pth\")\n",
        "    return val_acc\n",
        "\n",
        "def load_evaluate_model(model_path, device, data_dir, seed):\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"EuroSAT_SimCLR_LinearProbe\",\n",
        "        name=f\"LinearProbe_Seed{seed}\",\n",
        "        config=CONFIG,\n",
        "    )\n",
        "    results = []\n",
        "\n",
        "    backbone = resnet50(weights=None if not PRETRAINED else \"DEFAULT\")\n",
        "    backbone.fc = nn.Identity()  # same as in SimCLRModel\n",
        "    backbone.load_state_dict(torch.load(model_path), strict=False)\n",
        "    backbone.to(device)    \n",
        "    backbone.eval()\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # (Make sure 'data_dir' and 'seed' are in scope; if not, pass them in.)\n",
        "    _, _, test_loader, _ = get_data_loaders(data_dir, CONFIG[\"BATCH_SIZE\"])\n",
        "    print(f\"Starting linear probe on EuroSAT test split (seed={seed})...\")\n",
        "\n",
        "    # Split EuroSAT test‐subset into 80%/20% for probe‐train vs. probe‐val\n",
        "    full_test_ds = test_loader.dataset  # this is a Subset of dataset_eval\n",
        "    train_size = int(LINEAR_PROB_TRAIN_SPLIT * len(full_test_ds))\n",
        "    val_size = len(full_test_ds) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_test_ds, [train_size, val_size])\n",
        "\n",
        "    train_loader_from_test = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    val_loader_from_test = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    linear_probe_val_acc = train_linear_probe(\n",
        "        backbone,\n",
        "        train_loader_from_test,\n",
        "        val_loader_from_test,\n",
        "        DEVICE,\n",
        "        epochs=CONFIG[\"EPOCHS_LINEAR\"],\n",
        "        lr=CONFIG[\"LR_LINEAR\"],\n",
        "        run_id=seed\n",
        "    )\n",
        "    wandb.log({\"linear_probe_val_acc\": linear_probe_val_acc})\n",
        "    wandb.finish()\n",
        "    print(f\"[Linear‐Probe on EuroSAT test] Final Val Acc = {linear_probe_val_acc:.2f}%\\n\")\n",
        "\n",
        "    # ─── 4) Save & return ───────────────────────────────────────────\n",
        "    results.append({\n",
        "        \"seed\": seed,\n",
        "        \"val_acc\": linear_probe_val_acc\n",
        "    })\n",
        "    with open(\"linear_probe_results.txt\", \"a\") as f:\n",
        "        f.write(f\"Seed: {seed}, Val Acc: {linear_probe_val_acc:.2f}%\\n\")\n",
        "    print(\"Results saved to linear_probe_results.txt\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Starting run with seed 42 ===\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SimCLR training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250611_150922-9sxobtez</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch-grid-search/runs/9sxobtez' target=\"_blank\">BS64_LR1e-04_SEED42_TEMPERATURE0.2</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch-grid-search' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch-grid-search</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch-grid-search/runs/9sxobtez' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch-grid-search/runs/9sxobtez</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SimCLR] Epoch 1/100 - Loss: 3.2133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SimCLR] Epoch 2/100 - Loss: 1.7301\n",
            "[SimCLR] Epoch 3/100 - Loss: 1.3163\n",
            "[SimCLR] Epoch 4/100 - Loss: 1.1375\n",
            "[SimCLR] Epoch 5/100 - Loss: 1.0695\n",
            "[SimCLR] Epoch 6/100 - Loss: 1.0197\n",
            "[SimCLR] Epoch 7/100 - Loss: 0.9871\n",
            "[SimCLR] Epoch 8/100 - Loss: 0.9614\n",
            "[SimCLR] Epoch 9/100 - Loss: 0.9481\n",
            "[SimCLR] Epoch 10/100 - Loss: 0.9292\n",
            "[SimCLR] Epoch 11/100 - Loss: 0.9195\n",
            "[SimCLR] Epoch 12/100 - Loss: 0.9047\n",
            "[SimCLR] Epoch 13/100 - Loss: 0.8933\n",
            "[SimCLR] Epoch 14/100 - Loss: 0.8872\n",
            "[SimCLR] Epoch 15/100 - Loss: 0.8804\n",
            "[SimCLR] Epoch 16/100 - Loss: 0.8634\n",
            "[SimCLR] Epoch 17/100 - Loss: 0.8644\n",
            "[SimCLR] Epoch 18/100 - Loss: 0.8561\n",
            "[SimCLR] Epoch 19/100 - Loss: 0.8563\n",
            "[SimCLR] Epoch 20/100 - Loss: 0.8480\n",
            "[SimCLR] Epoch 21/100 - Loss: 0.8435\n",
            "[SimCLR] Epoch 22/100 - Loss: 0.8416\n",
            "[SimCLR] Epoch 23/100 - Loss: 0.8346\n",
            "[SimCLR] Epoch 24/100 - Loss: 0.8307\n",
            "[SimCLR] Epoch 25/100 - Loss: 0.8256\n",
            "[SimCLR] Epoch 26/100 - Loss: 0.8214\n",
            "[SimCLR] Epoch 27/100 - Loss: 0.8251\n",
            "[SimCLR] Epoch 28/100 - Loss: 0.8225\n",
            "[SimCLR] Epoch 29/100 - Loss: 0.8128\n",
            "[SimCLR] Epoch 30/100 - Loss: 0.8125\n",
            "[SimCLR] Epoch 31/100 - Loss: 0.8065\n",
            "[SimCLR] Epoch 32/100 - Loss: 0.8053\n",
            "[SimCLR] Epoch 33/100 - Loss: 0.8101\n",
            "[SimCLR] Epoch 34/100 - Loss: 0.8033\n",
            "[SimCLR] Epoch 35/100 - Loss: 0.7982\n",
            "[SimCLR] Epoch 36/100 - Loss: 0.7993\n",
            "[SimCLR] Epoch 37/100 - Loss: 0.7966\n",
            "[SimCLR] Epoch 38/100 - Loss: 0.7929\n",
            "[SimCLR] Epoch 39/100 - Loss: 0.7903\n",
            "[SimCLR] Epoch 40/100 - Loss: 0.7913\n",
            "[SimCLR] Epoch 41/100 - Loss: 0.7899\n",
            "[SimCLR] Epoch 42/100 - Loss: 0.7913\n",
            "[SimCLR] Epoch 43/100 - Loss: 0.7836\n",
            "[SimCLR] Epoch 44/100 - Loss: 0.7820\n",
            "[SimCLR] Epoch 45/100 - Loss: 0.7810\n",
            "[SimCLR] Epoch 46/100 - Loss: 0.7797\n",
            "[SimCLR] Epoch 47/100 - Loss: 0.7745\n",
            "[SimCLR] Epoch 48/100 - Loss: 0.7770\n",
            "[SimCLR] Epoch 49/100 - Loss: 0.7704\n",
            "[SimCLR] Epoch 50/100 - Loss: 0.7691\n",
            "[SimCLR] Epoch 51/100 - Loss: 0.7739\n",
            "[SimCLR] Epoch 52/100 - Loss: 0.7683\n",
            "[SimCLR] Epoch 53/100 - Loss: 0.7651\n",
            "[SimCLR] Epoch 54/100 - Loss: 0.7684\n",
            "[SimCLR] Epoch 55/100 - Loss: 0.7634\n",
            "[SimCLR] Epoch 56/100 - Loss: 0.7627\n",
            "[SimCLR] Epoch 57/100 - Loss: 0.7613\n",
            "[SimCLR] Epoch 58/100 - Loss: 0.7578\n",
            "[SimCLR] Epoch 59/100 - Loss: 0.7601\n",
            "[SimCLR] Epoch 60/100 - Loss: 0.7567\n",
            "[SimCLR] Epoch 61/100 - Loss: 0.7563\n",
            "[SimCLR] Epoch 62/100 - Loss: 0.7572\n",
            "[SimCLR] Epoch 63/100 - Loss: 0.7560\n",
            "[SimCLR] Epoch 64/100 - Loss: 0.7507\n",
            "[SimCLR] Epoch 65/100 - Loss: 0.7546\n",
            "[SimCLR] Epoch 66/100 - Loss: 0.7488\n",
            "[SimCLR] Epoch 67/100 - Loss: 0.7481\n",
            "[SimCLR] Epoch 68/100 - Loss: 0.7507\n",
            "[SimCLR] Epoch 69/100 - Loss: 0.7489\n",
            "[SimCLR] Epoch 70/100 - Loss: 0.7470\n",
            "[SimCLR] Epoch 71/100 - Loss: 0.7457\n",
            "[SimCLR] Epoch 72/100 - Loss: 0.7408\n",
            "[SimCLR] Epoch 73/100 - Loss: 0.7441\n",
            "[SimCLR] Epoch 74/100 - Loss: 0.7397\n",
            "[SimCLR] Epoch 75/100 - Loss: 0.7366\n",
            "[SimCLR] Epoch 76/100 - Loss: 0.7390\n",
            "[SimCLR] Epoch 77/100 - Loss: 0.7326\n",
            "[SimCLR] Epoch 78/100 - Loss: 0.7393\n",
            "[SimCLR] Epoch 79/100 - Loss: 0.7377\n",
            "[SimCLR] Epoch 80/100 - Loss: 0.7356\n",
            "[SimCLR] Epoch 81/100 - Loss: 0.7338\n",
            "[SimCLR] Epoch 82/100 - Loss: 0.7325\n",
            "[SimCLR] Epoch 83/100 - Loss: 0.7331\n",
            "[SimCLR] Epoch 84/100 - Loss: 0.7321\n",
            "[SimCLR] Epoch 85/100 - Loss: 0.7340\n",
            "[SimCLR] Epoch 86/100 - Loss: 0.7314\n",
            "[SimCLR] Epoch 87/100 - Loss: 0.7327\n",
            "[SimCLR] Epoch 88/100 - Loss: 0.7283\n",
            "[SimCLR] Epoch 89/100 - Loss: 0.7275\n",
            "[SimCLR] Epoch 90/100 - Loss: 0.7322\n",
            "[SimCLR] Epoch 91/100 - Loss: 0.7292\n",
            "[SimCLR] Epoch 92/100 - Loss: 0.7298\n",
            "[SimCLR] Epoch 93/100 - Loss: 0.7312\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     41\u001b[0m lr \u001b[38;5;241m=\u001b[39m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     42\u001b[0m wandb_run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     43\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meurosat-contrastive-scratch-grid-search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_LR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_SEED\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_TEMPERATURE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEMPERATURE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     }\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtrain_simclr\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimclr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPOCHS_SIMCLR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m wandb_run\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving encoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[5], line 222\u001b[0m, in \u001b[0;36mtrain_simclr\u001b[0;34m(model, loader, optimizer, criterion, device, epochs, scheduler)\u001b[0m\n\u001b[1;32m    220\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(z1, z2)\n\u001b[1;32m    221\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 222\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def make_optimizer_scheduler(params, lr, wd, steps_per_epoch, epochs):\n",
        "    total_steps  = epochs * steps_per_epoch\n",
        "    warmup_steps = steps_per_epoch\n",
        "    opt = optim.AdamW(params, lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "    sched = SequentialLR(\n",
        "        opt,\n",
        "        schedulers=[\n",
        "            LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "            CosineAnnealingLR(opt, T_max=total_steps - warmup_steps)\n",
        "        ],\n",
        "        milestones=[warmup_steps]\n",
        "    )\n",
        "    return opt, sched\n",
        "\n",
        "seeds = [GLOBAL_SEED]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
        "    set_seed(seed)\n",
        "    \n",
        "    data_dir = prepare_data()\n",
        "    train_loader, val_loader, test_loader, num_classes = get_data_loaders(data_dir, CONFIG[\"BATCH_SIZE\"])\n",
        "\n",
        "    # Initialize base encoder and SimCLR model\n",
        "    base_encoder = resnet50(weights=None if not PRETRAINED else \"DEFAULT\")\n",
        "    simclr_model = SimCLRModel(base_encoder, proj_dim=CONFIG[\"PROJ_DIM\"])\n",
        "    # optimizer = optim.Adam(simclr_model.parameters(), lr=CONFIG[\"LR\"])\n",
        "    wd =  0.5 \n",
        "    optimizer, scheduler = make_optimizer_scheduler(\n",
        "        simclr_model.parameters(),\n",
        "        CONFIG[\"LR\"],\n",
        "        CONFIG[\"WD\"],\n",
        "        len(train_loader),\n",
        "        CONFIG[\"EPOCHS_SIMCLR\"]\n",
        "        )\n",
        "    \n",
        "    bs = CONFIG[\"BATCH_SIZE\"]\n",
        "    loss_fn = NTXentLoss(bs, temperature=TEMPERATURE, device=DEVICE)\n",
        "\n",
        "    print(\"Starting SimCLR training...\")\n",
        "    lr = CONFIG[\"LR\"]\n",
        "    wandb_run = wandb.init(\n",
        "        project=\"eurosat-contrastive-scratch-grid-search\",\n",
        "        name=f\"BS{bs}_LR{lr:.0e}_SEED{seed}_TEMPERATURE{TEMPERATURE}\",\n",
        "        config={\n",
        "            \"seed\": seed,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"model\": \"SimCLR\",\n",
        "            \"dataset\": \"EuroSAT\",\n",
        "            \"batch_size\": bs,\n",
        "            \"learning_rate\": CONFIG[\"LR\"],\n",
        "            \"epochs\": CONFIG[\"EPOCHS_SIMCLR\"],\n",
        "            \"proj_dim\": CONFIG[\"PROJ_DIM\"],\n",
        "            \"feature_dim\": CONFIG[\"FEATURE_DIM\"],\n",
        "            \"pretrained\": PRETRAINED,\n",
        "        }\n",
        "    )\n",
        "    train_simclr(simclr_model, train_loader, optimizer, loss_fn, DEVICE, CONFIG[\"EPOCHS_SIMCLR\"], scheduler=scheduler)\n",
        "    wandb_run.finish()\n",
        "\n",
        "    print(\"Saving encoder...\")\n",
        "    torch.save(simclr_model.state_dict(), f\"simclr_model_seed{seed}_temperature{TEMPERATURE}_bs{bs}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250605_124701-2cr459yt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe/runs/2cr459yt' target=\"_blank\">LinearProbe_Seed42</a></strong> to <a href='https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe' target=\"_blank\">https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe/runs/2cr459yt' target=\"_blank\">https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe/runs/2cr459yt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_27018/142514229.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  backbone.load_state_dict(torch.load(model_path), strict=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting linear probe on EuroSAT test split (seed=42)...\n",
            "[Linear] Epoch 1/100 - Train Acc: 13.10%, Val Acc: 8.52%\n",
            "[Linear] Epoch 2/100 - Train Acc: 9.77%, Val Acc: 12.59%\n",
            "[Linear] Epoch 3/100 - Train Acc: 11.85%, Val Acc: 12.22%\n",
            "[Linear] Epoch 4/100 - Train Acc: 14.03%, Val Acc: 10.00%\n",
            "[Linear] Epoch 5/100 - Train Acc: 11.53%, Val Acc: 14.44%\n",
            "[Linear] Epoch 6/100 - Train Acc: 16.39%, Val Acc: 11.11%\n",
            "[Linear] Epoch 7/100 - Train Acc: 14.44%, Val Acc: 17.22%\n",
            "[Linear] Epoch 8/100 - Train Acc: 15.74%, Val Acc: 19.26%\n",
            "[Linear] Epoch 9/100 - Train Acc: 15.00%, Val Acc: 22.41%\n",
            "[Linear] Epoch 10/100 - Train Acc: 20.28%, Val Acc: 22.41%\n",
            "[Linear] Epoch 11/100 - Train Acc: 18.19%, Val Acc: 21.11%\n",
            "[Linear] Epoch 12/100 - Train Acc: 20.74%, Val Acc: 22.04%\n",
            "[Linear] Epoch 13/100 - Train Acc: 21.81%, Val Acc: 20.00%\n",
            "[Linear] Epoch 14/100 - Train Acc: 23.52%, Val Acc: 21.48%\n",
            "[Linear] Epoch 15/100 - Train Acc: 22.59%, Val Acc: 22.22%\n",
            "[Linear] Epoch 16/100 - Train Acc: 23.24%, Val Acc: 24.44%\n",
            "[Linear] Epoch 17/100 - Train Acc: 23.70%, Val Acc: 24.07%\n",
            "[Linear] Epoch 18/100 - Train Acc: 25.05%, Val Acc: 22.59%\n",
            "[Linear] Epoch 19/100 - Train Acc: 24.91%, Val Acc: 24.07%\n",
            "[Linear] Epoch 20/100 - Train Acc: 25.37%, Val Acc: 24.26%\n",
            "[Linear] Epoch 21/100 - Train Acc: 25.28%, Val Acc: 24.44%\n",
            "[Linear] Epoch 22/100 - Train Acc: 25.09%, Val Acc: 24.81%\n",
            "[Linear] Epoch 23/100 - Train Acc: 25.65%, Val Acc: 25.00%\n",
            "[Linear] Epoch 24/100 - Train Acc: 26.34%, Val Acc: 24.44%\n",
            "[Linear] Epoch 25/100 - Train Acc: 26.81%, Val Acc: 24.81%\n",
            "[Linear] Epoch 26/100 - Train Acc: 27.55%, Val Acc: 24.63%\n",
            "[Linear] Epoch 27/100 - Train Acc: 28.24%, Val Acc: 26.48%\n",
            "[Linear] Epoch 28/100 - Train Acc: 26.81%, Val Acc: 26.85%\n",
            "[Linear] Epoch 29/100 - Train Acc: 29.12%, Val Acc: 24.81%\n",
            "[Linear] Epoch 30/100 - Train Acc: 29.49%, Val Acc: 24.63%\n",
            "[Linear] Epoch 31/100 - Train Acc: 27.18%, Val Acc: 24.07%\n",
            "[Linear] Epoch 32/100 - Train Acc: 28.06%, Val Acc: 27.04%\n",
            "[Linear] Epoch 33/100 - Train Acc: 29.12%, Val Acc: 29.07%\n",
            "[Linear] Epoch 34/100 - Train Acc: 29.95%, Val Acc: 27.78%\n",
            "[Linear] Epoch 35/100 - Train Acc: 31.39%, Val Acc: 27.96%\n",
            "[Linear] Epoch 36/100 - Train Acc: 30.97%, Val Acc: 28.70%\n",
            "[Linear] Epoch 37/100 - Train Acc: 33.56%, Val Acc: 30.19%\n",
            "[Linear] Epoch 38/100 - Train Acc: 32.96%, Val Acc: 29.07%\n",
            "[Linear] Epoch 39/100 - Train Acc: 31.48%, Val Acc: 29.07%\n",
            "[Linear] Epoch 40/100 - Train Acc: 33.33%, Val Acc: 29.81%\n",
            "[Linear] Epoch 41/100 - Train Acc: 34.72%, Val Acc: 27.04%\n",
            "[Linear] Epoch 42/100 - Train Acc: 33.98%, Val Acc: 25.74%\n",
            "[Linear] Epoch 43/100 - Train Acc: 31.81%, Val Acc: 29.26%\n",
            "[Linear] Epoch 44/100 - Train Acc: 32.08%, Val Acc: 26.30%\n",
            "[Linear] Epoch 45/100 - Train Acc: 32.18%, Val Acc: 30.37%\n",
            "[Linear] Epoch 46/100 - Train Acc: 32.13%, Val Acc: 31.48%\n",
            "[Linear] Epoch 47/100 - Train Acc: 32.92%, Val Acc: 32.04%\n",
            "[Linear] Epoch 48/100 - Train Acc: 33.70%, Val Acc: 30.19%\n",
            "[Linear] Epoch 49/100 - Train Acc: 33.80%, Val Acc: 33.33%\n",
            "[Linear] Epoch 50/100 - Train Acc: 36.06%, Val Acc: 33.15%\n",
            "[Linear] Epoch 51/100 - Train Acc: 36.48%, Val Acc: 32.04%\n",
            "[Linear] Epoch 52/100 - Train Acc: 36.62%, Val Acc: 32.41%\n",
            "[Linear] Epoch 53/100 - Train Acc: 36.02%, Val Acc: 32.78%\n",
            "[Linear] Epoch 54/100 - Train Acc: 34.91%, Val Acc: 31.11%\n",
            "[Linear] Epoch 55/100 - Train Acc: 36.62%, Val Acc: 31.67%\n",
            "[Linear] Epoch 56/100 - Train Acc: 35.97%, Val Acc: 32.41%\n",
            "[Linear] Epoch 57/100 - Train Acc: 36.85%, Val Acc: 25.56%\n",
            "[Linear] Epoch 58/100 - Train Acc: 33.10%, Val Acc: 33.70%\n",
            "[Linear] Epoch 59/100 - Train Acc: 39.07%, Val Acc: 29.81%\n",
            "[Linear] Epoch 60/100 - Train Acc: 35.65%, Val Acc: 28.89%\n",
            "[Linear] Epoch 61/100 - Train Acc: 34.68%, Val Acc: 32.96%\n",
            "[Linear] Epoch 62/100 - Train Acc: 36.99%, Val Acc: 33.70%\n",
            "[Linear] Epoch 63/100 - Train Acc: 38.19%, Val Acc: 33.89%\n",
            "[Linear] Epoch 64/100 - Train Acc: 38.38%, Val Acc: 34.63%\n",
            "[Linear] Epoch 65/100 - Train Acc: 38.43%, Val Acc: 27.04%\n",
            "[Linear] Epoch 66/100 - Train Acc: 36.81%, Val Acc: 30.74%\n",
            "[Linear] Epoch 67/100 - Train Acc: 37.08%, Val Acc: 30.19%\n",
            "[Linear] Epoch 68/100 - Train Acc: 38.01%, Val Acc: 32.22%\n",
            "[Linear] Epoch 69/100 - Train Acc: 38.24%, Val Acc: 35.56%\n",
            "[Linear] Epoch 70/100 - Train Acc: 38.29%, Val Acc: 35.37%\n",
            "[Linear] Epoch 71/100 - Train Acc: 40.09%, Val Acc: 35.00%\n",
            "[Linear] Epoch 72/100 - Train Acc: 39.95%, Val Acc: 37.41%\n",
            "[Linear] Epoch 73/100 - Train Acc: 39.44%, Val Acc: 35.93%\n",
            "[Linear] Epoch 74/100 - Train Acc: 40.51%, Val Acc: 34.44%\n",
            "[Linear] Epoch 75/100 - Train Acc: 40.09%, Val Acc: 37.04%\n",
            "[Linear] Epoch 76/100 - Train Acc: 40.14%, Val Acc: 37.59%\n",
            "[Linear] Epoch 77/100 - Train Acc: 40.79%, Val Acc: 39.26%\n",
            "[Linear] Epoch 78/100 - Train Acc: 38.43%, Val Acc: 35.93%\n",
            "[Linear] Epoch 79/100 - Train Acc: 38.70%, Val Acc: 38.89%\n",
            "[Linear] Epoch 80/100 - Train Acc: 38.70%, Val Acc: 37.04%\n",
            "[Linear] Epoch 81/100 - Train Acc: 41.20%, Val Acc: 33.70%\n",
            "[Linear] Epoch 82/100 - Train Acc: 41.76%, Val Acc: 30.56%\n",
            "[Linear] Epoch 83/100 - Train Acc: 40.88%, Val Acc: 33.89%\n",
            "[Linear] Epoch 84/100 - Train Acc: 41.67%, Val Acc: 34.63%\n",
            "[Linear] Epoch 85/100 - Train Acc: 41.06%, Val Acc: 40.19%\n",
            "[Linear] Epoch 86/100 - Train Acc: 42.50%, Val Acc: 38.89%\n",
            "[Linear] Epoch 87/100 - Train Acc: 41.48%, Val Acc: 37.78%\n",
            "[Linear] Epoch 88/100 - Train Acc: 41.20%, Val Acc: 38.52%\n",
            "[Linear] Epoch 89/100 - Train Acc: 41.06%, Val Acc: 37.59%\n",
            "[Linear] Epoch 90/100 - Train Acc: 42.27%, Val Acc: 34.26%\n",
            "[Linear] Epoch 91/100 - Train Acc: 41.48%, Val Acc: 36.48%\n",
            "[Linear] Epoch 92/100 - Train Acc: 42.22%, Val Acc: 40.37%\n",
            "[Linear] Epoch 93/100 - Train Acc: 42.96%, Val Acc: 40.37%\n",
            "[Linear] Epoch 94/100 - Train Acc: 42.92%, Val Acc: 40.56%\n",
            "[Linear] Epoch 95/100 - Train Acc: 43.38%, Val Acc: 38.33%\n",
            "[Linear] Epoch 96/100 - Train Acc: 43.19%, Val Acc: 36.48%\n",
            "[Linear] Epoch 97/100 - Train Acc: 40.83%, Val Acc: 39.63%\n",
            "[Linear] Epoch 98/100 - Train Acc: 43.33%, Val Acc: 38.33%\n",
            "[Linear] Epoch 99/100 - Train Acc: 42.08%, Val Acc: 38.15%\n",
            "[Linear] Epoch 100/100 - Train Acc: 42.87%, Val Acc: 36.85%\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>linear_probe_val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>linear_probe_val_acc</td><td>36.85185</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LinearProbe_Seed42</strong> at: <a href='https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe/runs/2cr459yt' target=\"_blank\">https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe/runs/2cr459yt</a><br> View project at: <a href='https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe' target=\"_blank\">https://wandb.ai/analiju-paris/EuroSAT_SimCLR_LinearProbe</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_124701-2cr459yt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Linear‐Probe on EuroSAT test] Final Val Acc = 36.85%\n",
            "\n",
            "Results saved to linear_probe_results.txt\n",
            "Results for seed 42: [{'seed': 42, 'val_acc': 36.851851851851855}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run the evaluation\n",
        "for seed in seeds:\n",
        "    results = load_evaluate_model(f\"simclr_model_seed{seed}.pth\", DEVICE, data_dir, seed)\n",
        "    print(f\"Results for seed {seed}: {results}\")\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
