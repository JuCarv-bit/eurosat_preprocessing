{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementation of the SIMCLR with resnet50 backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()       # reads .env and sets os.environ\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBlEnEB7Rzfu",
        "outputId": "f0106383-237e-475e-d660-715dcc591a87"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "import numpy as np\n",
        "from utils.version_utils import print_versions, configure_gpu_device, set_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from simclr.data.datamodule import compute_mean_std, prepare_data, combine_train_val_loaders, SimCLRDataset, get_split_indexes, get_data_loaders\n",
        "\n",
        "\n",
        "from simclr.data.transforms import TwoCropsTransform, get_transforms\n",
        "    \n",
        "from simclr.models.simclr import SimCLRModel, ProjectionHead\n",
        "\n",
        "\n",
        "from simclr.models.loss import NTXentLoss, compute_contrastive_val_loss, compute_contrastive_accuracy\n",
        "\n",
        "\n",
        "from simclr.data.datamodule import LabeledEvalDataset\n",
        "\n",
        "from simclr.probes.logistic import get_probe_loaders, run_logistic_probe_experiment\n",
        "\n",
        "from simclr.utils.scheduler import make_optimizer_scheduler\n",
        "\n",
        "from simclr.utils.misc import evaluate\n",
        "\n",
        "from simclr.data.mydataloaders import get_data_loaders_train_test_linear_probe\n",
        "from simclr.config import CONFIG\n",
        "from simclr.train import train_simclr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conda version: 25.5.1\n",
            "Python version: 3.10.16\n",
            "PyTorch version: 2.5.1\n",
            "CUDA available: True\n",
            "CUDA device count: 3\n",
            "Torchvision version: 0.20.1\n",
            "Successfully set to use GPU: 0 (Quadro RTX 6000)\n",
            "Final DEVICE variable is set to: cuda:0\n",
            "Current PyTorch default device: 0\n",
            "Current PyTorch default device (after set_device): 0\n",
            "Dummy tensor is on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print_versions()\n",
        "set_seed(seed=42)\n",
        "\n",
        "TARGET_GPU_INDEX = CONFIG[\"TARGET_GPU_INDEX\"] if \"TARGET_GPU_INDEX\" in CONFIG else 0  # Default to 0 if not set\n",
        "\n",
        "DEVICE = configure_gpu_device(TARGET_GPU_INDEX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Prevent nondeterminism\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "torch.backends.cudnn.enabled = False\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = CONFIG[\"TRAIN_FRAC\"]\n",
        "VAL_FRAC   = CONFIG[\"VAL_FRAC\"]\n",
        "TEST_FRAC  = CONFIG[\"TEST_FRAC\"]\n",
        "\n",
        "SEED = CONFIG[\"SEED\"]\n",
        "\n",
        "PRETRAINED = False\n",
        "\n",
        "TEMPERATURE = CONFIG[\"TEMPERATURE\"]\n",
        "\n",
        "BETAS=(0.9,0.98)\n",
        "EPS = 1e-8\n",
        "\n",
        "GLOBAL_SEED = CONFIG[\"SEED\"]\n",
        "NUM_WORKERS = CONFIG[\"NUM_WORKERS\"]\n",
        "\n",
        "EUROSAT_IMAGE_SIZE = (64, 64)\n",
        "MODEL_INPUT_SIZE = [224, 224]\n",
        "EPOCH_SAVE_INTERVAL = CONFIG[\"EPOCH_SAVE_INTERVAL\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Starting run with seed 42 ===\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441457152366638, 0.3800986111164093, 0.40766361355781555]\n",
            "Computed std:  [0.09299743920564651, 0.06464490294456482, 0.054139167070388794]\n",
            "Mean and std saved to models/mean_std.txt\n",
            "Train/Val/Test loaders: 84/11/11 batches\n"
          ]
        }
      ],
      "source": [
        "\n",
        "seeds = [GLOBAL_SEED]\n",
        "for seed in seeds:\n",
        "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
        "    set_seed(seed)\n",
        "    \n",
        "    data_dir = prepare_data()\n",
        "    train_loader, val_loader, test_loader, val_subset_no_transform, num_classes = get_data_loaders(data_dir, CONFIG[\"BATCH_SIZE\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Starting run with seed 42 ===\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441457152366638, 0.3800986111164093, 0.40766361355781555]\n",
            "Computed std:  [0.09299743920564651, 0.06464490294456482, 0.054139167070388794]\n",
            "Mean and std saved to models/mean_std.txt\n",
            "Train/Val/Test loaders: 84/11/11 batches\n",
            "Starting SimCLR training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250702_193338-94mxmwtm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/94mxmwtm' target=\"_blank\">BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS2</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/94mxmwtm' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/94mxmwtm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=200).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[probe] val accuracy = 54.07%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=200).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[probe] val accuracy = 78.63%\n",
            "Epoch 01/2 | Train Loss: 6.1338, Val Loss: 6.0011 | Logistic Probe Acc (Val): 0.541, Logistic Probe Acc (Train): 0.786 | Contrastive Acc (Train): 0.008, Contrastive Acc (Val): 0.012 | KNN Acc (Val): 0.464\n"
          ]
        }
      ],
      "source": [
        "\n",
        "seeds = [GLOBAL_SEED]\n",
        "for seed in seeds:\n",
        "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
        "    set_seed(seed)\n",
        "    \n",
        "    data_dir = prepare_data()\n",
        "    train_loader, val_loader, test_loader, val_subset_no_transform, num_classes = get_data_loaders(data_dir, CONFIG[\"BATCH_SIZE\"])\n",
        "\n",
        "    base_encoder = resnet50(weights=None)\n",
        "    simclr_model = SimCLRModel(base_encoder, proj_dim=CONFIG[\"PROJ_DIM\"])\n",
        "    # optimizer = optim.Adam(simclr_model.parameters(), lr=CONFIG[\"LR\"])\n",
        "    wd =  0.5 \n",
        "    optimizer, scheduler = make_optimizer_scheduler(\n",
        "        simclr_model.parameters(),\n",
        "        CONFIG[\"LR\"],\n",
        "        CONFIG[\"WD\"],\n",
        "        len(train_loader),\n",
        "        CONFIG[\"EPOCHS_SIMCLR\"]\n",
        "        )\n",
        "    \n",
        "    bs = CONFIG[\"BATCH_SIZE\"]\n",
        "    loss_fn = NTXentLoss(bs, temperature=TEMPERATURE, device=DEVICE)\n",
        "\n",
        "    print(\"Starting SimCLR training...\")\n",
        "    epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
        "    lr = CONFIG[\"LR\"]\n",
        "    wandb_run = wandb.init(\n",
        "        project=\"eurosat-contrastive-scratch\",\n",
        "        name=f\"BS{bs}_LR{lr:.0e}_SEED{seed}_TEMPERATURE{TEMPERATURE}_EPOCHS{epochs_simclr}\",\n",
        "        tags=[\"SimCLR\", \"EuroSAT\", \"Contrastive Learning\"],\n",
        "        config={\n",
        "            \"seed\": seed,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"model\": \"SimCLR\",\n",
        "            \"dataset\": \"EuroSAT\",\n",
        "            \"batch_size\": bs,\n",
        "            \"learning_rate\": CONFIG[\"LR\"],\n",
        "            \"epochs\": CONFIG[\"EPOCHS_SIMCLR\"],\n",
        "            \"proj_dim\": CONFIG[\"PROJ_DIM\"],\n",
        "            \"feature_dim\": CONFIG[\"FEATURE_DIM\"],\n",
        "            \"pretrained\": PRETRAINED,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    eval_transform, augment_transform = get_transforms(\n",
        "        mean =CONFIG[\"MEAN\"],\n",
        "        std = CONFIG[\"STD\"]\n",
        "    )  # these must match the transforms used in test_loader\n",
        "\n",
        "    probe_train_loader, probe_val_loader = get_probe_loaders(\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        eval_transform,               # must match transforms used in test_loader\n",
        "        probe_batch_size=CONFIG[\"BATCH_SIZE\"]\n",
        "    )\n",
        "\n",
        "    train_simclr(\n",
        "        simclr_model,\n",
        "        train_loader, val_loader,\n",
        "        probe_train_loader, probe_val_loader,\n",
        "        optimizer, loss_fn, DEVICE,\n",
        "        simclr_epochs=CONFIG[\"EPOCHS_SIMCLR\"],\n",
        "        probe_lr=CONFIG[\"LR_LINEAR\"],\n",
        "        probe_epochs=1,            # 1 pass per epoch is typical\n",
        "        feature_dim=CONFIG[\"FEATURE_DIM\"],\n",
        "        num_classes=num_classes,\n",
        "        augment_transform=augment_transform,\n",
        "        val_subset_no_transform=val_subset_no_transform,\n",
        "        wandb_run=wandb_run,\n",
        "        scheduler=scheduler,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    wandb_run.finish()\n",
        "\n",
        "\n",
        "\n",
        "print(\"All runs completed.\")\n",
        "wandb.finish()\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the saved model and run linear probe\n",
        "seed = CONFIG[\"SEED\"]\n",
        "bs = CONFIG[\"BATCH_SIZE\"]\n",
        "epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
        "simclr_lr = CONFIG[\"LR\"]\n",
        "lr_str = f\"{simclr_lr:.0e}\" if simclr_lr < 0.0001 else f\"{simclr_lr:.6f}\"\n",
        "model_path = f\"models/simclr_seed{seed}_bs{bs}_temp{TEMPERATURE}_Tepochs{epochs_simclr}_lr{lr_str}.pth\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Model {model_path} does not exist. Please run the SimCLR pretraining first.\")\n",
        "\n",
        "base_encoder = resnet50(weights=None)\n",
        "simclr_model = SimCLRModel(base_encoder, proj_dim=CONFIG[\"PROJ_DIM\"])\n",
        "checkpoint_path = model_path\n",
        "state_dict = torch.load(checkpoint_path, map_location=torch.device(DEVICE), weights_only=True)\n",
        "simclr_model.load_state_dict(state_dict)\n",
        "\n",
        "# Perform linear probe on train+val as train set, and test as test set\n",
        "train_loader, test_loader, num_classes = get_data_loaders_train_test_linear_probe(CONFIG[\"DATA_DIR_LOCAL\"], CONFIG[\"BATCH_SIZE\"])\n",
        "run_logistic_probe_experiment(\n",
        "    42,\n",
        "    train_loader,\n",
        "    None,  # No validation loader for linear probe\n",
        "    test_loader,\n",
        "    num_classes,\n",
        "    simclr_model,\n",
        "    bs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grid search for best hyperparameters\n",
        "\n",
        "batch_sizes_epochs = [\n",
        "    (64, 35),\n",
        "    (128, 40),\n",
        "    (256, 100),\n",
        "    (512, 100),\n",
        "    (1024, 150),\n",
        "]\n",
        "\n",
        "learning_rates = [\n",
        "    1e-3,\n",
        "    3.75e-4,\n",
        "    1e-4,\n",
        "    3.75e-5,\n",
        "    1e-5,\n",
        "]\n",
        "\n",
        "# use linspace for computing the temperature\n",
        "temperatures = np.linspace(0.05, 0.5, 5).tolist() # [0.05, 0.1625, 0.275, 0.3875, 0.5]\n",
        "temperatures.append(0.2)  # add the original temperature\n",
        "\n",
        "gpu_indexes = [0, 1]\n",
        "# put half of the experiments on each GPU\n",
        "gpu_experiments = {0: [], 1: []}\n",
        "all_acc = []\n",
        "\n",
        "# train simclr with different hyperparameters and apply linear probe"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
