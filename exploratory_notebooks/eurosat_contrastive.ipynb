{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========== GLOBAL CONFIGURATION ===========\n",
        "import os\n",
        "import ssl\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.models import resnet18\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Prevent nondeterminism\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "torch.backends.cudnn.enabled = False\n",
        "\n",
        "CONFIG = {\n",
        "    \"LOCAL_OR_COLAB\": \"COLAB\",\n",
        "    \"DATA_DIR_LOCAL\": \"/home/juliana/internship_LINUX/datasets/EuroSAT_RGB\",\n",
        "    \"DATA_DIR_COLAB\": \"/content/EuroSAT_RGB\",\n",
        "    \"ZIP_PATH\": \"/content/EuroSAT.zip\",\n",
        "    \"EUROSAT_URL\": \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\",\n",
        "    \"SEED\": 42,  # Default seed (will be overridden per run)\n",
        "    \"BATCH_SIZE\": 128,\n",
        "    \"LR\": 0.001,\n",
        "    \"EPOCHS_SIMCLR\": 2,\n",
        "    \"EPOCHS_LINEAR\": 2,\n",
        "    \"PROJ_DIM\": 128,\n",
        "    \"FEATURE_DIM\": 512,\n",
        "}\n",
        "\n",
        "# =========== SETUP ===========\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def prepare_data():\n",
        "    if CONFIG[\"LOCAL_OR_COLAB\"] == \"LOCAL\":\n",
        "        return CONFIG[\"DATA_DIR_LOCAL\"]\n",
        "\n",
        "    if not os.path.exists(CONFIG[\"DATA_DIR_COLAB\"]):\n",
        "        print(\"Downloading EuroSAT RGB...\")\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(CONFIG[\"EUROSAT_URL\"], CONFIG[\"ZIP_PATH\"])\n",
        "        with zipfile.ZipFile(CONFIG[\"ZIP_PATH\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", CONFIG[\"DATA_DIR_COLAB\"])\n",
        "        print(\"EuroSAT RGB dataset downloaded and extracted.\")\n",
        "    return CONFIG[\"DATA_DIR_COLAB\"]\n",
        "\n",
        "# =========== TRANSFORMS ===========\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "simclr_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(64, scale=(0.5, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize(72),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transform(x), self.base_transform(x)]\n",
        "\n",
        "# =========== MODEL COMPONENTS ===========\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, proj_dim=128, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class SimCLRModel(nn.Module):\n",
        "    def __init__(self, base_encoder, proj_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = base_encoder\n",
        "        self.encoder.fc = nn.Identity()\n",
        "        self.projection_head = ProjectionHead(input_dim=CONFIG[\"FEATURE_DIM\"], proj_dim=proj_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.encoder(x)\n",
        "        proj = self.projection_head(feat)\n",
        "        return feat, proj\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature=0.5, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        N = zis.size(0)\n",
        "        z = F.normalize(torch.cat([zis, zjs], dim=0), dim=1)\n",
        "        sim = torch.matmul(z, z.T) / self.temperature\n",
        "        mask = torch.eye(2 * N, dtype=torch.bool).to(self.device)\n",
        "        sim = sim.masked_fill(mask, -1e9)\n",
        "        labels = torch.cat([torch.arange(N, 2 * N), torch.arange(0, N)]).to(self.device)\n",
        "        return self.criterion(sim, labels)\n",
        "\n",
        "# =========== TRAINING ===========\n",
        "def train_simclr(model, loader, optimizer, criterion, device, epochs):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for (x1, x2), _ in loader:\n",
        "            x1, x2 = x1.to(device), x2.to(device)\n",
        "            _, z1 = model(x1)\n",
        "            _, z2 = model(x2)\n",
        "            loss = criterion(z1, z2)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg = total_loss / len(loader)\n",
        "        print(f\"[SimCLR] Epoch {epoch+1}/{epochs} - Loss: {avg:.4f}\")\n",
        "    print(\"Finished SimCLR pretraining.\")\n",
        "\n",
        "def train_linear_probe(backbone, train_loader, val_loader, device, epochs, lr, run_id):\n",
        "    # Freeze backbone parameters\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "    # Create a classifier on top of the frozen features\n",
        "    classifier = nn.Linear(CONFIG[\"FEATURE_DIM\"], len(train_loader.dataset.dataset.classes)).to(device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        classifier.train()\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = backbone(images)\n",
        "            outputs = classifier(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_acc = correct / total * 100\n",
        "        val_acc = evaluate(classifier, backbone, val_loader, device)\n",
        "        print(f\"[Linear] Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the classifier weights uniquely for each run\n",
        "    torch.save(classifier.state_dict(), f\"linear_probe_seed{run_id}.pth\")\n",
        "    # Return the final validation accuracy\n",
        "    return val_acc\n",
        "\n",
        "def evaluate(classifier, backbone, loader, device):\n",
        "    classifier.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = backbone(images)\n",
        "            outputs = classifier(features)\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return correct / total * 100\n",
        "\n",
        "# =========== RUN EVERYTHING ===========\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the list of seeds for each run\n",
        "    seeds = [42, 43, 44]\n",
        "    results = []  # Will store the final linear probe validation accuracies\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n=== Starting run with seed {seed} ===\")\n",
        "        set_seed(seed)\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        data_dir = prepare_data()\n",
        "\n",
        "        # Prepare datasets and dataloaders for contrastive and evaluation\n",
        "        contrastive_dataset = datasets.ImageFolder(data_dir, transform=TwoCropsTransform(simclr_transform))\n",
        "        contrastive_loader = DataLoader(contrastive_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True, drop_last=True)\n",
        "\n",
        "        full_dataset = datasets.ImageFolder(data_dir, transform=eval_transform)\n",
        "        train_len = int(0.8 * len(full_dataset))\n",
        "        val_len = len(full_dataset) - train_len\n",
        "        train_set, val_set = random_split(full_dataset, [train_len, val_len])\n",
        "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=32)\n",
        "\n",
        "        # Initialize base encoder and SimCLR model\n",
        "        pretrained = True\n",
        "        base_encoder = resnet18(weights=None if not pretrained else \"DEFAULT\")\n",
        "        simclr_model = SimCLRModel(base_encoder, proj_dim=CONFIG[\"PROJ_DIM\"])\n",
        "        optimizer = optim.Adam(simclr_model.parameters(), lr=CONFIG[\"LR\"])\n",
        "        loss_fn = NTXentLoss(CONFIG[\"BATCH_SIZE\"], temperature=0.5, device=device)\n",
        "\n",
        "        print(\"Starting SimCLR training...\")\n",
        "        train_simclr(simclr_model, contrastive_loader, optimizer, loss_fn, device, CONFIG[\"EPOCHS_SIMCLR\"])\n",
        "\n",
        "        print(\"Saving encoder...\")\n",
        "        torch.save(simclr_model.state_dict(), f\"simclr_model_seed{seed}.pth\")\n",
        "\n",
        "        print(\"Starting linear probe training...\")\n",
        "        final_val_acc = train_linear_probe(simclr_model.encoder, train_loader, val_loader, device, CONFIG[\"EPOCHS_LINEAR\"], CONFIG[\"LR\"], seed)\n",
        "        results.append(final_val_acc)\n",
        "        print(f\"Run with seed {seed} finished with final Val Acc: {final_val_acc:.2f}%\")\n",
        "\n",
        "    # Compute and print overall mean and standard deviation of the final validation accuracies\n",
        "    mean_acc = np.mean(results)\n",
        "    std_acc = np.std(results)\n",
        "    print(\"\\n=== Summary over runs ===\")\n",
        "    print(f\"Final Linear Probe Validation Accuracies: {results}\")\n",
        "    print(f\"Mean Accuracy: {mean_acc:.2f}%\")\n",
        "    print(f\"Standard Deviation Accuracy: {std_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBlEnEB7Rzfu",
        "outputId": "f0106383-237e-475e-d660-715dcc591a87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting run with seed 42 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/2 - Loss: 3.9642\n",
            "[SimCLR] Epoch 2/2 - Loss: 3.8605\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n",
            "Starting linear probe training...\n",
            "[Linear] Epoch 1/2 - Train Acc: 80.32%, Val Acc: 80.24%\n",
            "[Linear] Epoch 2/2 - Train Acc: 81.44%, Val Acc: 80.35%\n",
            "Run with seed 42 finished with final Val Acc: 80.35%\n",
            "\n",
            "=== Starting run with seed 43 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/2 - Loss: 3.9722\n",
            "[SimCLR] Epoch 2/2 - Loss: 3.8580\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n",
            "Starting linear probe training...\n",
            "[Linear] Epoch 1/2 - Train Acc: 80.83%, Val Acc: 82.56%\n",
            "[Linear] Epoch 2/2 - Train Acc: 82.90%, Val Acc: 82.50%\n",
            "Run with seed 43 finished with final Val Acc: 82.50%\n",
            "\n",
            "=== Starting run with seed 44 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/2 - Loss: 3.9571\n",
            "[SimCLR] Epoch 2/2 - Loss: 3.8604\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n",
            "Starting linear probe training...\n",
            "[Linear] Epoch 1/2 - Train Acc: 80.98%, Val Acc: 81.67%\n",
            "[Linear] Epoch 2/2 - Train Acc: 82.50%, Val Acc: 83.39%\n",
            "Run with seed 44 finished with final Val Acc: 83.39%\n",
            "\n",
            "=== Summary over runs ===\n",
            "Final Linear Probe Validation Accuracies: [80.35185185185185, 82.5, 83.38888888888889]\n",
            "Mean Accuracy: 82.08%\n",
            "Standard Deviation Accuracy: 1.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define the list of seeds for each run\n",
        "    seeds = [42, 43, 44]\n",
        "    results = []  # Will store the final linear probe validation accuracies\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n=== Starting run with seed {seed} ===\")\n",
        "        set_seed(seed)\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        data_dir = prepare_data()\n",
        "\n",
        "        # Prepare datasets and dataloaders for contrastive and evaluation\n",
        "        contrastive_dataset = datasets.ImageFolder(data_dir, transform=TwoCropsTransform(simclr_transform))\n",
        "        contrastive_loader = DataLoader(contrastive_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True, drop_last=True)\n",
        "\n",
        "        full_dataset = datasets.ImageFolder(data_dir, transform=eval_transform)\n",
        "        train_len = int(0.8 * len(full_dataset))\n",
        "        val_len = len(full_dataset) - train_len\n",
        "        train_set, val_set = random_split(full_dataset, [train_len, val_len])\n",
        "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=32)\n",
        "\n",
        "        # Initialize base encoder and SimCLR model\n",
        "        pretrained = False\n",
        "        base_encoder = resnet18(weights=None if not pretrained else \"DEFAULT\")\n",
        "        simclr_model = SimCLRModel(base_encoder, proj_dim=CONFIG[\"PROJ_DIM\"])\n",
        "        optimizer = optim.Adam(simclr_model.parameters(), lr=CONFIG[\"LR\"])\n",
        "        loss_fn = NTXentLoss(CONFIG[\"BATCH_SIZE\"], temperature=0.5, device=device)\n",
        "\n",
        "        print(\"Starting SimCLR training...\")\n",
        "        train_simclr(simclr_model, contrastive_loader, optimizer, loss_fn, device, CONFIG[\"EPOCHS_SIMCLR\"])\n",
        "\n",
        "        print(\"Saving encoder...\")\n",
        "        torch.save(simclr_model.state_dict(), f\"simclr_model_seed{seed}.pth\")\n",
        "\n",
        "        print(\"Starting linear probe training...\")\n",
        "        final_val_acc = train_linear_probe(simclr_model.encoder, train_loader, val_loader, device, CONFIG[\"EPOCHS_LINEAR\"], CONFIG[\"LR\"], seed)\n",
        "        results.append(final_val_acc)\n",
        "        print(f\"Run with seed {seed} finished with final Val Acc: {final_val_acc:.2f}%\")\n",
        "\n",
        "    # Compute and print overall mean and standard deviation of the final validation accuracies\n",
        "    mean_acc = np.mean(results)\n",
        "    std_acc = np.std(results)\n",
        "    print(\"\\n=== Summary over runs ===\")\n",
        "    print(f\"Final Linear Probe Validation Accuracies: {results}\")\n",
        "    print(f\"Mean Accuracy: {mean_acc:.2f}%\")\n",
        "    print(f\"Standard Deviation Accuracy: {std_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71S-cLKvS07P",
        "outputId": "d0154a2e-f2e6-475c-cb50-c70402fc81c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting run with seed 42 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/2 - Loss: 4.5780\n",
            "[SimCLR] Epoch 2/2 - Loss: 4.2769\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n",
            "Starting linear probe training...\n",
            "[Linear] Epoch 1/2 - Train Acc: 60.72%, Val Acc: 61.83%\n",
            "[Linear] Epoch 2/2 - Train Acc: 61.83%, Val Acc: 60.17%\n",
            "Run with seed 42 finished with final Val Acc: 60.17%\n",
            "\n",
            "=== Starting run with seed 43 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/2 - Loss: 4.5547\n",
            "[SimCLR] Epoch 2/2 - Loss: 4.2572\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n",
            "Starting linear probe training...\n",
            "[Linear] Epoch 1/2 - Train Acc: 59.77%, Val Acc: 60.80%\n",
            "[Linear] Epoch 2/2 - Train Acc: 61.25%, Val Acc: 62.41%\n",
            "Run with seed 43 finished with final Val Acc: 62.41%\n",
            "\n",
            "=== Starting run with seed 44 ===\n",
            "Starting SimCLR training...\n",
            "[SimCLR] Epoch 1/2 - Loss: 4.4806\n",
            "[SimCLR] Epoch 2/2 - Loss: 4.1867\n",
            "Finished SimCLR pretraining.\n",
            "Saving encoder...\n",
            "Starting linear probe training...\n",
            "[Linear] Epoch 1/2 - Train Acc: 61.06%, Val Acc: 60.89%\n",
            "[Linear] Epoch 2/2 - Train Acc: 62.45%, Val Acc: 63.30%\n",
            "Run with seed 44 finished with final Val Acc: 63.30%\n",
            "\n",
            "=== Summary over runs ===\n",
            "Final Linear Probe Validation Accuracies: [60.16666666666667, 62.40740740740741, 63.2962962962963]\n",
            "Mean Accuracy: 61.96%\n",
            "Standard Deviation Accuracy: 1.32%\n"
          ]
        }
      ]
    }
  ]
}