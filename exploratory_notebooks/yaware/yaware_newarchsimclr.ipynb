{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5325f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Implementation of the SIMCLR with resnet18 backbone\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()       # reads .env and sets os.environ\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1e1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine_loss import HaversineRBFNTXenLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1e0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "from utils.version_utils import print_versions, configure_gpu_device, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74f1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr.data.datamodule import compute_mean_std, prepare_data, combine_train_val_loaders, SimCLRDataset, get_split_indexes\n",
    "from simclr.data.transforms import  get_transforms\n",
    "from simclr.models.loss import NTXentLoss\n",
    "from simclr.probes.logistic import get_probe_loaders, run_logistic_probe_experiment\n",
    "from simclr.utils.scheduler import make_optimizer_scheduler\n",
    "from simclr.utils.misc import evaluate\n",
    "from simclr.data.mydataloaders import get_data_loaders_train_test_linear_probe\n",
    "from simclr.config import CONFIG\n",
    "from simclr.train import train_simclr\n",
    "import argparse\n",
    "from new_architecture_simclr.network import resnet18, projection_MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d4ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from information_extraction import get_data_loaders, EuroSATDataset, extract_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f46a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda version: 25.5.1\n",
      "Python version: 3.10.13\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "Torchvision version: 0.20.1\n",
      "Successfully set to use GPU: 0 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:0\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 0\n",
      "Dummy tensor is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print_versions()\n",
    "set_seed(seed=42)\n",
    "TARGET_GPU_INDEX = CONFIG[\"TARGET_GPU_INDEX\"] if \"TARGET_GPU_INDEX\" in CONFIG else 0  # Default to 0 if not set\n",
    "DEVICE = configure_gpu_device(TARGET_GPU_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1f36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent nondeterminism\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# split fractions\n",
    "TRAIN_FRAC = CONFIG[\"TRAIN_FRAC\"]\n",
    "VAL_FRAC   = CONFIG[\"VAL_FRAC\"]\n",
    "TEST_FRAC  = CONFIG[\"TEST_FRAC\"]\n",
    "\n",
    "SEED = CONFIG[\"SEED\"]\n",
    "\n",
    "PRETRAINED = False\n",
    "\n",
    "TEMPERATURE = CONFIG[\"TEMPERATURE\"]\n",
    "\n",
    "BETAS=(0.9,0.98)\n",
    "EPS = 1e-8\n",
    "\n",
    "GLOBAL_SEED = CONFIG[\"SEED\"]\n",
    "NUM_WORKERS = CONFIG[\"NUM_WORKERS\"]\n",
    "\n",
    "EUROSAT_IMAGE_SIZE = (64, 64)\n",
    "MODEL_INPUT_SIZE = [224, 224]\n",
    "EPOCH_SAVE_INTERVAL = CONFIG[\"EPOCH_SAVE_INTERVAL\"]\n",
    "\n",
    "MS_PATH  = \"/users/c/carvalhj/datasets/eurosat/EuroSAT_MS/\"\n",
    "RGB_PATH = \"/users/c/carvalhj/datasets/eurosat/EuroSAT_RGB/\"\n",
    "\n",
    "BATCH_SIZE = CONFIG[\"BATCH_SIZE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324c2728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting run with seed 42 ===\n",
      "Batchsize= 256\n",
      "Loading metadata from cache (eurosat_metadata_cache.pkl)\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Loaders: train=84, val=11, test=11 batches\n"
     ]
    }
   ],
   "source": [
    "seeds = [GLOBAL_SEED]\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "    print(f\"Batchsize= {BATCH_SIZE}\")\n",
    "    loaders = get_data_loaders(MS_PATH, RGB_PATH, batch_size=BATCH_SIZE)\n",
    "    train_loader, val_loader, test_loader, val_subset_no_transform, num_classes = loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549abd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(dataset='eurosat', model='resnet18', n_classes=10, feature_dim=512, proj_dim=64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"SimCLR EuroSAT\")\n",
    "parser.add_argument(\"--dataset\",    type=str,   default=\"eurosat\",\n",
    "                    help=\"dataset name (controls CIFAR‑stem in network.py)\")\n",
    "parser.add_argument(\"--model\",      type=str,   default=\"resnet18\",\n",
    "                    choices=[\"resnet18\",\"resnet34\",\"resnet50\",\"resnet101\",\"resnet152\"],\n",
    "                    help=\"which ResNet depth to use\")\n",
    "parser.add_argument(\"--n_classes\",  type=int,   default=10,\n",
    "                    help=\"# of EuroSAT semantic classes\")\n",
    "parser.add_argument(\"--feature_dim\",type=int,   default=512,\n",
    "                    help=\"backbone output dim (for SimCLR we set fc→feature_dim)\")\n",
    "parser.add_argument(\"--proj_dim\",   type=int,   default=CONFIG[\"PROJ_DIM\"],\n",
    "                    help=\"projection MLP output dim (usually 128)\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c63b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_encoder = resnet18(\n",
    "    args,\n",
    "    num_classes=args.feature_dim,     # make fc output = feature_dim\n",
    "    zero_init_residual=False\n",
    ")\n",
    "proj_head = projection_MLP(args)\n",
    "\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, base_encoder, proj_head):\n",
    "        super().__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.projection_head = proj_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        proj = self.projection_head(feat)\n",
    "        return feat, proj\n",
    "    \n",
    "simclr_model = SimCLRModel(base_encoder, proj_head).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d7a914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([256, 3, 64, 64]), Meta shape: torch.Size([256, 2]), torch.Size([256, 3, 64, 64])\n",
      "<information_extraction.SimCLRWithMetaDataset object at 0x7f2f92bd4b20>\n"
     ]
    }
   ],
   "source": [
    "img, img2, meta = next(iter(train_loader))\n",
    "print(f\"Image shape: {img.shape}, Meta shape: {meta.shape}, {img2.shape}\")\n",
    "print(train_loader.dataset)\n",
    "# get just the image from the dataset, discarding the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dfcf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = HaversineRBFNTXenLoss(temperature=0.9, sigma=0.003).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cd7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting run with seed 42 ===\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Loaders: train=84, val=11, test=11 batches\n",
      "Starting SimCLR training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (2.7s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/yaware/wandb/run-20250717_141015-n3y99q5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/n3y99q5t' target=\"_blank\">BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS2</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/n3y99q5t' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/n3y99q5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final contrastive accuracy on val split: 4.78%\n",
      "Final contrastive accuracy on train split: 4.64%\n",
      "Final kNN (k=5) on train: 100.00%\n",
      "Final kNN (k=5) on val: 78.37%\n",
      "Model saved to models/simclr_seed42_bs256_temp0.2_Tepochs2_lr0.000375.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>contrastive_train_acc</td><td>▁▁</td></tr><tr><td>contrastive_val_acc</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>final_contrastive_accuracy</td><td>▁</td></tr><tr><td>final_contrastive_accuracy_train</td><td>▁</td></tr><tr><td>final_knn_acc</td><td>▁</td></tr><tr><td>knn_val_acc</td><td>▁▁</td></tr><tr><td>logistic_probe_acc</td><td>▁▁</td></tr><tr><td>logistic_probe_train_acc</td><td>▁▁</td></tr><tr><td>simclr_train_loss</td><td>█▁</td></tr><tr><td>simclr_val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>contrastive_train_acc</td><td>0</td></tr><tr><td>contrastive_val_acc</td><td>0</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>final_contrastive_accuracy</td><td>0.04778</td></tr><tr><td>final_contrastive_accuracy_train</td><td>0.04636</td></tr><tr><td>final_knn_acc</td><td>0.7837</td></tr><tr><td>knn_val_acc</td><td>0</td></tr><tr><td>logistic_probe_acc</td><td>0</td></tr><tr><td>logistic_probe_train_acc</td><td>0</td></tr><tr><td>simclr_train_loss</td><td>5.50941</td></tr><tr><td>simclr_val_loss</td><td>5.20015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS2</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/n3y99q5t' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/n3y99q5t</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250717_141015-n3y99q5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All runs completed.\n"
     ]
    }
   ],
   "source": [
    "seeds = [GLOBAL_SEED]\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "    \n",
    "    loaders = get_data_loaders(MS_PATH, RGB_PATH, batch_size=BATCH_SIZE)\n",
    "    train_loader, val_loader, test_loader, val_subset_no_transform, num_classes = loaders\n",
    "\n",
    "    wd =  0.5 \n",
    "    optimizer, scheduler = make_optimizer_scheduler(\n",
    "        simclr_model.parameters(),\n",
    "        CONFIG[\"LR\"],\n",
    "        CONFIG[\"WD\"],\n",
    "        len(train_loader),\n",
    "        CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "        )\n",
    "    \n",
    "    bs = CONFIG[\"BATCH_SIZE\"]\n",
    "    loss_fn = NTXentLoss(bs, temperature=TEMPERATURE, device=DEVICE)\n",
    "\n",
    "    print(\"Starting SimCLR training...\")\n",
    "    epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "    lr = CONFIG[\"LR\"]\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"eurosat-contrastive-scratch\",\n",
    "        name=f\"BS{bs}_LR{lr:.0e}_SEED{seed}_TEMPERATURE{TEMPERATURE}_EPOCHS{epochs_simclr}\",\n",
    "        tags=[\"SimCLR\", \"EuroSAT\", \"Contrastive Learning\"],\n",
    "        config={\n",
    "            \"seed\": seed,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"model\": \"SimCLR\",\n",
    "            \"dataset\": \"EuroSAT\",\n",
    "            \"batch_size\": bs,\n",
    "            \"learning_rate\": CONFIG[\"LR\"],\n",
    "            \"epochs\": CONFIG[\"EPOCHS_SIMCLR\"],\n",
    "            \"proj_dim\": CONFIG[\"PROJ_DIM\"],\n",
    "            \"feature_dim\": CONFIG[\"FEATURE_DIM\"],\n",
    "            \"pretrained\": PRETRAINED,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    eval_transform, augment_transform = get_transforms(\n",
    "        mean =CONFIG[\"MEAN\"],\n",
    "        std = CONFIG[\"STD\"]\n",
    "    )  # these must match the transforms used in test_loader\n",
    "\n",
    "    probe_train_loader, probe_val_loader = get_probe_loaders(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        eval_transform,               # must match transforms used in test_loader\n",
    "        probe_batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "        yaware=True\n",
    "\n",
    "    )\n",
    "\n",
    "    train_simclr(\n",
    "        simclr_model,\n",
    "        train_loader, val_loader,\n",
    "        probe_train_loader, probe_val_loader,\n",
    "        optimizer, loss_fn, DEVICE,\n",
    "        simclr_epochs=CONFIG[\"EPOCHS_SIMCLR\"],\n",
    "        probe_lr=CONFIG[\"LR_LINEAR\"],\n",
    "        probe_epochs=1,            # 1 pass per epoch is typical\n",
    "        feature_dim=CONFIG[\"FEATURE_DIM\"],\n",
    "        num_classes=num_classes,\n",
    "        augment_transform=augment_transform,\n",
    "        val_subset_no_transform=val_subset_no_transform,\n",
    "        wandb_run=wandb_run,\n",
    "        scheduler=scheduler,\n",
    "        seed=seed,\n",
    "        yaware=True  # Set to True for Yaware model\n",
    "    )\n",
    "\n",
    "    wandb_run.finish()\n",
    "\n",
    "print(\"All runs completed.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c7dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean: [0.34493255615234375, 0.38065004348754883, 0.4079166650772095]\n",
      "Using std: [0.09294305741786957, 0.06471628695726395, 0.05417811498045921]\n",
      "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Train/Test loaders: 94/11 batches\n",
      "Successfully set to use GPU: 0 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:0\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 0\n",
      "Dummy tensor is on device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/yaware/wandb/run-20250717_141524-8amtgo40</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/8amtgo40' target=\"_blank\">logistic_probe_seed42_temperature0.2_bs256</a></strong> to <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/8amtgo40' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/8amtgo40</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] train+val loader has 94 batches\n",
      "Fitting logistic regression probe…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Probe] Train+Val Acc: 66.51%,  Test Acc: 71.30%\n",
      "Saved probe + encoder to models/logistic_probe_seed42_bs256.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6651429521276596, 0.7129629629629629)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the saved model and run linear probe\n",
    "seed = CONFIG[\"SEED\"]\n",
    "bs = CONFIG[\"BATCH_SIZE\"]\n",
    "epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "simclr_lr = CONFIG[\"LR\"]\n",
    "lr_str = f\"{simclr_lr:.0e}\" if simclr_lr < 0.0001 else f\"{simclr_lr:.6f}\"\n",
    "model_path = f\"models/simclr_seed{seed}_bs{bs}_temp{TEMPERATURE}_Tepochs{epochs_simclr}_lr{lr_str}.pth\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model {model_path} does not exist. Please run the SimCLR pretraining first.\")\n",
    "\n",
    "checkpoint_path = model_path\n",
    "state_dict = torch.load(checkpoint_path, map_location=torch.device(DEVICE), weights_only=True)\n",
    "simclr_model.load_state_dict(state_dict)\n",
    "\n",
    "# Perform linear probe on train+val as train set, and test as test set\n",
    "train_loader, test_loader, num_classes = get_data_loaders_train_test_linear_probe(CONFIG[\"DATA_DIR_LOCAL\"], CONFIG[\"BATCH_SIZE\"])\n",
    "run_logistic_probe_experiment(\n",
    "    42,\n",
    "    train_loader,\n",
    "    None,  # No validation loader for linear probe\n",
    "    test_loader,\n",
    "    num_classes,\n",
    "    simclr_model,\n",
    "    bs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c41a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grid search for best hyperparameters\n",
    "\n",
    "batch_sizes_epochs = [\n",
    "    (64, 35),\n",
    "    (128, 40),\n",
    "    (256, 100),\n",
    "    (512, 100),\n",
    "    (1024, 150),\n",
    "]\n",
    "\n",
    "learning_rates = [\n",
    "    1e-3,\n",
    "    3.75e-4,\n",
    "    1e-4,\n",
    "    3.75e-5,\n",
    "    1e-5,\n",
    "]\n",
    "\n",
    "# use linspace for computing the temperature\n",
    "temperatures = np.linspace(0.05, 0.5, 5).tolist() # [0.05, 0.1625, 0.275, 0.3875, 0.5]\n",
    "temperatures.append(0.2)  # add the original temperature\n",
    "\n",
    "gpu_indexes = [0, 1]\n",
    "# put half of the experiments on each GPU\n",
    "gpu_experiments = {0: [], 1: []}\n",
    "all_acc = []\n",
    "\n",
    "# train simclr with different hyperparameters and apply linear probe\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaware_eurosat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
