{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, urllib.request, zipfile\n",
        "\n",
        "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
        "LOCAL_OR_COLAB = \"COLAB\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 10\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# split fractions\n",
        "TRAIN_FRAC = 0.6\n",
        "VAL_FRAC   = 0.2\n",
        "TEST_FRAC  = 0.2\n",
        "\n",
        "# hyperparameter grid\n",
        "BATCH_SIZES = [32]\n",
        "GRID        = [\n",
        "    (2e-4,    0.1  ),  # SimCLR\n",
        "    (1.875e-4,0.5  ),  # SatMIP\n",
        "    (3.75e-4, 0.5  ),  # SatMIPS\n",
        "]\n",
        "\n",
        "# ─── DATASET DOWNLOAD ────────────────────────────────────────────────────────────\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/home/juliana/internship_LINUX/datasets/EuroSAT_RGB\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "# ─── HELPERS ─────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485,0.456,0.406],\n",
        "            std =[0.229,0.224,0.225]\n",
        "        )\n",
        "    ])\n",
        "    ds = datasets.ImageFolder(root=data_dir, transform=tf)\n",
        "    n   = len(ds)\n",
        "    n_train = int(TRAIN_FRAC * n)\n",
        "    n_val   = int(VAL_FRAC   * n)\n",
        "    n_test  = n - n_train - n_val\n",
        "    train_ds, val_ds, test_ds = random_split(ds, [n_train, n_val, n_test])\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size, shuffle=True),\n",
        "        DataLoader(val_ds,   batch_size, shuffle=False),\n",
        "        DataLoader(test_ds,  batch_size, shuffle=False),\n",
        "        len(ds.classes)\n",
        "    )\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet18(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "    model.train()\n",
        "    tot_loss, corr, tot = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss   = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if sched: sched.step()\n",
        "        tot_loss += loss.item()\n",
        "        preds    = logits.argmax(dim=1)\n",
        "        corr    += (preds==yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "    return tot_loss/len(loader), 100*corr/tot\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    corr, tot = 0,0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            corr += (preds==yb).sum().item()\n",
        "            tot  += yb.size(0)\n",
        "    return 100 * corr / tot\n",
        "\n",
        "# ─── PHASE 1: GRID SEARCH ────────────────────────────────────────────────────────\n",
        "def hyperparam_search():\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "    # loop over all combos in one go\n",
        "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd}\")\n",
        "        set_seed(SEED)\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "        model = build_model(n_cls, pretrained=False)\n",
        "\n",
        "        # optimizer + paper schedule\n",
        "        opt = optim.AdamW(model.parameters(),\n",
        "                          lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "        total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "        warmup_steps = len(tr_dl)\n",
        "        sched = SequentialLR(\n",
        "            opt,\n",
        "            schedulers=[\n",
        "                LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "                CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "            ],\n",
        "            milestones=[warmup_steps]\n",
        "        )\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # train & validate\n",
        "        for ep in range(NUM_EPOCHS):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "            val_acc          = evaluate(model, val_dl)\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train={tr_acc:.1f}%  val={val_acc:.1f}%\")\n",
        "\n",
        "        # pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_cfg = (bs, lr, wd)\n",
        "            best_model = copy.deepcopy(model)   # store the weights\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]} \"\n",
        "          f\"→ val={best_val:.1f}%\")\n",
        "    return best_cfg, best_model\n",
        "\n",
        "# ─── PHASE 2: LINEAR PROBE ───────────────────────────────────────────────────────\n",
        "def linear_probe(frozen_model, train_dl, test_dl, lr, wd):\n",
        "    # freeze backbone\n",
        "    for p in frozen_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # new head\n",
        "    n_in = frozen_model.fc.in_features\n",
        "    n_out = frozen_model.fc.out_features\n",
        "    frozen_model.fc = nn.Linear(n_in, n_out).to(DEVICE)\n",
        "\n",
        "    opt = optim.AdamW(frozen_model.fc.parameters(),\n",
        "                      lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n>>> Running linear probe on frozen backbone\")\n",
        "    for ep in range(NUM_EPOCHS):\n",
        "        loss, acc = train_one_epoch(frozen_model, train_dl, opt, crit, sched=None)\n",
        "        print(f\"  Probe Ep{ep+1}/{NUM_EPOCHS}: train={acc:.1f}%\")\n",
        "    test_acc = evaluate(frozen_model, test_dl)\n",
        "    print(f\"→ Probe test acc: {test_acc:.1f}%\")\n",
        "    return test_acc\n",
        "\n",
        "# ─── MAIN ───────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    best_cfg, best_model = hyperparam_search()\n",
        "    # rebuild loaders once more so we have the same splits\n",
        "    bs, lr, wd = best_cfg\n",
        "    tr_dl, val_dl, te_dl, _ = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "    # Option A: probe on just the original training split\n",
        "    probe_acc = linear_probe(best_model, tr_dl, te_dl, lr, wd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AD0e5irzQc0",
        "outputId": "443d9973-313e-4506-8395-8af4a9a1dfed",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:09:05.901368Z",
          "iopub.execute_input": "2025-04-22T13:09:05.901785Z",
          "iopub.status.idle": "2025-04-22T13:20:26.682044Z",
          "shell.execute_reply.started": "2025-04-22T13:09:05.901745Z",
          "shell.execute_reply": "2025-04-22T13:20:26.681120Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n>>> Testing BS=32, LR=2.0e-04, WD=0.1\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  Ep1/10: train=79.1%  val=92.3%\n  Ep2/10: train=92.7%  val=95.2%\n  Ep3/10: train=95.2%  val=96.4%\n  Ep4/10: train=97.1%  val=96.1%\n  Ep5/10: train=98.2%  val=96.4%\n  Ep6/10: train=99.0%  val=97.3%\n  Ep7/10: train=99.4%  val=97.5%\n  Ep8/10: train=99.8%  val=97.9%\n  Ep9/10: train=99.9%  val=97.9%\n  Ep10/10: train=99.9%  val=98.0%\n\n>>> Testing BS=32, LR=1.9e-04, WD=0.5\n  Ep1/10: train=78.8%  val=92.9%\n  Ep2/10: train=92.7%  val=94.8%\n  Ep3/10: train=95.5%  val=95.7%\n  Ep4/10: train=97.1%  val=96.5%\n  Ep5/10: train=98.0%  val=96.8%\n  Ep6/10: train=98.8%  val=96.7%\n  Ep7/10: train=99.3%  val=97.2%\n  Ep8/10: train=99.7%  val=97.7%\n  Ep9/10: train=99.9%  val=97.7%\n  Ep10/10: train=99.9%  val=97.9%\n\n>>> Testing BS=32, LR=3.8e-04, WD=0.5\n  Ep1/10: train=81.1%  val=91.0%\n  Ep2/10: train=90.8%  val=94.8%\n  Ep3/10: train=93.7%  val=92.5%\n  Ep4/10: train=95.8%  val=95.3%\n  Ep5/10: train=96.8%  val=95.2%\n  Ep6/10: train=98.2%  val=96.2%\n  Ep7/10: train=99.0%  val=97.1%\n  Ep8/10: train=99.7%  val=97.4%\n  Ep9/10: train=99.8%  val=97.6%\n  Ep10/10: train=99.9%  val=97.6%\n\n>>> Best config: BS=32, LR=2.0e-04, WD=0.1 → val=98.0%\n\n>>> Running linear probe on frozen backbone\n  Probe Ep1/10: train=95.8%\n  Probe Ep2/10: train=98.5%\n  Probe Ep3/10: train=98.7%\n  Probe Ep4/10: train=98.6%\n  Probe Ep5/10: train=98.6%\n  Probe Ep6/10: train=98.7%\n  Probe Ep7/10: train=98.8%\n  Probe Ep8/10: train=98.7%\n  Probe Ep9/10: train=98.6%\n  Probe Ep10/10: train=98.8%\n→ Probe test acc: 99.2%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B (train head on train+val):\n",
        "merged = torch.utils.data.ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "merged_dl = DataLoader(merged, bs, shuffle=True)\n",
        "probe_acc = linear_probe(best_model, merged_dl, te_dl, lr, wd)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:23:12.750347Z",
          "iopub.execute_input": "2025-04-22T13:23:12.750684Z",
          "iopub.status.idle": "2025-04-22T13:25:30.744259Z",
          "shell.execute_reply.started": "2025-04-22T13:23:12.750660Z",
          "shell.execute_reply": "2025-04-22T13:25:30.743546Z"
        },
        "id": "kIWk1ugaMZp3",
        "outputId": "65bd1247-f7b1-4c70-e107-7fb1c30b9f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n>>> Running linear probe on frozen backbone\n  Probe Ep1/10: train=96.7%\n  Probe Ep2/10: train=98.8%\n  Probe Ep3/10: train=98.7%\n  Probe Ep4/10: train=98.8%\n  Probe Ep5/10: train=98.8%\n  Probe Ep6/10: train=98.8%\n  Probe Ep7/10: train=98.9%\n  Probe Ep8/10: train=98.8%\n  Probe Ep9/10: train=98.8%\n  Probe Ep10/10: train=98.8%\n→ Probe test acc: 99.2%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear probing with scikit learn"
      ],
      "metadata": {
        "id": "kOms5jYnMZp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    # remove last classifier layer\n",
        "    backbone = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    backbone.to(device)\n",
        "    all_feats, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(loader, desc=\"Extracting\"):\n",
        "            xb = xb.to(device)\n",
        "            feats = backbone(xb)           # shape: (B, C, 1, 1)\n",
        "            feats = feats.view(feats.size(0), -1)  # (B, C)\n",
        "            all_feats.append(feats.cpu().numpy())\n",
        "            all_labels.append(yb.numpy())\n",
        "    return np.vstack(all_feats), np.concatenate(all_labels)\n",
        "\n",
        "# 1) Extract embeddings from frozen best_model\n",
        "X_train, y_train = extract_embeddings(best_model, tr_dl, DEVICE)\n",
        "X_test,  y_test  = extract_embeddings(best_model, te_dl, DEVICE)\n",
        "\n",
        "# 2) Fit a scikit‑learn “linear probe” (logistic regression)\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.preprocessing   import StandardScaler\n",
        "from sklearn.metrics         import accuracy_score, classification_report\n",
        "\n",
        "# scale features\n",
        "scaler  = StandardScaler().fit(X_train)\n",
        "X_tr_s  = scaler.transform(X_train)\n",
        "X_te_s  = scaler.transform(X_test)\n",
        "\n",
        "# C ≃ 1/weight_decay — try a small grid\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='saga',\n",
        "    multi_class='multinomial',\n",
        "    max_iter=200\n",
        ").fit(X_tr_s, y_train)\n",
        "\n",
        "# 3) Evaluate\n",
        "preds = clf.predict(X_te_s)\n",
        "acc   = accuracy_score(y_test, preds)\n",
        "print(f\"sklearn probe test accuracy: {acc*100:.2f}%\")\n",
        "print(classification_report(y_test, preds, digits=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T13:33:59.313768Z",
          "iopub.execute_input": "2025-04-22T13:33:59.314333Z",
          "iopub.status.idle": "2025-04-22T13:36:03.813977Z",
          "shell.execute_reply.started": "2025-04-22T13:33:59.314310Z",
          "shell.execute_reply": "2025-04-22T13:36:03.813245Z"
        },
        "id": "7EZBzJVYMZp4",
        "outputId": "fbe6cee1-2bba-4e4d-f801-07051e8fe1bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Extracting: 100%|██████████| 507/507 [00:10<00:00, 50.00it/s]\nExtracting: 100%|██████████| 169/169 [00:03<00:00, 51.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "sklearn probe test accuracy: 99.28%\n              precision    recall  f1-score   support\n\n           0     0.9900    0.9967    0.9933       598\n           1     0.9950    0.9934    0.9942       602\n           2     0.9848    0.9864    0.9856       590\n           3     0.9897    0.9938    0.9917       482\n           4     0.9980    0.9960    0.9970       506\n           5     0.9868    0.9894    0.9881       379\n           6     0.9900    0.9841    0.9871       504\n           7     0.9951    0.9983    0.9967       605\n           8     0.9980    0.9884    0.9932       516\n           9     0.9984    0.9984    0.9984       618\n\n    accuracy                         0.9928      5400\n   macro avg     0.9926    0.9925    0.9925      5400\nweighted avg     0.9928    0.9928    0.9928      5400\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}