{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, zipfile, urllib\n",
        "\n",
        "LOCAL_OR_COLAB = \"LOCAL\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 100\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TRAIN_FRAC = 0.8\n",
        "VAL_FRAC   = 0.1\n",
        "TEST_FRAC  = 0.1\n",
        "\n",
        "# hyperparameter grid\n",
        "BATCH_SIZES = [256, 512] \n",
        "LRS = [1e-4]\n",
        "\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/users/c/carvalhj/datasets/EuroSAT_RGB/\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=256, LR=1.0e-04\n",
            "Computing mean and std from training set...\n",
            "Computed mean: [0.3438493311405182, 0.38001248240470886, 0.4077288508415222]\n",
            "Computed std:  [0.09294864535331726, 0.06473352760076523, 0.05418824777007103]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250604_190659-mci4kyg7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/mci4kyg7' target=\"_blank\">BS256_LR1e-04</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/mci4kyg7' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/mci4kyg7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/100: train=48.1%  val=60.2%\n",
            "  Ep2/100: train=63.7%  val=66.7%\n",
            "  Ep3/100: train=68.8%  val=67.8%\n",
            "  Ep4/100: train=70.8%  val=68.3%\n",
            "  Ep5/100: train=71.7%  val=68.4%\n",
            "  Ep6/100: train=71.8%  val=69.4%\n",
            "  Ep7/100: train=72.2%  val=68.9%\n",
            "  Ep8/100: train=72.4%  val=68.2%\n",
            "  Ep9/100: train=72.4%  val=68.9%\n",
            "  Ep10/100: train=72.4%  val=68.9%\n",
            "  Ep11/100: train=72.3%  val=69.0%\n",
            "  Ep12/100: train=72.1%  val=68.8%\n",
            "  Ep13/100: train=72.2%  val=69.3%\n",
            "  Ep14/100: train=72.3%  val=68.6%\n",
            "  Ep15/100: train=72.0%  val=68.6%\n",
            "  Ep16/100: train=72.4%  val=69.5%\n",
            "  Ep17/100: train=72.0%  val=69.0%\n",
            "  Ep18/100: train=72.4%  val=69.1%\n",
            "  Ep19/100: train=72.1%  val=68.9%\n",
            "  Ep20/100: train=72.0%  val=68.8%\n",
            "  Ep21/100: train=72.4%  val=69.0%\n",
            "  Ep22/100: train=72.3%  val=68.4%\n",
            "  Ep23/100: train=72.2%  val=68.8%\n",
            "  Ep24/100: train=72.1%  val=69.1%\n",
            "  Ep25/100: train=71.8%  val=68.8%\n",
            "  Ep26/100: train=72.3%  val=69.1%\n",
            "  Ep27/100: train=72.1%  val=68.9%\n",
            "  Ep28/100: train=72.5%  val=68.9%\n",
            "  Ep29/100: train=72.3%  val=68.9%\n",
            "  Ep30/100: train=72.4%  val=68.9%\n",
            "  Ep31/100: train=72.2%  val=68.7%\n",
            "  Ep32/100: train=71.9%  val=68.9%\n",
            "  Ep33/100: train=72.3%  val=68.9%\n",
            "  Ep34/100: train=72.5%  val=68.9%\n",
            "  Ep35/100: train=72.5%  val=69.0%\n",
            "  Ep36/100: train=72.7%  val=68.9%\n",
            "  Ep37/100: train=72.2%  val=69.1%\n",
            "  Ep38/100: train=72.0%  val=69.4%\n",
            "  Ep39/100: train=72.4%  val=68.2%\n",
            "  Ep40/100: train=72.0%  val=69.0%\n",
            "  Ep41/100: train=72.4%  val=68.8%\n",
            "  Ep42/100: train=72.1%  val=69.1%\n",
            "  Ep43/100: train=71.8%  val=68.9%\n",
            "  Ep44/100: train=72.0%  val=68.7%\n",
            "  Ep45/100: train=71.9%  val=68.9%\n",
            "  Ep46/100: train=72.4%  val=68.7%\n",
            "  Ep47/100: train=72.3%  val=68.5%\n",
            "  Ep48/100: train=72.4%  val=68.9%\n",
            "  Ep49/100: train=72.2%  val=68.4%\n",
            "  Ep50/100: train=72.3%  val=68.4%\n",
            "  Ep51/100: train=72.8%  val=68.9%\n",
            "  Ep52/100: train=71.7%  val=68.7%\n",
            "  Ep53/100: train=72.6%  val=68.6%\n",
            "  Ep54/100: train=72.4%  val=69.0%\n",
            "  Ep55/100: train=72.2%  val=69.1%\n",
            "  Ep56/100: train=72.1%  val=69.2%\n",
            "  Ep57/100: train=72.2%  val=69.0%\n",
            "  Ep58/100: train=72.7%  val=69.3%\n",
            "  Ep59/100: train=72.3%  val=68.9%\n",
            "  Ep60/100: train=72.4%  val=69.1%\n",
            "  Ep61/100: train=72.1%  val=68.7%\n",
            "  Ep62/100: train=72.5%  val=68.9%\n",
            "  Ep63/100: train=72.1%  val=68.8%\n",
            "  Ep64/100: train=72.3%  val=69.2%\n",
            "  Ep65/100: train=72.1%  val=68.5%\n",
            "  Ep66/100: train=72.2%  val=68.6%\n",
            "  Ep67/100: train=72.5%  val=68.9%\n",
            "  Ep68/100: train=72.3%  val=69.4%\n",
            "  Ep69/100: train=72.4%  val=69.0%\n",
            "  Ep70/100: train=72.4%  val=69.4%\n",
            "  Ep71/100: train=72.2%  val=68.7%\n",
            "  Ep72/100: train=72.2%  val=69.1%\n",
            "  Ep73/100: train=72.1%  val=69.6%\n",
            "  Ep74/100: train=72.6%  val=69.0%\n",
            "  Ep75/100: train=72.2%  val=69.1%\n",
            "  Ep76/100: train=72.1%  val=69.2%\n",
            "  Ep77/100: train=72.2%  val=68.7%\n",
            "  Ep78/100: train=72.7%  val=68.0%\n",
            "  Ep79/100: train=72.1%  val=69.1%\n",
            "  Ep80/100: train=72.2%  val=69.0%\n",
            "  Ep81/100: train=72.3%  val=68.9%\n",
            "  Ep82/100: train=72.2%  val=69.2%\n",
            "  Ep83/100: train=72.5%  val=69.0%\n",
            "  Ep84/100: train=72.1%  val=68.4%\n",
            "  Ep85/100: train=72.4%  val=69.3%\n",
            "  Ep86/100: train=71.9%  val=68.7%\n",
            "  Ep87/100: train=72.4%  val=68.3%\n",
            "  Ep88/100: train=72.3%  val=68.9%\n",
            "  Ep89/100: train=72.3%  val=69.1%\n",
            "  Ep90/100: train=72.0%  val=68.8%\n",
            "  Ep91/100: train=72.3%  val=68.4%\n",
            "  Ep92/100: train=72.2%  val=69.4%\n",
            "  Ep93/100: train=72.0%  val=68.7%\n",
            "  Ep94/100: train=72.2%  val=69.1%\n",
            "  Ep95/100: train=72.1%  val=68.3%\n",
            "  Ep96/100: train=72.2%  val=68.8%\n",
            "  Ep97/100: train=72.4%  val=69.0%\n",
            "  Ep98/100: train=72.1%  val=69.2%\n",
            "  Ep99/100: train=72.5%  val=68.8%\n",
            "  Ep100/100: train=72.1%  val=68.6%\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇█▇█▇█▇█▇▇▇▇█▇▇▇▇▇█▇▇▇▇████▇█▇▇▇▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>train_acc</td><td>72.10648</td></tr><tr><td>train_loss</td><td>0.78771</td></tr><tr><td>val_acc</td><td>68.55556</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS256_LR1e-04</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/mci4kyg7' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/mci4kyg7</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_190659-mci4kyg7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=512, LR=1.0e-04\n",
            "Computing mean and std from training set...\n",
            "Computed mean: [0.3438493609428406, 0.3800123929977417, 0.4077288508415222]\n",
            "Computed std:  [0.09294863790273666, 0.06473352760076523, 0.05418826639652252]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250604_201553-niypkcl8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/niypkcl8' target=\"_blank\">BS512_LR1e-04</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/niypkcl8' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/niypkcl8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/100: train=43.6%  val=51.1%\n",
            "  Ep2/100: train=61.1%  val=63.7%\n",
            "  Ep3/100: train=67.9%  val=67.6%\n",
            "  Ep4/100: train=72.6%  val=70.1%\n",
            "  Ep5/100: train=75.0%  val=71.3%\n",
            "  Ep6/100: train=77.6%  val=71.4%\n",
            "  Ep7/100: train=78.8%  val=71.6%\n",
            "  Ep8/100: train=79.6%  val=71.5%\n",
            "  Ep9/100: train=80.0%  val=71.5%\n",
            "  Ep10/100: train=80.5%  val=72.1%\n",
            "  Ep11/100: train=80.7%  val=71.5%\n",
            "  Ep12/100: train=80.8%  val=72.0%\n",
            "  Ep13/100: train=80.5%  val=71.5%\n",
            "  Ep14/100: train=81.1%  val=72.0%\n",
            "  Ep15/100: train=81.2%  val=71.8%\n",
            "  Ep16/100: train=81.5%  val=71.8%\n",
            "  Ep17/100: train=80.5%  val=71.9%\n",
            "  Ep18/100: train=81.0%  val=71.8%\n",
            "  Ep19/100: train=80.7%  val=71.7%\n",
            "  Ep20/100: train=81.3%  val=72.0%\n",
            "  Ep21/100: train=81.1%  val=72.0%\n",
            "  Ep22/100: train=80.9%  val=71.9%\n",
            "  Ep23/100: train=80.8%  val=71.4%\n",
            "  Ep24/100: train=81.1%  val=71.7%\n",
            "  Ep25/100: train=80.7%  val=71.7%\n",
            "  Ep26/100: train=81.1%  val=71.5%\n",
            "  Ep27/100: train=81.0%  val=71.8%\n",
            "  Ep28/100: train=81.5%  val=71.6%\n",
            "  Ep29/100: train=80.8%  val=71.4%\n",
            "  Ep30/100: train=81.1%  val=71.2%\n",
            "  Ep31/100: train=80.6%  val=71.8%\n",
            "  Ep32/100: train=80.8%  val=71.6%\n",
            "  Ep33/100: train=81.1%  val=71.2%\n",
            "  Ep34/100: train=80.8%  val=71.4%\n",
            "  Ep35/100: train=80.7%  val=71.5%\n",
            "  Ep36/100: train=81.4%  val=71.7%\n",
            "  Ep37/100: train=81.1%  val=71.6%\n",
            "  Ep38/100: train=80.9%  val=71.6%\n",
            "  Ep39/100: train=81.0%  val=71.3%\n",
            "  Ep40/100: train=80.8%  val=71.7%\n",
            "  Ep41/100: train=81.0%  val=71.2%\n",
            "  Ep42/100: train=80.2%  val=71.4%\n",
            "  Ep43/100: train=81.2%  val=71.6%\n",
            "  Ep44/100: train=81.1%  val=71.3%\n",
            "  Ep45/100: train=80.8%  val=71.5%\n",
            "  Ep46/100: train=81.1%  val=71.6%\n",
            "  Ep47/100: train=81.0%  val=71.7%\n",
            "  Ep48/100: train=80.8%  val=71.6%\n",
            "  Ep49/100: train=80.9%  val=71.9%\n",
            "  Ep50/100: train=80.6%  val=72.0%\n",
            "  Ep51/100: train=81.3%  val=71.3%\n",
            "  Ep52/100: train=80.8%  val=72.1%\n",
            "  Ep53/100: train=80.8%  val=71.7%\n",
            "  Ep54/100: train=80.6%  val=71.0%\n",
            "  Ep55/100: train=81.0%  val=71.5%\n",
            "  Ep56/100: train=81.2%  val=71.9%\n",
            "  Ep57/100: train=80.7%  val=71.8%\n",
            "  Ep58/100: train=81.3%  val=71.1%\n",
            "  Ep59/100: train=81.2%  val=72.0%\n",
            "  Ep60/100: train=80.6%  val=72.0%\n",
            "  Ep61/100: train=81.0%  val=71.5%\n",
            "  Ep62/100: train=81.2%  val=71.6%\n",
            "  Ep63/100: train=81.3%  val=71.7%\n",
            "  Ep64/100: train=81.4%  val=71.6%\n",
            "  Ep65/100: train=81.0%  val=71.5%\n",
            "  Ep66/100: train=80.8%  val=71.8%\n",
            "  Ep67/100: train=81.3%  val=71.7%\n",
            "  Ep68/100: train=81.2%  val=71.5%\n",
            "  Ep69/100: train=81.7%  val=71.9%\n",
            "  Ep70/100: train=81.2%  val=71.5%\n",
            "  Ep71/100: train=80.9%  val=72.0%\n",
            "  Ep72/100: train=81.0%  val=71.8%\n",
            "  Ep73/100: train=80.8%  val=71.7%\n",
            "  Ep74/100: train=81.0%  val=72.0%\n",
            "  Ep75/100: train=81.3%  val=71.1%\n",
            "  Ep76/100: train=81.2%  val=71.9%\n",
            "  Ep77/100: train=81.6%  val=71.5%\n",
            "  Ep78/100: train=80.8%  val=71.8%\n",
            "  Ep79/100: train=81.2%  val=70.6%\n",
            "  Ep80/100: train=81.5%  val=71.7%\n",
            "  Ep81/100: train=81.0%  val=71.0%\n",
            "  Ep82/100: train=81.1%  val=71.9%\n",
            "  Ep83/100: train=80.8%  val=71.7%\n",
            "  Ep84/100: train=80.9%  val=71.8%\n",
            "  Ep85/100: train=80.9%  val=71.6%\n",
            "  Ep86/100: train=81.0%  val=71.7%\n",
            "  Ep87/100: train=80.9%  val=71.6%\n",
            "  Ep88/100: train=81.0%  val=71.3%\n",
            "  Ep89/100: train=81.0%  val=71.6%\n",
            "  Ep90/100: train=81.0%  val=72.0%\n",
            "  Ep91/100: train=81.8%  val=71.6%\n",
            "  Ep92/100: train=80.7%  val=71.5%\n",
            "  Ep93/100: train=81.3%  val=71.3%\n",
            "  Ep94/100: train=80.2%  val=71.7%\n",
            "  Ep95/100: train=81.6%  val=72.2%\n",
            "  Ep96/100: train=80.8%  val=71.4%\n",
            "  Ep97/100: train=81.2%  val=71.2%\n",
            "  Ep98/100: train=81.2%  val=72.0%\n",
            "  Ep99/100: train=81.4%  val=72.0%\n",
            "  Ep100/100: train=80.3%  val=71.4%\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_acc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇███████████████████████████▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>train_acc</td><td>80.34722</td></tr><tr><td>train_loss</td><td>0.56053</td></tr><tr><td>val_acc</td><td>71.37037</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS512_LR1e-04</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/niypkcl8' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search/runs/niypkcl8</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_201553-niypkcl8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Best config: BS=512, LR=1.0e-04, val=71.4%\n",
            "Computing mean and std from training set...\n",
            "Computed mean: [0.3444792926311493, 0.3804510235786438, 0.40790361166000366]\n",
            "Computed std:  [0.09314047545194626, 0.06477275490760803, 0.054232291877269745]\n",
            "\n",
            ">>> Running linear probe on frozen backbone\n",
            "  Probe Ep1/100: train=55.1%\n",
            "  Probe Ep2/100: train=73.0%\n",
            "  Probe Ep3/100: train=74.7%\n",
            "  Probe Ep4/100: train=75.5%\n",
            "  Probe Ep5/100: train=75.8%\n",
            "  Probe Ep6/100: train=76.0%\n",
            "  Probe Ep7/100: train=77.0%\n",
            "  Probe Ep8/100: train=77.1%\n",
            "  Probe Ep9/100: train=77.0%\n",
            "  Probe Ep10/100: train=77.4%\n",
            "  Probe Ep11/100: train=77.2%\n",
            "  Probe Ep12/100: train=77.4%\n",
            "  Probe Ep13/100: train=77.7%\n",
            "  Probe Ep14/100: train=77.7%\n",
            "  Probe Ep15/100: train=78.1%\n",
            "  Probe Ep16/100: train=78.3%\n",
            "  Probe Ep17/100: train=78.0%\n",
            "  Probe Ep18/100: train=78.2%\n",
            "  Probe Ep19/100: train=78.2%\n",
            "  Probe Ep20/100: train=78.3%\n",
            "  Probe Ep21/100: train=78.3%\n",
            "  Probe Ep22/100: train=78.5%\n",
            "  Probe Ep23/100: train=78.2%\n",
            "  Probe Ep24/100: train=78.9%\n",
            "  Probe Ep25/100: train=78.8%\n",
            "  Probe Ep26/100: train=78.2%\n",
            "  Probe Ep27/100: train=79.0%\n",
            "  Probe Ep28/100: train=78.4%\n",
            "  Probe Ep29/100: train=78.8%\n",
            "  Probe Ep30/100: train=79.0%\n",
            "  Probe Ep31/100: train=79.2%\n",
            "  Probe Ep32/100: train=78.3%\n",
            "  Probe Ep33/100: train=78.6%\n",
            "  Probe Ep34/100: train=79.4%\n",
            "  Probe Ep35/100: train=79.0%\n",
            "  Probe Ep36/100: train=78.8%\n",
            "  Probe Ep37/100: train=79.1%\n",
            "  Probe Ep38/100: train=79.5%\n",
            "  Probe Ep39/100: train=79.1%\n",
            "  Probe Ep40/100: train=79.4%\n",
            "  Probe Ep41/100: train=79.5%\n",
            "  Probe Ep42/100: train=79.1%\n",
            "  Probe Ep43/100: train=79.1%\n",
            "  Probe Ep44/100: train=79.6%\n",
            "  Probe Ep45/100: train=79.1%\n",
            "  Probe Ep46/100: train=79.5%\n",
            "  Probe Ep47/100: train=79.3%\n",
            "  Probe Ep48/100: train=79.5%\n",
            "  Probe Ep49/100: train=79.6%\n",
            "  Probe Ep50/100: train=79.7%\n",
            "  Probe Ep51/100: train=79.7%\n",
            "  Probe Ep52/100: train=79.6%\n",
            "  Probe Ep53/100: train=79.1%\n",
            "  Probe Ep54/100: train=79.9%\n",
            "  Probe Ep55/100: train=79.6%\n",
            "  Probe Ep56/100: train=79.9%\n",
            "  Probe Ep57/100: train=79.5%\n",
            "  Probe Ep58/100: train=79.9%\n",
            "  Probe Ep59/100: train=79.9%\n",
            "  Probe Ep60/100: train=79.4%\n",
            "  Probe Ep61/100: train=79.5%\n",
            "  Probe Ep62/100: train=80.3%\n",
            "  Probe Ep63/100: train=79.6%\n",
            "  Probe Ep64/100: train=79.8%\n",
            "  Probe Ep65/100: train=79.8%\n",
            "  Probe Ep66/100: train=79.7%\n",
            "  Probe Ep67/100: train=79.9%\n",
            "  Probe Ep68/100: train=79.9%\n",
            "  Probe Ep69/100: train=79.7%\n",
            "  Probe Ep70/100: train=79.9%\n",
            "  Probe Ep71/100: train=79.8%\n",
            "  Probe Ep72/100: train=79.9%\n",
            "  Probe Ep73/100: train=79.9%\n",
            "  Probe Ep74/100: train=79.5%\n",
            "  Probe Ep75/100: train=79.9%\n",
            "  Probe Ep76/100: train=80.0%\n",
            "  Probe Ep77/100: train=79.9%\n",
            "  Probe Ep78/100: train=79.8%\n",
            "  Probe Ep79/100: train=79.8%\n",
            "  Probe Ep80/100: train=79.8%\n",
            "  Probe Ep81/100: train=80.4%\n",
            "  Probe Ep82/100: train=79.8%\n",
            "  Probe Ep83/100: train=80.0%\n",
            "  Probe Ep84/100: train=80.5%\n",
            "  Probe Ep85/100: train=80.3%\n",
            "  Probe Ep86/100: train=80.2%\n",
            "  Probe Ep87/100: train=80.2%\n",
            "  Probe Ep88/100: train=79.8%\n",
            "  Probe Ep89/100: train=80.4%\n",
            "  Probe Ep90/100: train=80.5%\n",
            "  Probe Ep91/100: train=80.6%\n",
            "  Probe Ep92/100: train=80.2%\n",
            "  Probe Ep93/100: train=80.7%\n",
            "  Probe Ep94/100: train=80.8%\n",
            "  Probe Ep95/100: train=80.4%\n",
            "  Probe Ep96/100: train=80.7%\n",
            "  Probe Ep97/100: train=80.2%\n",
            "  Probe Ep98/100: train=80.1%\n",
            "  Probe Ep99/100: train=80.8%\n",
            "  Probe Ep100/100: train=80.4%\n",
            "Probe test acc: 81.9%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250604_212545-ztmp3rno</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-linear-probe/runs/ztmp3rno' target=\"_blank\">BS512_LR1e-04_probe</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-linear-probe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-linear-probe' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-linear-probe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-linear-probe/runs/ztmp3rno' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-linear-probe/runs/ztmp3rno</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "    base_tf = transforms.ToTensor()\n",
        "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "\n",
        "    n = len(ds_all)\n",
        "    n_train = int(TRAIN_FRAC * n)\n",
        "    n_val = int(VAL_FRAC * n)\n",
        "    n_test = n - n_train - n_val\n",
        "    train_ds, val_ds, test_ds = random_split(ds_all, [n_train, n_val, n_test])\n",
        "\n",
        "    print(\"Computing mean and std from training set...\")\n",
        "    mean, std = compute_mean_std(train_ds, batch_size)\n",
        "    print(f\"Computed mean: {mean}\")\n",
        "    print(f\"Computed std:  {std}\")\n",
        "\n",
        "    tf_final = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "    # Redefine datasets with transform\n",
        "    ds_all = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
        "    train_ds, val_ds, test_ds = random_split(ds_all, [n_train, n_val, n_test])\n",
        "\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size, shuffle=True),\n",
        "        DataLoader(val_ds, batch_size, shuffle=False),\n",
        "        DataLoader(test_ds, batch_size, shuffle=False),\n",
        "        len(ds_all.classes)\n",
        "    )\n",
        "\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet50(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "    model.train()\n",
        "    tot_loss, corr, tot = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "\n",
        "        loss   = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if sched: sched.step()\n",
        "        tot_loss += loss.item()\n",
        "        preds    = logits.argmax(dim=1)\n",
        "        corr    += (preds==yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "    return tot_loss/len(loader), 100*corr/tot\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    corr, tot = 0,0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            corr += (preds==yb).sum().item()\n",
        "            tot  += yb.size(0)\n",
        "    return 100 * corr / tot\n",
        "\n",
        "def hyperparam_search(pretrained=True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "\n",
        "    for bs, lr in product(BATCH_SIZES, LRS):\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}\")\n",
        "        set_seed(SEED)\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "        model = build_model(n_cls, pretrained=pretrained)\n",
        "\n",
        "        opt = optim.Adam(model.parameters(), lr=lr)\n",
        "        sched = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.9)\n",
        "\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Start a W&B run here\n",
        "        wandb_run = wandb.init(\n",
        "            project=\"eurosat-supervised-scratch-grid-search\",\n",
        "            name=f\"BS{bs}_LR{lr:.0e}\",\n",
        "            config={\n",
        "                \"batch_size\": bs,\n",
        "                \"learning_rate\": lr,\n",
        "                \"epochs\": NUM_EPOCHS,\n",
        "                \"pretrained\": pretrained,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        for ep in range(NUM_EPOCHS):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "            val_acc = evaluate(model, val_dl)\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train={tr_acc:.1f}%  val={val_acc:.1f}%\")\n",
        "\n",
        "            # Log metrics to W&B\n",
        "            wandb.log({\n",
        "                \"epoch\": ep + 1,\n",
        "                \"train_loss\": tr_loss,\n",
        "                \"train_acc\": tr_acc,\n",
        "                \"val_acc\": val_acc\n",
        "            })\n",
        "\n",
        "        wandb_run.finish()\n",
        "\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_cfg = (bs, lr)\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, val={best_val:.1f}%\")\n",
        "    return best_cfg, best_model\n",
        "\n",
        "def linear_probe(frozen_model, train_dl, test_dl, lr):\n",
        "    for p in frozen_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # new head\n",
        "    n_in = frozen_model.fc.in_features\n",
        "    n_out = frozen_model.fc.out_features\n",
        "    frozen_model.fc = nn.Linear(n_in, n_out).to(DEVICE)\n",
        "\n",
        "    opt = optim.Adam(frozen_model.fc.parameters(), lr=lr)\n",
        "    sched = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.9)\n",
        "\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n>>> Running linear probe on frozen backbone\")\n",
        "    for ep in range(NUM_EPOCHS):\n",
        "        loss, acc = train_one_epoch(frozen_model, train_dl, opt, crit, sched=None)\n",
        "        print(f\"  Probe Ep{ep+1}/{NUM_EPOCHS}: train={acc:.1f}%\")\n",
        "    test_acc = evaluate(frozen_model, test_dl)\n",
        "    print(f\"Probe test acc: {test_acc:.1f}%\")\n",
        "    wandb.init(\n",
        "        project=\"eurosat-supervised-scratch-linear-probe\",\n",
        "        name=f\"BS{train_dl.batch_size}_LR{lr:.0e}_probe\",\n",
        "        config={\n",
        "            \"batch_size\": train_dl.batch_size,\n",
        "            \"learning_rate\": lr,\n",
        "            \"epochs\": NUM_EPOCHS,\n",
        "            \"pretrained\": False,\n",
        "            \"probe\": True\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\"probe_test_acc\": test_acc})\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "# ─── MAIN ───────────────────────────────────────────────────────────────────────\n",
        "best_cfg, best_model = hyperparam_search(pretrained = False)\n",
        "# rebuild loaders once more so we have the same splits\n",
        "bs, lr = best_cfg\n",
        "tr_dl, val_dl, te_dl, _ = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "# Option A: probe on just the original training split\n",
        "probe_acc = linear_probe(best_model, tr_dl, te_dl, lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((512, 0.0001),\n",
              " ResNet(\n",
              "   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   (relu): ReLU(inplace=True)\n",
              "   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "   (layer1): Sequential(\n",
              "     (0): Bottleneck(\n",
              "       (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "       (downsample): Sequential(\n",
              "         (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "         (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): Bottleneck(\n",
              "       (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (2): Bottleneck(\n",
              "       (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "   )\n",
              "   (layer2): Sequential(\n",
              "     (0): Bottleneck(\n",
              "       (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "       (downsample): Sequential(\n",
              "         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): Bottleneck(\n",
              "       (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (2): Bottleneck(\n",
              "       (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (3): Bottleneck(\n",
              "       (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "   )\n",
              "   (layer3): Sequential(\n",
              "     (0): Bottleneck(\n",
              "       (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "       (downsample): Sequential(\n",
              "         (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): Bottleneck(\n",
              "       (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (2): Bottleneck(\n",
              "       (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (3): Bottleneck(\n",
              "       (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (4): Bottleneck(\n",
              "       (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (5): Bottleneck(\n",
              "       (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "   )\n",
              "   (layer4): Sequential(\n",
              "     (0): Bottleneck(\n",
              "       (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "       (downsample): Sequential(\n",
              "         (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): Bottleneck(\n",
              "       (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "     (2): Bottleneck(\n",
              "       (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (relu): ReLU(inplace=True)\n",
              "     )\n",
              "   )\n",
              "   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "   (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              " ))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_cfg, best_model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Artifact best_model>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.save(best_model.state_dict(), \"best_model.pt\")\n",
        "artifact = wandb.Artifact(\"best_model\", type=\"model\")\n",
        "artifact.add_file(\"best_model.pt\")\n",
        "wandb.log_artifact(artifact)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
