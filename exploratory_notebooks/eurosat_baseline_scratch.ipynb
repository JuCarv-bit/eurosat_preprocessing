{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5650b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()  # Opens a browser once to authenticate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet50\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import os, ssl, zipfile, urllib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d444694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set to use GPU: 1 (Quadro RTX 5000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TARGET_GPU_INDEX = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if TARGET_GPU_INDEX < torch.cuda.device_count():\n",
    "        DEVICE = torch.device(f\"cuda:{TARGET_GPU_INDEX}\")\n",
    "        print(f\"Successfully set to use GPU: {TARGET_GPU_INDEX} ({torch.cuda.get_device_name(TARGET_GPU_INDEX)})\")\n",
    "    else:\n",
    "        print(f\"Error: Physical GPU {TARGET_GPU_INDEX} is not available. There are only {torch.cuda.device_count()} GPUs (0 to {torch.cuda.device_count() - 1}).\")\n",
    "        print(\"Falling back to CPU.\")\n",
    "        DEVICE = torch.device(\"CPU\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Falling back to CPU.\")\n",
    "    DEVICE = torch.device(\"CPU\")\n",
    "\n",
    "print(f\"Final DEVICE variable is set to: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"Current PyTorch default device: {torch.cuda.current_device()}\")\n",
    "    torch.cuda.set_device(TARGET_GPU_INDEX)\n",
    "    print(f\"Current PyTorch default device (after set_device): {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "dummy_tensor = torch.randn(2, 2)\n",
    "dummy_tensor_on_gpu = dummy_tensor.to(DEVICE)\n",
    "print(f\"Dummy tensor is on device: {dummy_tensor_on_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d1b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOCAL_OR_COLAB = \"LOCAL\"\n",
    "SEED           = 42\n",
    "NUM_EPOCHS     = 34\n",
    "\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "\n",
    "# hyperparameter grid\n",
    "# BATCH_SIZES = [64, 128, 256]\n",
    "BATCH_SIZES = [32]  # Using a single batch size for simplicity\n",
    "LRS = [1e-4, 3e-4]\n",
    "\n",
    "GRID        = [\n",
    "    (3.75e-4, 0.5  ),\n",
    "]\n",
    "\n",
    "WEIGHT_DECAY = 0.5\n",
    "\n",
    "BETAS=(0.9,0.98)\n",
    "EPS = 1e-8\n",
    "\n",
    "if LOCAL_OR_COLAB == \"LOCAL\":\n",
    "    DATA_DIR = \"/users/c/carvalhj/datasets/EuroSAT_RGB/\"\n",
    "else:\n",
    "    data_root = \"/content/EuroSAT_RGB\"\n",
    "    zip_path  = \"/content/EuroSAT.zip\"\n",
    "    if not os.path.exists(data_root):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
    "        )\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(\"/content\")\n",
    "        os.rename(\"/content/2750\", data_root)\n",
    "    DATA_DIR = data_root\n",
    "\n",
    "NUM_WORKERS = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30bbde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_std(dataset, batch_size):\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        n_samples += batch_samples\n",
    "\n",
    "    mean /= n_samples\n",
    "    std /= n_samples\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "def get_data_loaders(data_dir, batch_size):\n",
    "\n",
    "    base_tf = transforms.ToTensor()\n",
    "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
    "    labels = np.array(ds_all.targets)   # numpy array of shape (N,)\n",
    "    num_classes = len(ds_all.classes)\n",
    "    total_count = len(ds_all)\n",
    "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
    "\n",
    "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
    "\n",
    "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
    "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
    "    print(f\"Computed mean: {mean}\")\n",
    "    print(f\"Computed std:  {std}\")\n",
    "\n",
    "    tf_final = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    #  full ImageFolder but now with normalization baked in\n",
    "    ds_all_norm = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
    "\n",
    "    train_ds = Subset(ds_all_norm, train_idx)\n",
    "    val_ds   = Subset(ds_all_norm, val_idx)\n",
    "    test_ds  = Subset(ds_all_norm, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "    print(f\"Train/Val/Test splits: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n",
    "\n",
    "def get_proportion(num_classes, dataset):\n",
    "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
    "\n",
    "def get_split_indexes(labels, total_count):\n",
    "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
    "    n_temp = total_count - n_train   # this is val + test\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=n_train,\n",
    "        test_size=n_temp,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    # Train and temp(val+test) indices\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
    "\n",
    "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
    "    n_test = total_count - n_train - n_val\n",
    "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
    "\n",
    "    labels_temp = labels[temp_idx]\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=n_val,\n",
    "        test_size=n_test,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
    "\n",
    "    val_idx = temp_idx[val_idx_in_temp]\n",
    "    test_idx = temp_idx[test_idx_in_temp]\n",
    "\n",
    "    assert len(train_idx) == n_train\n",
    "    assert len(val_idx) == n_val\n",
    "    assert len(test_idx) == n_test\n",
    "\n",
    "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
    "    return train_idx,val_idx,test_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9c5e5",
   "metadata": {},
   "source": [
    "# Logistic regresssion with Scikit-learn for comparing linear probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0804bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = BATCH_SIZES[0]\n",
    "LEARNING_RATE, WEIGHT_DECAY = GRID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e80da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: Wed Jun 11 19:40:17 2025\n",
      "\n",
      "--- Setting up DataLoaders (Train / Val / Test) ---\n",
      "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Mean/Std: 100%|██████████| 675/675 [00:03<00:00, 190.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean: [0.3441457152366638, 0.38009852170944214, 0.40766340494155884]\n",
      "Computed std:  [0.09299741685390472, 0.06464488059282303, 0.054139144718647]\n",
      "Train/Val/Test splits: 21600/2700/2700\n",
      "\n",
      "--- Setting up Common Feature Extractor (Randomly Initialized & Frozen) ---\n",
      "Common ResNet50 feature extractor (randomly initialized and frozen) created.\n",
      "\n",
      "--- Running Logistic Regression (sklearn) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 675/675 [00:34<00:00, 19.59it/s]\n",
      "Extracting features: 100%|██████████| 85/85 [00:04<00:00, 18.71it/s]\n",
      "Extracting features: 100%|██████████| 85/85 [00:04<00:00, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (21600, 2048), labels shape: (21600,)\n",
      "Validation features shape: (2700, 2048), labels shape: (2700,)\n",
      "Test features shape: (2700, 2048), labels shape: (2700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found for Logistic Regression: {'C': 0.1, 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "Best cross-validation accuracy for Logistic Regression: 0.7134\n",
      "Logistic Regression Validation Accuracy: 0.7078\n",
      "Logistic Regression Test Accuracy: 0.7122\n",
      "\n",
      "--- Running PyTorch Linear Probing ---\n",
      "Starting PyTorch Linear Probing Training for 10 epochs on combined Train+Val set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train): 100%|██████████| 760/760 [00:41<00:00, 18.11it/s, acc=0.3582, loss=2.2528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.8052, Train Acc: 0.3582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.97it/s, acc=0.4563, loss=1.4161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 1.5845, Train Acc: 0.4563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.96it/s, acc=0.4863, loss=1.4693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 1.4871, Train Acc: 0.4863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.90it/s, acc=0.5041, loss=1.9950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 1.4269, Train Acc: 0.5041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.91it/s, acc=0.5183, loss=1.3394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 1.3808, Train Acc: 0.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.96it/s, acc=0.5310, loss=1.1902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 1.3449, Train Acc: 0.5310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.96it/s, acc=0.5366, loss=1.1155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 1.3283, Train Acc: 0.5366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.91it/s, acc=0.5453, loss=0.8521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 1.3045, Train Acc: 0.5453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.94it/s, acc=0.5536, loss=1.3915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 1.2826, Train Acc: 0.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 760/760 [00:42<00:00, 17.95it/s, acc=0.5581, loss=1.4818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 1.2695, Train Acc: 0.5581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set (PyTorch): 100%|██████████| 85/85 [00:04<00:00, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Set Performance (PyTorch Linear Probing) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.44      0.80      0.57       300\n",
      "     class_1       0.54      0.91      0.67       300\n",
      "     class_2       0.66      0.40      0.50       300\n",
      "     class_3       0.25      0.02      0.04       250\n",
      "     class_4       0.84      0.80      0.82       250\n",
      "     class_5       0.64      0.73      0.69       200\n",
      "     class_6       0.55      0.35      0.43       250\n",
      "     class_7       0.56      0.69      0.61       300\n",
      "     class_8       0.47      0.29      0.36       250\n",
      "     class_9       0.75      0.72      0.73       300\n",
      "\n",
      "    accuracy                           0.58      2700\n",
      "   macro avg       0.57      0.57      0.54      2700\n",
      "weighted avg       0.57      0.58      0.55      2700\n",
      "\n",
      "Test Accuracy: 0.5811\n",
      "Confusion Matrix:\n",
      " [[240   8   4   1   0   8  19   9   2   9]\n",
      " [  0 272   2   0   0  18   0   0   2   6]\n",
      " [ 72  36 121   3   5   6  18  20   9  10]\n",
      " [ 73  51   8   6   6  18  17  39  28   4]\n",
      " [  0   0   0   0 201   0   1  47   1   0]\n",
      " [  8  23  10   0   0 147   0   2   2   8]\n",
      " [ 87   8  27   2   2  10  88  16   6   4]\n",
      " [ 25   3   4   6  21   4  10 206  21   0]\n",
      " [ 33  54   4   6   5  10   6  27  73  32]\n",
      " [  9  51   2   0   0   8   0   5  10 215]]\n",
      "\n",
      "Total script runtime: 619.63 seconds.\n",
      "\n",
      "--- Final Comparison (Randomly Initialized & Frozen ResNet50 Features) ---\n",
      "Logistic Regression Test Accuracy: 0.7122\n",
      "PyTorch Linear Probing Test Accuracy: 0.5811\n",
      "Expected: Both accuracies should be low (e.g., 10-20%) as they are based on randomly initialized, uninformative features. PyTorch might be slightly higher due to Adam optimizer and more epochs, but fundamentally limited by the features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_common_feature_extractor_model():\n",
    "    \"\"\"\n",
    "    Initializes a ResNet50 backbone with random weights, freezes it,\n",
    "    and prepares it for feature extraction.\n",
    "    \"\"\"\n",
    "    base_model = models.resnet50(weights=None) # No pre-trained ImageNet weights\n",
    "    feature_extractor = nn.Sequential(*list(base_model.children())[:-1]) # Exclude the last fc layer\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "    feature_extractor.eval() \n",
    "    feature_extractor.to(DEVICE)\n",
    "    print(\"Common ResNet50 feature extractor (randomly initialized and frozen) created.\")\n",
    "    return feature_extractor\n",
    "\n",
    "def extract_features(dataloader, model):\n",
    "    \"\"\"Extracts features and labels from a DataLoader using the provided model.\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(DEVICE)\n",
    "            features = model(images)\n",
    "            features = features.squeeze(-1).squeeze(-1) # Flatten (batch_size, 2048, 1, 1) to (batch_size, 2048)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    return np.vstack(all_features), np.concatenate(all_labels)\n",
    "\n",
    "class PyTorchLinearProbingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch model that uses a shared, frozen feature extractor\n",
    "    and a trainable linear classification head.\n",
    "    \"\"\"\n",
    "    def __init__(self, shared_feature_extractor, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = shared_feature_extractor # Use the externally created and frozen feature extractor\n",
    "        self.backbone.eval()\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        feature_dim = 2048 \n",
    "        self.linear_head = nn.Linear(feature_dim, num_classes)\n",
    "        self.linear_head.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad(): # Ensure no gradients for the frozen backbone\n",
    "            features = self.backbone(x)\n",
    "            features = features.squeeze(-1).squeeze(-1)\n",
    "        logits = self.linear_head(features)\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} (Train)\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct_predictions/total_samples:.4f}\")\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def evaluate_test_set_pytorch(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating Test Set (PyTorch)\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(\"\\n--- Test Set Performance (PyTorch Linear Probing) ---\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[f'class_{i}' for i in range(num_classes)]))\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
    "    return test_accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    train_loader, val_loader, test_loader, num_classes = get_data_loaders(DATA_DIR, BATCH_SIZE)\n",
    "\n",
    "    common_feature_extractor = get_common_feature_extractor_model()\n",
    "\n",
    "    print(\"\\n--- Running Logistic Regression (sklearn) ---\")\n",
    "\n",
    "    X_train_features, y_train_labels = extract_features(train_loader, common_feature_extractor)\n",
    "    X_val_features, y_val_labels = extract_features(val_loader, common_feature_extractor)\n",
    "    X_test_features, y_test_labels = extract_features(test_loader, common_feature_extractor)\n",
    "\n",
    "    print(f\"Train features shape: {X_train_features.shape}, labels shape: {y_train_labels.shape}\")\n",
    "    print(f\"Validation features shape: {X_val_features.shape}, labels shape: {y_val_labels.shape}\")\n",
    "    print(f\"Test features shape: {X_test_features.shape}, labels shape: {y_test_labels.shape}\")\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0],\n",
    "        'solver': ['lbfgs'],\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "\n",
    "    logistic_classifier = LogisticRegression(random_state=SEED)\n",
    "    grid_search = GridSearchCV(\n",
    "        logistic_classifier,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0 # Suppress verbose output for cleaner run\n",
    "    )\n",
    "    grid_search.fit(X_train_features, y_train_labels)\n",
    "\n",
    "    print(f\"\\nBest parameters found for Logistic Regression: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for Logistic Regression: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    best_logistic_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred_val_lr = best_logistic_model.predict(X_val_features)\n",
    "    val_accuracy_lr = accuracy_score(y_val_labels, y_pred_val_lr)\n",
    "    print(f\"Logistic Regression Validation Accuracy: {val_accuracy_lr:.4f}\")\n",
    "\n",
    "    y_pred_test_lr = best_logistic_model.predict(X_test_features)\n",
    "    test_accuracy_lr = accuracy_score(y_test_labels, y_pred_test_lr)\n",
    "    print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Running PyTorch Linear Probing ---\")\n",
    "\n",
    "    pytorch_model = PyTorchLinearProbingModel(common_feature_extractor, num_classes=num_classes)\n",
    "    optimizer = optim.Adam(pytorch_model.linear_head.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    combined_train_val_dataset = ConcatDataset([train_loader.dataset, val_loader.dataset])\n",
    "    combined_train_val_loader = DataLoader(combined_train_val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=NUM_WORKERS,\n",
    "                                           generator=torch.Generator().manual_seed(SEED),\n",
    "                                           pin_memory=True)\n",
    "    \n",
    "    print(f\"Starting PyTorch Linear Probing Training for {NUM_EPOCHS} epochs on combined Train+Val set.\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(pytorch_model, combined_train_val_loader, criterion, optimizer, epoch, NUM_EPOCHS)\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    test_accuracy_pytorch = evaluate_test_set_pytorch(pytorch_model, test_loader, num_classes)\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\nTotal script runtime: {end_time_total - start_time_total:.2f} seconds.\")\n",
    "\n",
    "    # --- Final Comparison ---\n",
    "    print(\"\\n--- Final Comparison (Randomly Initialized & Frozen ResNet50 Features) ---\")\n",
    "    print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "    print(f\"PyTorch Linear Probing Test Accuracy: {test_accuracy_pytorch:.4f}\")\n",
    "    print(\"Expected: Both accuracies should be low (e.g., 10-20%) as they are based on randomly initialized, uninformative features. \" \\\n",
    "    \"PyTorch might be slightly higher due to Adam optimizer and more epochs, but fundamentally limited by the features.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
