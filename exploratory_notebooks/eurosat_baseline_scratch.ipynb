{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5650b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()  # Opens a browser once to authenticate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet50\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import os, ssl, zipfile, urllib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d444694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set to use GPU: 1 (Quadro RTX 5000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TARGET_GPU_INDEX = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if TARGET_GPU_INDEX < torch.cuda.device_count():\n",
    "        DEVICE = torch.device(f\"cuda:{TARGET_GPU_INDEX}\")\n",
    "        print(f\"Successfully set to use GPU: {TARGET_GPU_INDEX} ({torch.cuda.get_device_name(TARGET_GPU_INDEX)})\")\n",
    "    else:\n",
    "        print(f\"Error: Physical GPU {TARGET_GPU_INDEX} is not available. There are only {torch.cuda.device_count()} GPUs (0 to {torch.cuda.device_count() - 1}).\")\n",
    "        print(\"Falling back to CPU.\")\n",
    "        DEVICE = torch.device(\"CPU\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Falling back to CPU.\")\n",
    "    DEVICE = torch.device(\"CPU\")\n",
    "\n",
    "print(f\"Final DEVICE variable is set to: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"Current PyTorch default device: {torch.cuda.current_device()}\")\n",
    "    torch.cuda.set_device(TARGET_GPU_INDEX)\n",
    "    print(f\"Current PyTorch default device (after set_device): {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "dummy_tensor = torch.randn(2, 2)\n",
    "dummy_tensor_on_gpu = dummy_tensor.to(DEVICE)\n",
    "print(f\"Dummy tensor is on device: {dummy_tensor_on_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOCAL_OR_COLAB = \"LOCAL\"\n",
    "SEED           = 42\n",
    "NUM_EPOCHS     = 34\n",
    "\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "\n",
    "# hyperparameter grid\n",
    "# BATCH_SIZES = [64, 128, 256]\n",
    "BATCH_SIZES = [32]  # Using a single batch size for simplicity\n",
    "LRS = [1e-4, 3e-4]\n",
    "\n",
    "GRID        = [\n",
    "    (3.75e-4, 0.5  ),\n",
    "]\n",
    "\n",
    "WEIGHT_DECAY = 0.5\n",
    "\n",
    "BETAS=(0.9,0.98)\n",
    "EPS = 1e-8\n",
    "\n",
    "if LOCAL_OR_COLAB == \"LOCAL\":\n",
    "    DATA_DIR = \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB/\"\n",
    "else:\n",
    "    data_root = \"/content/EuroSAT_RGB\"\n",
    "    zip_path  = \"/content/EuroSAT.zip\"\n",
    "    if not os.path.exists(data_root):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
    "        )\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(\"/content\")\n",
    "        os.rename(\"/content/2750\", data_root)\n",
    "    DATA_DIR = data_root\n",
    "\n",
    "NUM_WORKERS = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bbde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_std(dataset, batch_size):\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        n_samples += batch_samples\n",
    "\n",
    "    mean /= n_samples\n",
    "    std /= n_samples\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "def get_data_loaders(data_dir, batch_size):\n",
    "\n",
    "    base_tf = transforms.ToTensor()\n",
    "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
    "    labels = np.array(ds_all.targets)   # numpy array of shape (N,)\n",
    "    num_classes = len(ds_all.classes)\n",
    "    total_count = len(ds_all)\n",
    "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
    "\n",
    "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
    "\n",
    "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
    "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
    "    print(f\"Computed mean: {mean}\")\n",
    "    print(f\"Computed std:  {std}\")\n",
    "\n",
    "    tf_final = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    #  full ImageFolder but now with normalization baked in\n",
    "    ds_all_norm = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
    "\n",
    "    train_ds = Subset(ds_all_norm, train_idx)\n",
    "    val_ds   = Subset(ds_all_norm, val_idx)\n",
    "    test_ds  = Subset(ds_all_norm, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "    print(f\"Train/Val/Test splits: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n",
    "\n",
    "def get_proportion(num_classes, dataset):\n",
    "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
    "\n",
    "def get_split_indexes(labels, total_count):\n",
    "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
    "    n_temp = total_count - n_train   # this is val + test\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=n_train,\n",
    "        test_size=n_temp,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    # Train and temp(val+test) indices\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
    "\n",
    "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
    "    n_test = total_count - n_train - n_val\n",
    "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
    "\n",
    "    labels_temp = labels[temp_idx]\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=n_val,\n",
    "        test_size=n_test,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
    "\n",
    "    val_idx = temp_idx[val_idx_in_temp]\n",
    "    test_idx = temp_idx[test_idx_in_temp]\n",
    "\n",
    "    assert len(train_idx) == n_train\n",
    "    assert len(val_idx) == n_val\n",
    "    assert len(test_idx) == n_test\n",
    "\n",
    "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
    "    return train_idx,val_idx,test_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9c5e5",
   "metadata": {},
   "source": [
    "# Logistic regresssion with Scikit-learn for comparing linear probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0804bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = BATCH_SIZES[0]\n",
    "LEARNING_RATE, WEIGHT_DECAY = GRID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e80da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_feature_extractor_model():\n",
    "\n",
    "    base_model = models.resnet50(weights=None) # No pre-trained ImageNet weights\n",
    "    feature_extractor = nn.Sequential(*list(base_model.children())[:-1]) # Exclude the last fc layer\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "    feature_extractor.eval() \n",
    "    feature_extractor.to(DEVICE)\n",
    "    print(\"Common ResNet50 feature extractor (randomly initialized and frozen) created.\")\n",
    "    return feature_extractor\n",
    "\n",
    "def extract_features(dataloader, model):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(DEVICE)\n",
    "            features = model(images)\n",
    "            features = features.squeeze(-1).squeeze(-1) # Flatten (batch_size, 2048, 1, 1) to (batch_size, 2048)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    return np.vstack(all_features), np.concatenate(all_labels)\n",
    "\n",
    "class PyTorchLinearProbingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, shared_feature_extractor, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = shared_feature_extractor\n",
    "        self.backbone.eval()\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        feature_dim = 2048 \n",
    "        self.linear_head = nn.Linear(feature_dim, num_classes)\n",
    "        self.linear_head.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "            features = features.squeeze(-1).squeeze(-1)\n",
    "        logits = self.linear_head(features)\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} (Train)\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct_predictions/total_samples:.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.6f}\") # Added LR to postfix\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def evaluate_test_set_pytorch(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating Test Set (PyTorch)\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(classification_report(all_labels, all_preds, target_names=[f'class_{i}' for i in range(num_classes)]))\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
    "    return test_accuracy\n",
    "\n",
    "def make_optimizer_scheduler(params, lr, wd, steps_per_epoch, epochs):\n",
    "    total_steps  = epochs * steps_per_epoch\n",
    "    warmup_steps = steps_per_epoch\n",
    "    opt = optim.Adam(params, lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
    "    sched = SequentialLR(\n",
    "        opt,\n",
    "        schedulers=[\n",
    "            LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
    "            CosineAnnealingLR(opt, T_max=total_steps - warmup_steps)\n",
    "        ],\n",
    "        milestones=[warmup_steps]\n",
    "    )\n",
    "    return opt, sched\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def hyperparameter_search_pytorch(train_loader, val_loader, num_classes, common_feature_extractor):\n",
    "    best_val_accuracy = -1.0\n",
    "    best_params = {}\n",
    "\n",
    "    print(\"\\n--- Starting PyTorch Hyperparameter Search ---\")\n",
    "    \n",
    "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
    "        epochs = NUM_EPOCHS\n",
    "\n",
    "        print(f\"\\n--- Trying config: LR={lr}, WD={wd}, Epochs={epochs} ---\")\n",
    "\n",
    "        model = PyTorchLinearProbingModel(common_feature_extractor, num_classes=num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        steps_per_epoch = len(train_loader) \n",
    "        optimizer, scheduler = make_optimizer_scheduler(\n",
    "            model.linear_head.parameters(),\n",
    "            lr,\n",
    "            wd,\n",
    "            steps_per_epoch,\n",
    "            epochs\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, epoch, epochs)\n",
    "            val_loss, val_acc = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_params = {\n",
    "                'batch_size': bs,\n",
    "                'learning_rate': lr,\n",
    "                'weight_decay': wd,\n",
    "                'epochs': epochs\n",
    "            }\n",
    "            print(f\"New best validation accuracy: {best_val_accuracy:.4f} with params: {best_params}\")\n",
    "\n",
    "    print(\"\\n--- PyTorch Hyperparameter Search Complete ---\")\n",
    "    print(f\"Best validation accuracy found: {best_val_accuracy:.4f}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    return best_params, best_val_accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3186bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Computed mean: [0.3441457152366638, 0.38009852170944214, 0.40766340494155884]\n",
      "Computed std:  [0.09299741685390472, 0.06464488059282303, 0.054139144718647]\n",
      "Train/Val/Test splits: 21600/2700/2700\n",
      "Common ResNet50 feature extractor (randomly initialized and frozen) created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 675/675 [00:06<00:00, 99.01it/s] \n",
      "Extracting features: 100%|██████████| 85/85 [00:01<00:00, 83.38it/s] \n",
      "Extracting features: 100%|██████████| 85/85 [00:00<00:00, 86.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (21600, 2048), labels shape: (21600,)\n",
      "Validation features shape: (2700, 2048), labels shape: (2700,)\n",
      "Test features shape: (2700, 2048), labels shape: (2700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found for Logistic Regression: {'C': 0.1, 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "Best cross-validation accuracy for Logistic Regression: 0.6353\n",
      "Logistic Regression Validation Accuracy: 0.6581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy: 0.6363\n",
      "\n",
      "--- Running PyTorch Linear Probing ---\n",
      "\n",
      "--- Starting PyTorch Hyperparameter Search ---\n",
      "\n",
      "--- Trying config: LR=0.000375, WD=0.5, Epochs=34 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/34 (Train): 100%|█████████▉| 672/675 [00:08<00:00, 87.47it/s, acc=0.2662, loss=1.5945, lr=0.000374]/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 81.32it/s, acc=0.2662, loss=2.3996, lr=0.000375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/34 - Train Loss: 2.0009, Train Acc: 0.2662 | Val Loss: 1.8190, Val Acc: 0.3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 88.88it/s, acc=0.3685, loss=1.5108, lr=0.000374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/34 - Train Loss: 1.7605, Train Acc: 0.3685 | Val Loss: 1.6986, Val Acc: 0.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 83.36it/s, acc=0.4036, loss=1.7769, lr=0.000372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/34 - Train Loss: 1.6771, Train Acc: 0.4036 | Val Loss: 1.6071, Val Acc: 0.4178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 89.18it/s, acc=0.4168, loss=1.5810, lr=0.000367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/34 - Train Loss: 1.6434, Train Acc: 0.4168 | Val Loss: 1.5914, Val Acc: 0.4293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 82.71it/s, acc=0.4321, loss=1.7036, lr=0.000362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/34 - Train Loss: 1.6046, Train Acc: 0.4321 | Val Loss: 1.6117, Val Acc: 0.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 84.20it/s, acc=0.4327, loss=1.4639, lr=0.000354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/34 - Train Loss: 1.5947, Train Acc: 0.4327 | Val Loss: 1.5241, Val Acc: 0.4719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 83.07it/s, acc=0.4448, loss=1.4150, lr=0.000345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/34 - Train Loss: 1.5702, Train Acc: 0.4448 | Val Loss: 1.5227, Val Acc: 0.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.51it/s, acc=0.4465, loss=1.7296, lr=0.000335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/34 - Train Loss: 1.5593, Train Acc: 0.4465 | Val Loss: 1.4926, Val Acc: 0.4807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 85.43it/s, acc=0.4520, loss=1.6485, lr=0.000323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/34 - Train Loss: 1.5560, Train Acc: 0.4520 | Val Loss: 1.5345, Val Acc: 0.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 87.63it/s, acc=0.4526, loss=1.7020, lr=0.000310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/34 - Train Loss: 1.5489, Train Acc: 0.4526 | Val Loss: 1.5033, Val Acc: 0.4689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 81.32it/s, acc=0.4553, loss=1.2320, lr=0.000296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/34 - Train Loss: 1.5462, Train Acc: 0.4553 | Val Loss: 1.5473, Val Acc: 0.4707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 83.99it/s, acc=0.4569, loss=1.2536, lr=0.000281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/34 - Train Loss: 1.5378, Train Acc: 0.4569 | Val Loss: 1.5191, Val Acc: 0.4674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.83it/s, acc=0.4634, loss=1.8601, lr=0.000265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/34 - Train Loss: 1.5237, Train Acc: 0.4634 | Val Loss: 1.4865, Val Acc: 0.4744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 85.77it/s, acc=0.4646, loss=1.5183, lr=0.000249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/34 - Train Loss: 1.5228, Train Acc: 0.4646 | Val Loss: 1.4871, Val Acc: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.63it/s, acc=0.4689, loss=1.5836, lr=0.000232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/34 - Train Loss: 1.5126, Train Acc: 0.4689 | Val Loss: 1.4771, Val Acc: 0.4919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 88.44it/s, acc=0.4706, loss=1.4220, lr=0.000214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/34 - Train Loss: 1.5084, Train Acc: 0.4706 | Val Loss: 1.4827, Val Acc: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 87.87it/s, acc=0.4693, loss=1.6670, lr=0.000196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/34 - Train Loss: 1.5077, Train Acc: 0.4693 | Val Loss: 1.5191, Val Acc: 0.4667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.93it/s, acc=0.4678, loss=1.6751, lr=0.000179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/34 - Train Loss: 1.5067, Train Acc: 0.4678 | Val Loss: 1.4797, Val Acc: 0.4919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 84.73it/s, acc=0.4778, loss=1.4348, lr=0.000161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/34 - Train Loss: 1.4933, Train Acc: 0.4778 | Val Loss: 1.4652, Val Acc: 0.4833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.45it/s, acc=0.4792, loss=1.4950, lr=0.000143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/34 - Train Loss: 1.4933, Train Acc: 0.4792 | Val Loss: 1.5059, Val Acc: 0.4604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 83.18it/s, acc=0.4782, loss=1.5077, lr=0.000126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/34 - Train Loss: 1.4907, Train Acc: 0.4782 | Val Loss: 1.4577, Val Acc: 0.4863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 85.95it/s, acc=0.4825, loss=1.4439, lr=0.000110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/34 - Train Loss: 1.4847, Train Acc: 0.4825 | Val Loss: 1.4600, Val Acc: 0.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 85.18it/s, acc=0.4799, loss=1.3755, lr=0.000094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/34 - Train Loss: 1.4840, Train Acc: 0.4799 | Val Loss: 1.4691, Val Acc: 0.4837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 83.34it/s, acc=0.4839, loss=1.2406, lr=0.000079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/34 - Train Loss: 1.4781, Train Acc: 0.4839 | Val Loss: 1.4510, Val Acc: 0.4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 85.86it/s, acc=0.4835, loss=1.7900, lr=0.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/34 - Train Loss: 1.4781, Train Acc: 0.4835 | Val Loss: 1.4526, Val Acc: 0.4956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 79.00it/s, acc=0.4876, loss=1.5616, lr=0.000052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/34 - Train Loss: 1.4656, Train Acc: 0.4876 | Val Loss: 1.4677, Val Acc: 0.4837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.29it/s, acc=0.4913, loss=1.3736, lr=0.000040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/34 - Train Loss: 1.4649, Train Acc: 0.4913 | Val Loss: 1.4519, Val Acc: 0.4867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.18it/s, acc=0.4910, loss=1.4605, lr=0.000030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/34 - Train Loss: 1.4661, Train Acc: 0.4910 | Val Loss: 1.4455, Val Acc: 0.4878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 87.79it/s, acc=0.4853, loss=1.3278, lr=0.000021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/34 - Train Loss: 1.4721, Train Acc: 0.4853 | Val Loss: 1.4399, Val Acc: 0.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 82.09it/s, acc=0.4899, loss=1.4870, lr=0.000013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/34 - Train Loss: 1.4615, Train Acc: 0.4899 | Val Loss: 1.4401, Val Acc: 0.4996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 86.39it/s, acc=0.4956, loss=1.2326, lr=0.000008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/34 - Train Loss: 1.4575, Train Acc: 0.4956 | Val Loss: 1.4466, Val Acc: 0.4956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 92.50it/s, acc=0.4888, loss=1.2378, lr=0.000003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/34 - Train Loss: 1.4593, Train Acc: 0.4888 | Val Loss: 1.4525, Val Acc: 0.4819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/34 (Train): 100%|██████████| 675/675 [00:07<00:00, 89.33it/s, acc=0.4945, loss=1.4273, lr=0.000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/34 - Train Loss: 1.4578, Train Acc: 0.4945 | Val Loss: 1.4580, Val Acc: 0.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/34 (Train): 100%|██████████| 675/675 [00:08<00:00, 83.33it/s, acc=0.4985, loss=1.5904, lr=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/34 - Train Loss: 1.4518, Train Acc: 0.4985 | Val Loss: 1.4551, Val Acc: 0.4989\n",
      "New best validation accuracy: 0.4989 with params: {'batch_size': 32, 'learning_rate': 0.000375, 'weight_decay': 0.5, 'epochs': 34}\n",
      "\n",
      "--- PyTorch Hyperparameter Search Complete ---\n",
      "Best validation accuracy found: 0.4989\n",
      "Best parameters: {'batch_size': 32, 'learning_rate': 0.000375, 'weight_decay': 0.5, 'epochs': 34}\n",
      "\n",
      "PyTorch Best Hyperparameters: {'batch_size': 32, 'learning_rate': 0.000375, 'weight_decay': 0.5, 'epochs': 34}\n",
      "PyTorch Best Validation Accuracy: 0.4989\n",
      "\n",
      "--- Training Final PyTorch Linear Probing Model on combined Train+Val set ---\n",
      "Starting PyTorch Linear Probing Training for 34 epochs on combined Train+Val set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 85.13it/s, acc=0.2654, loss=2.4603, lr=0.000375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/34 - Train Loss: 1.9956, Train Acc: 0.2654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 88.85it/s, acc=0.3747, loss=1.5264, lr=0.000374] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/34 - Train Loss: 1.7505, Train Acc: 0.3747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.78it/s, acc=0.4100, loss=1.9082, lr=0.000372] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/34 - Train Loss: 1.6658, Train Acc: 0.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 92.81it/s, acc=0.4240, loss=2.6055, lr=0.000367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/34 - Train Loss: 1.6230, Train Acc: 0.4240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 90.51it/s, acc=0.4337, loss=1.2419, lr=0.000362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/34 - Train Loss: 1.5968, Train Acc: 0.4337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 94.27it/s, acc=0.4388, loss=1.6535, lr=0.000354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/34 - Train Loss: 1.5731, Train Acc: 0.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 98.54it/s, acc=0.4447, loss=1.5941, lr=0.000345] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/34 - Train Loss: 1.5710, Train Acc: 0.4447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.33it/s, acc=0.4493, loss=1.2083, lr=0.000335] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/34 - Train Loss: 1.5584, Train Acc: 0.4493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 90.64it/s, acc=0.4517, loss=1.8303, lr=0.000323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/34 - Train Loss: 1.5476, Train Acc: 0.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 90.80it/s, acc=0.4508, loss=1.7474, lr=0.000310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/34 - Train Loss: 1.5462, Train Acc: 0.4508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 97.45it/s, acc=0.4538, loss=2.0977, lr=0.000296] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/34 - Train Loss: 1.5401, Train Acc: 0.4538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.31it/s, acc=0.4562, loss=1.4392, lr=0.000281] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/34 - Train Loss: 1.5354, Train Acc: 0.4562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.23it/s, acc=0.4582, loss=1.7405, lr=0.000265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/34 - Train Loss: 1.5351, Train Acc: 0.4582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 95.19it/s, acc=0.4627, loss=1.6475, lr=0.000249] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/34 - Train Loss: 1.5296, Train Acc: 0.4627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 93.66it/s, acc=0.4692, loss=1.6834, lr=0.000232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/34 - Train Loss: 1.5129, Train Acc: 0.4692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.92it/s, acc=0.4687, loss=1.3259, lr=0.000214] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/34 - Train Loss: 1.5105, Train Acc: 0.4687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 95.45it/s, acc=0.4647, loss=1.9021, lr=0.000196] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/34 - Train Loss: 1.5175, Train Acc: 0.4647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 97.14it/s, acc=0.4703, loss=1.4090, lr=0.000179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/34 - Train Loss: 1.5012, Train Acc: 0.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 89.51it/s, acc=0.4722, loss=2.3696, lr=0.000161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/34 - Train Loss: 1.4982, Train Acc: 0.4722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 99.61it/s, acc=0.4680, loss=1.7766, lr=0.000143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/34 - Train Loss: 1.5052, Train Acc: 0.4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 94.40it/s, acc=0.4793, loss=3.0633, lr=0.000126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/34 - Train Loss: 1.4889, Train Acc: 0.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 92.18it/s, acc=0.4831, loss=1.6639, lr=0.000110] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/34 - Train Loss: 1.4811, Train Acc: 0.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 90.25it/s, acc=0.4831, loss=1.6819, lr=0.000094] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/34 - Train Loss: 1.4819, Train Acc: 0.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 93.29it/s, acc=0.4849, loss=1.2290, lr=0.000079] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/34 - Train Loss: 1.4819, Train Acc: 0.4849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 95.45it/s, acc=0.4833, loss=1.2006, lr=0.000065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/34 - Train Loss: 1.4726, Train Acc: 0.4833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.42it/s, acc=0.4859, loss=1.3127, lr=0.000052] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/34 - Train Loss: 1.4741, Train Acc: 0.4859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 89.83it/s, acc=0.4877, loss=1.5311, lr=0.000040] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/34 - Train Loss: 1.4746, Train Acc: 0.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 87.78it/s, acc=0.4883, loss=1.4363, lr=0.000030] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/34 - Train Loss: 1.4753, Train Acc: 0.4883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 97.08it/s, acc=0.4884, loss=1.3445, lr=0.000021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/34 - Train Loss: 1.4713, Train Acc: 0.4884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 90.74it/s, acc=0.4920, loss=1.3274, lr=0.000013] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/34 - Train Loss: 1.4602, Train Acc: 0.4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.34it/s, acc=0.4910, loss=1.4085, lr=0.000008] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/34 - Train Loss: 1.4601, Train Acc: 0.4910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 92.13it/s, acc=0.4953, loss=1.2025, lr=0.000003] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/34 - Train Loss: 1.4589, Train Acc: 0.4953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/34 (Train): 100%|██████████| 760/760 [00:07<00:00, 96.24it/s, acc=0.4931, loss=1.4888, lr=0.000001] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/34 - Train Loss: 1.4571, Train Acc: 0.4931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/34 (Train): 100%|██████████| 760/760 [00:08<00:00, 93.35it/s, acc=0.4906, loss=1.4744, lr=0.000000] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/34 - Train Loss: 1.4628, Train Acc: 0.4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set (PyTorch): 100%|██████████| 85/85 [00:01<00:00, 84.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.47      0.42      0.45       300\n",
      "     class_1       0.57      0.87      0.69       300\n",
      "     class_2       0.34      0.56      0.42       300\n",
      "     class_3       0.38      0.06      0.10       250\n",
      "     class_4       0.72      0.82      0.77       250\n",
      "     class_5       0.46      0.51      0.48       200\n",
      "     class_6       0.35      0.11      0.17       250\n",
      "     class_7       0.41      0.47      0.44       300\n",
      "     class_8       0.33      0.14      0.20       250\n",
      "     class_9       0.61      0.84      0.71       300\n",
      "\n",
      "    accuracy                           0.49      2700\n",
      "   macro avg       0.47      0.48      0.44      2700\n",
      "weighted avg       0.47      0.49      0.45      2700\n",
      "\n",
      "Test Accuracy: 0.4937\n",
      "Confusion Matrix:\n",
      " [[127   7  58   3  14  17  15  23  11  25]\n",
      " [  0 260   2   0   0  12   0   0   1  25]\n",
      " [ 17  24 167   1   7  21   6  30   4  23]\n",
      " [ 29  34  53  15  12  13  14  42  26  12]\n",
      " [  3   0   1   4 206   0   2  34   0   0]\n",
      " [  1  47  20   1   0 102   0   0   3  26]\n",
      " [ 56   5  80   4  10   9  28  41  10   7]\n",
      " [ 19   2  68   6  27   7  10 141  17   3]\n",
      " [ 16  48  36   5   9  28   3  30  36  39]\n",
      " [  0  26   5   1   0  13   1   2   1 251]]\n",
      "Logistic Regression Test Accuracy: 0.6363\n",
      "PyTorch Linear Probing Test Accuracy: 0.4937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_loader, val_loader, test_loader, num_classes = get_data_loaders(DATA_DIR, BATCH_SIZE)\n",
    "\n",
    "common_feature_extractor = get_common_feature_extractor_model()\n",
    "\n",
    "X_train_features, y_train_labels = extract_features(train_loader, common_feature_extractor)\n",
    "X_val_features, y_val_labels = extract_features(val_loader, common_feature_extractor)\n",
    "X_test_features, y_test_labels = extract_features(test_loader, common_feature_extractor)\n",
    "\n",
    "X_train_val_features = np.vstack((X_train_features, X_val_features))\n",
    "y_train_val_labels = np.concatenate((y_train_labels, y_val_labels))\n",
    "\n",
    "print(f\"Train features shape: {X_train_features.shape}, labels shape: {y_train_labels.shape}\")\n",
    "print(f\"Validation features shape: {X_val_features.shape}, labels shape: {y_val_labels.shape}\")\n",
    "print(f\"Test features shape: {X_test_features.shape}, labels shape: {y_test_labels.shape}\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "logistic_classifier = LogisticRegression(random_state=SEED)\n",
    "grid_search = GridSearchCV(\n",
    "    logistic_classifier,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0 \n",
    ")\n",
    "grid_search.fit(X_train_features, y_train_labels)\n",
    "\n",
    "print(f\"\\nBest parameters found for Logistic Regression: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy for Logistic Regression: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_logistic_model = grid_search.best_estimator_\n",
    "y_pred_val_lr = best_logistic_model.predict(X_val_features)\n",
    "val_accuracy_lr = accuracy_score(y_val_labels, y_pred_val_lr)\n",
    "print(f\"Logistic Regression Validation Accuracy: {val_accuracy_lr:.4f}\")\n",
    "\n",
    "final_logistic_model = LogisticRegression(**grid_search.best_params_, random_state=SEED)\n",
    "final_logistic_model.fit(X_train_val_features, y_train_val_labels)\n",
    "\n",
    "y_pred_test_lr = final_logistic_model.predict(X_test_features)\n",
    "test_accuracy_lr = accuracy_score(y_test_labels, y_pred_test_lr)\n",
    "print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "\n",
    "print(\"\\n--- Running PyTorch Linear Probing ---\")\n",
    "\n",
    "best_pytorch_params, best_pytorch_val_accuracy = hyperparameter_search_pytorch(\n",
    "        train_loader, val_loader, num_classes, common_feature_extractor\n",
    "    )\n",
    "\n",
    "print(f\"\\nPyTorch Best Hyperparameters: {best_pytorch_params}\")\n",
    "print(f\"PyTorch Best Validation Accuracy: {best_pytorch_val_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Final PyTorch Linear Probing Model on combined Train+Val set ---\")\n",
    "final_pytorch_model = PyTorchLinearProbingModel(common_feature_extractor, num_classes=num_classes)\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "combined_train_val_dataset = ConcatDataset([train_loader.dataset, val_loader.dataset])\n",
    "combined_train_val_loader = DataLoader(combined_train_val_dataset,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=NUM_WORKERS,\n",
    "                                        generator=torch.Generator().manual_seed(SEED),\n",
    "                                        pin_memory=True)\n",
    "\n",
    "steps_per_epoch = len(combined_train_val_loader)\n",
    "optimizer, scheduler = make_optimizer_scheduler(\n",
    "    final_pytorch_model.linear_head.parameters(),\n",
    "    LEARNING_RATE,\n",
    "    WEIGHT_DECAY,\n",
    "    steps_per_epoch,\n",
    "    NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"Starting PyTorch Linear Probing Training for {NUM_EPOCHS} epochs on combined Train+Val set.\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(final_pytorch_model, combined_train_val_loader, criterion_final, optimizer, scheduler, epoch, NUM_EPOCHS)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "test_accuracy_pytorch = evaluate_test_set_pytorch(final_pytorch_model, test_loader, num_classes)\n",
    "\n",
    "\n",
    "print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "print(f\"PyTorch Linear Probing Test Accuracy: {test_accuracy_pytorch:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
