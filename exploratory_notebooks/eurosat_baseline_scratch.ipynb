{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5650b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()  # Opens a browser once to authenticate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet50\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import os, ssl, zipfile, urllib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d444694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set to use GPU: 1 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TARGET_GPU_INDEX = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if TARGET_GPU_INDEX < torch.cuda.device_count():\n",
    "        DEVICE = torch.device(f\"cuda:{TARGET_GPU_INDEX}\")\n",
    "        print(f\"Successfully set to use GPU: {TARGET_GPU_INDEX} ({torch.cuda.get_device_name(TARGET_GPU_INDEX)})\")\n",
    "    else:\n",
    "        print(f\"Error: Physical GPU {TARGET_GPU_INDEX} is not available. There are only {torch.cuda.device_count()} GPUs (0 to {torch.cuda.device_count() - 1}).\")\n",
    "        print(\"Falling back to CPU.\")\n",
    "        DEVICE = torch.device(\"CPU\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Falling back to CPU.\")\n",
    "    DEVICE = torch.device(\"CPU\")\n",
    "\n",
    "print(f\"Final DEVICE variable is set to: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"Current PyTorch default device: {torch.cuda.current_device()}\")\n",
    "    torch.cuda.set_device(TARGET_GPU_INDEX)\n",
    "    print(f\"Current PyTorch default device (after set_device): {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "dummy_tensor = torch.randn(2, 2)\n",
    "dummy_tensor_on_gpu = dummy_tensor.to(DEVICE)\n",
    "print(f\"Dummy tensor is on device: {dummy_tensor_on_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOCAL_OR_COLAB = \"LOCAL\"\n",
    "SEED           = 42\n",
    "NUM_EPOCHS     = 34\n",
    "\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC   = 0.1\n",
    "TEST_FRAC  = 0.1\n",
    "\n",
    "# hyperparameter grid\n",
    "# BATCH_SIZES = [64, 128, 256]\n",
    "BATCH_SIZES = [64, 128]  # Using a single batch size for simplicity\n",
    "LRS = [1e-4, 3e-4]\n",
    "\n",
    "GRID        = [\n",
    "    (3.75e-4, 0.5  ),\n",
    "]\n",
    "\n",
    "WEIGHT_DECAY = 0.5\n",
    "\n",
    "BETAS=(0.9,0.98)\n",
    "EPS = 1e-8\n",
    "\n",
    "if LOCAL_OR_COLAB == \"LOCAL\":\n",
    "    DATA_DIR = \"/share/DEEPLEARNING/carvalhj/EuroSAT_RGB/\"\n",
    "else:\n",
    "    data_root = \"/content/EuroSAT_RGB\"\n",
    "    zip_path  = \"/content/EuroSAT.zip\"\n",
    "    if not os.path.exists(data_root):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
    "        )\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(\"/content\")\n",
    "        os.rename(\"/content/2750\", data_root)\n",
    "    DATA_DIR = data_root\n",
    "\n",
    "NUM_WORKERS = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bbde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mean_std(dataset, batch_size):\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        n_samples += batch_samples\n",
    "\n",
    "    mean /= n_samples\n",
    "    std /= n_samples\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "def get_data_loaders(data_dir, batch_size):\n",
    "\n",
    "    base_tf = transforms.ToTensor()\n",
    "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
    "    labels = np.array(ds_all.targets)   # numpy array of shape (N,)\n",
    "    num_classes = len(ds_all.classes)\n",
    "    total_count = len(ds_all)\n",
    "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
    "\n",
    "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
    "\n",
    "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
    "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
    "    print(f\"Computed mean: {mean}\")\n",
    "    print(f\"Computed std:  {std}\")\n",
    "\n",
    "    tf_final = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    #  full ImageFolder but now with normalization baked in\n",
    "    ds_all_norm = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
    "\n",
    "    train_ds = Subset(ds_all_norm, train_idx)\n",
    "    val_ds   = Subset(ds_all_norm, val_idx)\n",
    "    test_ds  = Subset(ds_all_norm, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "    print(f\"Train/Val/Test splits: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n",
    "\n",
    "def get_proportion(num_classes, dataset):\n",
    "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
    "\n",
    "def get_split_indexes(labels, total_count):\n",
    "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
    "    n_temp = total_count - n_train   # this is val + test\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=n_train,\n",
    "        test_size=n_temp,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    # Train and temp(val+test) indices\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
    "\n",
    "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
    "    n_test = total_count - n_train - n_val\n",
    "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
    "\n",
    "    labels_temp = labels[temp_idx]\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=n_val,\n",
    "        test_size=n_test,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
    "\n",
    "    val_idx = temp_idx[val_idx_in_temp]\n",
    "    test_idx = temp_idx[test_idx_in_temp]\n",
    "\n",
    "    assert len(train_idx) == n_train\n",
    "    assert len(val_idx) == n_val\n",
    "    assert len(test_idx) == n_test\n",
    "\n",
    "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
    "    return train_idx,val_idx,test_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9c5e5",
   "metadata": {},
   "source": [
    "# Logistic regresssion with Scikit-learn for comparing linear probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0804bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = BATCH_SIZES[0]\n",
    "LEARNING_RATE, WEIGHT_DECAY = GRID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e80da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_feature_extractor_model():\n",
    "\n",
    "    base_model = models.resnet50(weights=None) # No pre-trained ImageNet weights\n",
    "    feature_extractor = nn.Sequential(*list(base_model.children())[:-1]) # Exclude the last fc layer\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "    feature_extractor.eval() \n",
    "    feature_extractor.to(DEVICE)\n",
    "    print(\"Common ResNet50 feature extractor (randomly initialized and frozen) created.\")\n",
    "    return feature_extractor\n",
    "\n",
    "def extract_features(dataloader, model):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(DEVICE)\n",
    "            features = model(images)\n",
    "            features = features.squeeze(-1).squeeze(-1) # Flatten (batch_size, 2048, 1, 1) to (batch_size, 2048)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    return np.vstack(all_features), np.concatenate(all_labels)\n",
    "\n",
    "class PyTorchLinearProbingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, shared_feature_extractor, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = shared_feature_extractor\n",
    "        self.backbone.eval()\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        feature_dim = 2048 \n",
    "        self.linear_head = nn.Linear(feature_dim, num_classes)\n",
    "        self.linear_head.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "            features = features.squeeze(-1).squeeze(-1)\n",
    "        logits = self.linear_head(features)\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} (Train)\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct_predictions/total_samples:.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.6f}\") # Added LR to postfix\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def evaluate_test_set_pytorch(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating Test Set (PyTorch)\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(classification_report(all_labels, all_preds, target_names=[f'class_{i}' for i in range(num_classes)]))\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
    "    return test_accuracy\n",
    "\n",
    "def make_optimizer_scheduler(params, lr, wd, steps_per_epoch, epochs):\n",
    "    total_steps  = epochs * steps_per_epoch\n",
    "    warmup_steps = steps_per_epoch\n",
    "    opt = optim.Adam(params, lr=lr, betas=(0.9,0.98), eps=1e-8, weight_decay=wd)\n",
    "    sched = SequentialLR(\n",
    "        opt,\n",
    "        schedulers=[\n",
    "            LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
    "            CosineAnnealingLR(opt, T_max=total_steps - warmup_steps)\n",
    "        ],\n",
    "        milestones=[warmup_steps]\n",
    "    )\n",
    "    return opt, sched\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def hyperparameter_search_pytorch(train_loader, val_loader, num_classes, common_feature_extractor):\n",
    "    best_val_accuracy = -1.0\n",
    "    best_params = {}\n",
    "\n",
    "    print(\"\\n--- Starting PyTorch Hyperparameter Search ---\")\n",
    "    \n",
    "    for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
    "        epochs = NUM_EPOCHS\n",
    "\n",
    "        print(f\"\\n--- Trying config: LR={lr}, WD={wd}, Epochs={epochs} ---\")\n",
    "\n",
    "        model = PyTorchLinearProbingModel(common_feature_extractor, num_classes=num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        steps_per_epoch = len(train_loader) \n",
    "        optimizer, scheduler = make_optimizer_scheduler(\n",
    "            model.linear_head.parameters(),\n",
    "            lr,\n",
    "            wd,\n",
    "            steps_per_epoch,\n",
    "            epochs\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, epoch, epochs)\n",
    "            val_loss, val_acc = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_params = {\n",
    "                'batch_size': bs,\n",
    "                'learning_rate': lr,\n",
    "                'weight_decay': wd,\n",
    "                'epochs': epochs\n",
    "            }\n",
    "            print(f\"New best validation accuracy: {best_val_accuracy:.4f} with params: {best_params}\")\n",
    "\n",
    "    print(\"\\n--- PyTorch Hyperparameter Search Complete ---\")\n",
    "    print(f\"Best validation accuracy found: {best_val_accuracy:.4f}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    return best_params, best_val_accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Computed mean: [0.3441457152366638, 0.38009852170944214, 0.40766340494155884]\n",
      "Computed std:  [0.09299741685390472, 0.06464488059282303, 0.054139144718647]\n",
      "Train/Val/Test splits: 21600/2700/2700\n",
      "Common ResNet50 feature extractor (randomly initialized and frozen) created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 675/675 [00:05<00:00, 126.97it/s]\n",
      "Extracting features: 100%|██████████| 85/85 [00:00<00:00, 104.00it/s]\n",
      "Extracting features: 100%|██████████| 85/85 [00:00<00:00, 105.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (21600, 2048), labels shape: (21600,)\n",
      "Validation features shape: (2700, 2048), labels shape: (2700,)\n",
      "Test features shape: (2700, 2048), labels shape: (2700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found for Logistic Regression: {'C': 1.0, 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "Best cross-validation accuracy for Logistic Regression: 0.6199\n",
      "Logistic Regression Validation Accuracy: 0.6111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_loader, val_loader, test_loader, num_classes = get_data_loaders(DATA_DIR, BATCH_SIZE)\n",
    "\n",
    "common_feature_extractor = get_common_feature_extractor_model()\n",
    "\n",
    "X_train_features, y_train_labels = extract_features(train_loader, common_feature_extractor)\n",
    "X_val_features, y_val_labels = extract_features(val_loader, common_feature_extractor)\n",
    "X_test_features, y_test_labels = extract_features(test_loader, common_feature_extractor)\n",
    "\n",
    "X_train_val_features = np.vstack((X_train_features, X_val_features))\n",
    "y_train_val_labels = np.concatenate((y_train_labels, y_val_labels))\n",
    "\n",
    "print(f\"Train features shape: {X_train_features.shape}, labels shape: {y_train_labels.shape}\")\n",
    "print(f\"Validation features shape: {X_val_features.shape}, labels shape: {y_val_labels.shape}\")\n",
    "print(f\"Test features shape: {X_test_features.shape}, labels shape: {y_test_labels.shape}\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "logistic_classifier = LogisticRegression(random_state=SEED)\n",
    "grid_search = GridSearchCV(\n",
    "    logistic_classifier,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0 \n",
    ")\n",
    "grid_search.fit(X_train_features, y_train_labels)\n",
    "\n",
    "print(f\"\\nBest parameters found for Logistic Regression: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy for Logistic Regression: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_logistic_model = grid_search.best_estimator_\n",
    "y_pred_val_lr = best_logistic_model.predict(X_val_features)\n",
    "val_accuracy_lr = accuracy_score(y_val_labels, y_pred_val_lr)\n",
    "print(f\"Logistic Regression Validation Accuracy: {val_accuracy_lr:.4f}\")\n",
    "\n",
    "final_logistic_model = LogisticRegression(**grid_search.best_params_, random_state=SEED)\n",
    "final_logistic_model.fit(X_train_val_features, y_train_val_labels)\n",
    "\n",
    "y_pred_test_lr = final_logistic_model.predict(X_test_features)\n",
    "test_accuracy_lr = accuracy_score(y_test_labels, y_pred_test_lr)\n",
    "print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "\n",
    "print(\"\\n--- Running PyTorch Linear Probing ---\")\n",
    "\n",
    "best_pytorch_params, best_pytorch_val_accuracy = hyperparameter_search_pytorch(\n",
    "        train_loader, val_loader, num_classes, common_feature_extractor\n",
    "    )\n",
    "\n",
    "print(f\"\\nPyTorch Best Hyperparameters: {best_pytorch_params}\")\n",
    "print(f\"PyTorch Best Validation Accuracy: {best_pytorch_val_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Final PyTorch Linear Probing Model on combined Train+Val set ---\")\n",
    "final_pytorch_model = PyTorchLinearProbingModel(common_feature_extractor, num_classes=num_classes)\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "combined_train_val_dataset = ConcatDataset([train_loader.dataset, val_loader.dataset])\n",
    "combined_train_val_loader = DataLoader(combined_train_val_dataset,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=NUM_WORKERS,\n",
    "                                        generator=torch.Generator().manual_seed(SEED),\n",
    "                                        pin_memory=True)\n",
    "\n",
    "steps_per_epoch = len(combined_train_val_loader)\n",
    "optimizer, scheduler = make_optimizer_scheduler(\n",
    "    final_pytorch_model.linear_head.parameters(),\n",
    "    LEARNING_RATE,\n",
    "    WEIGHT_DECAY,\n",
    "    steps_per_epoch,\n",
    "    NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"Starting PyTorch Linear Probing Training for {NUM_EPOCHS} epochs on combined Train+Val set.\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(final_pytorch_model, combined_train_val_loader, criterion_final, optimizer, scheduler, epoch, NUM_EPOCHS)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "test_accuracy_pytorch = evaluate_test_set_pytorch(final_pytorch_model, test_loader, num_classes)\n",
    "\n",
    "\n",
    "print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "print(f\"PyTorch Linear Probing Test Accuracy: {test_accuracy_pytorch:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
