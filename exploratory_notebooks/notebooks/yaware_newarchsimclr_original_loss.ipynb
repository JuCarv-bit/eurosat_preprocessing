{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5325f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Implementation of the SIMCLR with resnet18 backbone\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()       # reads .env and sets os.environ\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1e0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74f1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaware import information_extraction\n",
    "from new_architecture_simclr.network import resnet18, projection_MLP\n",
    "from yaware.haversine_loss import HaversineRBFNTXenLoss\n",
    "from yaware.losses import GeneralizedSupervisedNTXenLoss\n",
    "import simclr.data.datamodule as simclr_datamodule\n",
    "\n",
    "\n",
    "from simclr.data.datamodule import compute_mean_std, prepare_data, combine_train_val_loaders, SimCLRDataset, get_split_indexes\n",
    "from utils.version_utils import print_versions, configure_gpu_device, set_seed\n",
    "\n",
    "from simclr.data.transforms import  get_transforms\n",
    "from simclr.models.loss import NTXentLoss\n",
    "from simclr.probes.logistic import get_probe_loaders, run_logistic_probe_experiment\n",
    "from simclr.utils.scheduler import make_optimizer_scheduler\n",
    "from simclr.utils.misc import evaluate\n",
    "from simclr.data.mydataloaders import get_data_loaders_train_test_linear_probe\n",
    "from simclr.config import CONFIG\n",
    "from simclr.train import train_simclr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c64f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"BATCH_SIZE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f46a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda version: 25.5.1\n",
      "Python version: 3.10.13\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "Torchvision version: 0.20.1\n",
      "Successfully set to use GPU: 1 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "print_versions()\n",
    "set_seed(seed=42)\n",
    "TARGET_GPU_INDEX = CONFIG[\"TARGET_GPU_INDEX\"] if \"TARGET_GPU_INDEX\" in CONFIG else 0  # Default to 0 if not set\n",
    "DEVICE = configure_gpu_device(TARGET_GPU_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1f36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_AWARE: True\n"
     ]
    }
   ],
   "source": [
    "# Prevent nondeterminism\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# split fractions\n",
    "TRAIN_FRAC = CONFIG[\"TRAIN_FRAC\"]\n",
    "VAL_FRAC   = CONFIG[\"VAL_FRAC\"]\n",
    "TEST_FRAC  = CONFIG[\"TEST_FRAC\"]\n",
    "\n",
    "SEED = CONFIG[\"SEED\"]\n",
    "\n",
    "PRETRAINED = False\n",
    "\n",
    "TEMPERATURE = CONFIG[\"TEMPERATURE\"]\n",
    "\n",
    "BETAS=(0.9,0.98)\n",
    "EPS = 1e-8\n",
    "\n",
    "GLOBAL_SEED = CONFIG[\"SEED\"]\n",
    "NUM_WORKERS = CONFIG[\"NUM_WORKERS\"]\n",
    "\n",
    "EUROSAT_IMAGE_SIZE = (64, 64)\n",
    "MODEL_INPUT_SIZE = [224, 224]\n",
    "EPOCH_SAVE_INTERVAL = CONFIG[\"EPOCH_SAVE_INTERVAL\"]\n",
    "\n",
    "MS_PATH  = \"/users/c/carvalhj/datasets/eurosat/EuroSAT_MS/\"\n",
    "RGB_PATH = \"/users/c/carvalhj/datasets/eurosat/EuroSAT_RGB/\"\n",
    "\n",
    "BATCH_SIZE = CONFIG[\"BATCH_SIZE\"]\n",
    "YAWARE = CONFIG[\"Y_AWARE\"] if \"Y_AWARE\" in CONFIG else False\n",
    "print(f\"Y_AWARE: {YAWARE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549abd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(dataset='eurosat', model='resnet18', n_classes=10, feature_dim=512, proj_dim=64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"SimCLR EuroSAT\")\n",
    "parser.add_argument(\"--dataset\",    type=str,   default=\"eurosat\",\n",
    "                    help=\"dataset name (controls CIFAR‑stem in network.py)\")\n",
    "parser.add_argument(\"--model\",      type=str,   default=\"resnet18\",\n",
    "                    choices=[\"resnet18\",\"resnet34\",\"resnet50\",\"resnet101\",\"resnet152\"],\n",
    "                    help=\"which ResNet depth to use\")\n",
    "parser.add_argument(\"--n_classes\",  type=int,   default=10,\n",
    "                    help=\"# of EuroSAT semantic classes\")\n",
    "parser.add_argument(\"--feature_dim\",type=int,   default=512,\n",
    "                    help=\"backbone output dim (for SimCLR we set fc→feature_dim)\")\n",
    "parser.add_argument(\"--proj_dim\",   type=int,   default=CONFIG[\"PROJ_DIM\"],\n",
    "                    help=\"projection MLP output dim (usually 128)\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c63b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_encoder = resnet18(\n",
    "    args,\n",
    "    num_classes=args.feature_dim,     # make fc output = feature_dim\n",
    "    zero_init_residual=False\n",
    ")\n",
    "proj_head = projection_MLP(args)\n",
    "\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, base_encoder, proj_head):\n",
    "        super().__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.projection_head = proj_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        proj = self.projection_head(feat)\n",
    "        return feat, proj\n",
    "    \n",
    "simclr_model = SimCLRModel(base_encoder, proj_head).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting run with seed 42 ===\n",
      "Loading metadata from cache (eurosat_metadata_cache.pkl)\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Loaders: train=84, val=11, test=11 batches\n",
      "Starting SimCLR training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/wandb/run-20250730_135249-31u2dpoo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/31u2dpoo' target=\"_blank\">BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS200</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/31u2dpoo' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/31u2dpoo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SimCLR training at 2025-07-30 13:52:50\n",
      "Measuring time...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0f5f10bea94bfbad0949f2732050b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=200).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[probe] val accuracy = 85.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=200).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[probe] val accuracy = 89.64%\n",
      "Epoch 20/200 | Train Loss: 12.4068, Val Loss: 11.9615 | Logistic Probe Acc (Val): 0.854, Logistic Probe Acc (Train): 0.896 | Contrastive Acc (Train): 0.105, Contrastive Acc (Val): 0.102 | KNN Acc (Val): 0.754\n"
     ]
    }
   ],
   "source": [
    "seeds = [GLOBAL_SEED]\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "    if YAWARE:\n",
    "        loaders = information_extraction.get_data_loaders(MS_PATH, RGB_PATH, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        loaders = simclr_datamodule.get_data_loaders(RGB_PATH, BATCH_SIZE)\n",
    "    train_loader, val_loader, test_loader, val_subset_no_transform, num_classes = loaders\n",
    "\n",
    "    wd =  0.5 \n",
    "    optimizer, scheduler = make_optimizer_scheduler(\n",
    "        simclr_model.parameters(),\n",
    "        CONFIG[\"LR\"],\n",
    "        CONFIG[\"WD\"],\n",
    "        len(train_loader),\n",
    "        CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "        )\n",
    "    \n",
    "    bs = CONFIG[\"BATCH_SIZE\"]\n",
    "    if YAWARE:\n",
    "        if CONFIG[\"ORIGINAL_Y_AWARE\"]:\n",
    "            loss_fn = GeneralizedSupervisedNTXenLoss(\n",
    "                temperature=TEMPERATURE,\n",
    "                return_logits=True,\n",
    "                sigma=0.8\n",
    "            ).to(DEVICE)\n",
    "        else:\n",
    "            loss_fn = HaversineRBFNTXenLoss(temperature=0.9, sigma=0.003).to(DEVICE)\n",
    "    else:\n",
    "        loss_fn = NTXentLoss(\n",
    "            batch_size=bs,\n",
    "            temperature=TEMPERATURE,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    print(\"Starting SimCLR training...\")\n",
    "    epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "    lr = CONFIG[\"LR\"]\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"eurosat-contrastive-scratch\",\n",
    "        name=f\"BS{bs}_LR{lr:.0e}_SEED{seed}_TEMPERATURE{TEMPERATURE}_EPOCHS{epochs_simclr}\",\n",
    "        tags=[\"SimCLR\", \"EuroSAT\", \"Contrastive Learning\"],\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "    eval_transform, augment_transform = get_transforms(\n",
    "        mean = CONFIG[\"MEAN\"],\n",
    "        std = CONFIG[\"STD\"]\n",
    "    )  # these must match the transforms used in test_loader\n",
    "\n",
    "\n",
    "    probe_train_loader, probe_val_loader = get_probe_loaders(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        eval_transform,               # must match transforms used in test_loader\n",
    "        probe_batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "        yaware=YAWARE\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Starting SimCLR training at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start))}\")\n",
    "    train_simclr(\n",
    "        simclr_model,\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        probe_train_loader, \n",
    "        probe_val_loader,\n",
    "        optimizer, \n",
    "        loss_fn, \n",
    "        DEVICE,\n",
    "        simclr_epochs=CONFIG[\"EPOCHS_SIMCLR\"],\n",
    "        probe_lr=CONFIG[\"LR_LINEAR\"],\n",
    "        probe_epochs=1,            # 1 pass per epoch is typical\n",
    "        feature_dim=CONFIG[\"FEATURE_DIM\"],\n",
    "        num_classes=num_classes,\n",
    "        augment_transform=augment_transform,\n",
    "        val_subset_no_transform=val_subset_no_transform,\n",
    "        wandb_run=wandb_run,\n",
    "        scheduler=scheduler,\n",
    "        seed=seed,\n",
    "        yaware=YAWARE  # Set to True for Yaware model\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"SimCLR training completed in {end - start:.2f} seconds.\")\n",
    "    wandb_run.log({\n",
    "        \"training_time_seconds\": end - start,\n",
    "    })\n",
    "\n",
    "    wandb_run.finish()\n",
    "\n",
    "print(\"All runs completed.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean: [0.3449324667453766, 0.38065022230148315, 0.4079160690307617]\n",
      "Using std: [0.09294303506612778, 0.06471642106771469, 0.05417822301387787]\n",
      "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Train/Test loaders: 94/11 batches\n",
      "Successfully set to use GPU: 1 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 1\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/wandb/run-20250722_191942-5w4e2tka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5w4e2tka' target=\"_blank\">logistic_probe_seed42_temperature0.2_bs256</a></strong> to <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5w4e2tka' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5w4e2tka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] train+val loader has 94 batches\n",
      "Fitting logistic regression probe…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Probe] Train+Val Acc: 51.90%,  Test Acc: 58.30%\n",
      "Saved probe + encoder to models/logistic_probe_seed42_bs256.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5189910239361702, 0.582962962962963)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the saved model and run linear probe\n",
    "seed = CONFIG[\"SEED\"]\n",
    "bs = CONFIG[\"BATCH_SIZE\"]\n",
    "epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "simclr_lr = CONFIG[\"LR\"]\n",
    "lr_str = f\"{simclr_lr:.0e}\" if simclr_lr < 0.0001 else f\"{simclr_lr:.6f}\"\n",
    "model_path = f\"models/simclr_seed{seed}_bs{bs}_temp{TEMPERATURE}_Tepochs{epochs_simclr}_lr{lr_str}.pth\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model {model_path} does not exist. Please run the SimCLR pretraining first.\")\n",
    "\n",
    "checkpoint_path = model_path\n",
    "state_dict = torch.load(checkpoint_path, map_location=torch.device(DEVICE), weights_only=True)\n",
    "simclr_model.load_state_dict(state_dict)\n",
    "\n",
    "# Perform linear probe on train+val as train set, and test as test set\n",
    "train_loader, test_loader, num_classes = get_data_loaders_train_test_linear_probe(CONFIG[\"DATA_DIR_LOCAL\"], CONFIG[\"BATCH_SIZE\"])\n",
    "run_logistic_probe_experiment(\n",
    "    42,\n",
    "    train_loader,\n",
    "    None,  # No validation loader for linear probe\n",
    "    test_loader,\n",
    "    num_classes,\n",
    "    simclr_model,\n",
    "    bs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c41a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grid search for best hyperparameters\n",
    "\n",
    "batch_sizes_epochs = [\n",
    "    (64, 35),\n",
    "    (128, 40),\n",
    "    (256, 100),\n",
    "    (512, 100),\n",
    "    (1024, 150),\n",
    "]\n",
    "\n",
    "learning_rates = [\n",
    "    1e-3,\n",
    "    3.75e-4,\n",
    "    1e-4,\n",
    "    3.75e-5,\n",
    "    1e-5,\n",
    "]\n",
    "\n",
    "# use linspace for computing the temperature\n",
    "temperatures = np.linspace(0.05, 0.5, 5).tolist() # [0.05, 0.1625, 0.275, 0.3875, 0.5]\n",
    "temperatures.append(0.2)  # add the original temperature\n",
    "\n",
    "gpu_indexes = [0, 1]\n",
    "# put half of the experiments on each GPU\n",
    "gpu_experiments = {0: [], 1: []}\n",
    "all_acc = []\n",
    "\n",
    "# train simclr with different hyperparameters and apply linear probe\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaware_eurosat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
