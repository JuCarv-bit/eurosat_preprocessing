{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa9f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /share/homes/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5325f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Implementation of the SIMCLR with resnet18 backbone\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()       # reads .env and sets os.environ\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49aa4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/share/homes/carvalhj/projects/eurosat_preprocessing\")\n",
    "\n",
    "# Now import\n",
    "from yaware import information_extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1e0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74f1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaware import information_extraction\n",
    "from new_architecture_simclr.network import resnet18, projection_MLP\n",
    "from yaware.haversine_loss import HaversineRBFNTXenLoss\n",
    "from yaware.losses import GeneralizedSupervisedNTXenLoss\n",
    "import simclr.data.datamodule as simclr_datamodule\n",
    "\n",
    "\n",
    "from simclr.data.datamodule import compute_mean_std, prepare_data, combine_train_val_loaders, SimCLRDataset, get_split_indexes\n",
    "from utils.version_utils import print_versions, configure_gpu_device, set_seed\n",
    "\n",
    "from simclr.data.transforms import  get_transforms\n",
    "from simclr.models.loss import NTXentLoss\n",
    "from simclr.probes.logistic import get_probe_loaders, run_logistic_probe_experiment\n",
    "from simclr.utils.scheduler import make_optimizer_scheduler\n",
    "from simclr.utils.misc import evaluate\n",
    "from simclr.data.mydataloaders import get_data_loaders_train_test_linear_probe\n",
    "from simclr.config import CONFIG\n",
    "from simclr.train import train_simclr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935363ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_locations = [\"/share/homes/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/models/2025-07-29_19-41-31/simclr_seed42_bs256_temp0.2_Tepochs200_lr0.000375_epoch_200.pth\",\n",
    "                     \"/share/homes/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/models/2025-07-29_19-35-58/simclr_seed42_bs256_temp0.2_Tepochs200_lr0.000375_epoch_200.pth\",\n",
    "                     \"/share/homes/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/models/2025-07-29_19-25-49/simclr_seed42_bs256_temp0.2_Tepochs200_lr0.000375_epoch_200.pth\"]\n",
    "\n",
    "wandb_runs = [(\"SimCLR\", \"trd1en52\"), \n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef035cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y_AWARE': True, 'ORIGINAL_Y_AWARE': True, 'TARGET_GPU_INDEX': 1, 'LOCAL_OR_COLAB': 'LOCAL', 'DATA_DIR_LOCAL': '/share/DEEPLEARNING/carvalhj/EuroSAT_RGB/', 'DATA_DIR_COLAB': '/content/EuroSAT_RGB', 'DATA_DIR_EUROSAT_MS': '/share/DEEPLEARNING/carvalhj/EuroSAT_MS/', 'ZIP_PATH': '/content/EuroSAT.zip', 'EUROSAT_URL': 'https://madm.dfki.de/files/sentinel/EuroSAT.zip', 'SEED': 42, 'BATCH_SIZE': 256, 'LR': 0.000375, 'WD': 0.5, 'LR_LINEAR': 0.000375, 'EPOCHS_SIMCLR': 2, 'EPOCHS_LINEAR': 2, 'EPOCH_SAVE_INTERVAL': 1, 'INTERVAL_EPOCHS_LINEAR_PROBE': 20, 'INTERVAL_EPOCHS_KNN': 20, 'INTERVAL_CONTRASTIVE_ACC': 5, 'TEMPERATURE': 0.2, 'PROJ_DIM': 64, 'FEATURE_DIM': 512, 'MEAN': [0.3441457152366638, 0.3800985515117645, 0.40766361355781555], 'STD': [0.09299741685390472, 0.06464490294456482, 0.05413917079567909], 'NUM_WORKERS': 8, 'K': 5, 'TRAIN_FRAC': 0.8, 'VAL_FRAC': 0.1, 'TEST_FRAC': 0.1, 'EUROSAT_IMAGE_SIZE': (64, 64)}\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f46a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda version: 25.5.1\n",
      "Python version: 3.10.13\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "Torchvision version: 0.20.1\n",
      "Successfully set to use GPU: 1 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 0\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "print_versions()\n",
    "set_seed(seed=42)\n",
    "TARGET_GPU_INDEX = CONFIG[\"TARGET_GPU_INDEX\"] if \"TARGET_GPU_INDEX\" in CONFIG else 0  # Default to 0 if not set\n",
    "DEVICE = configure_gpu_device(TARGET_GPU_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1f36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_AWARE: True\n"
     ]
    }
   ],
   "source": [
    "# Prevent nondeterminism\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# split fractions\n",
    "TRAIN_FRAC = CONFIG[\"TRAIN_FRAC\"]\n",
    "VAL_FRAC   = CONFIG[\"VAL_FRAC\"]\n",
    "TEST_FRAC  = CONFIG[\"TEST_FRAC\"]\n",
    "\n",
    "SEED = CONFIG[\"SEED\"]\n",
    "\n",
    "PRETRAINED = False\n",
    "\n",
    "TEMPERATURE = CONFIG[\"TEMPERATURE\"]\n",
    "\n",
    "BETAS=(0.9,0.98)\n",
    "EPS = 1e-8\n",
    "\n",
    "GLOBAL_SEED = CONFIG[\"SEED\"]\n",
    "NUM_WORKERS = CONFIG[\"NUM_WORKERS\"]\n",
    "\n",
    "EUROSAT_IMAGE_SIZE = (64, 64)\n",
    "MODEL_INPUT_SIZE = [224, 224]\n",
    "EPOCH_SAVE_INTERVAL = CONFIG[\"EPOCH_SAVE_INTERVAL\"]\n",
    "\n",
    "MS_PATH  = \"/users/c/carvalhj/datasets/eurosat/EuroSAT_MS/\"\n",
    "RGB_PATH = \"/users/c/carvalhj/datasets/eurosat/EuroSAT_RGB/\"\n",
    "\n",
    "BATCH_SIZE = CONFIG[\"BATCH_SIZE\"]\n",
    "YAWARE = CONFIG[\"Y_AWARE\"] if \"Y_AWARE\" in CONFIG else False\n",
    "print(f\"Y_AWARE: {YAWARE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549abd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(dataset='eurosat', model='resnet18', n_classes=10, feature_dim=512, proj_dim=64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"SimCLR EuroSAT\")\n",
    "parser.add_argument(\"--dataset\",    type=str,   default=\"eurosat\",\n",
    "                    help=\"dataset name (controls CIFAR‑stem in network.py)\")\n",
    "parser.add_argument(\"--model\",      type=str,   default=\"resnet18\",\n",
    "                    choices=[\"resnet18\",\"resnet34\",\"resnet50\",\"resnet101\",\"resnet152\"],\n",
    "                    help=\"which ResNet depth to use\")\n",
    "parser.add_argument(\"--n_classes\",  type=int,   default=10,\n",
    "                    help=\"# of EuroSAT semantic classes\")\n",
    "parser.add_argument(\"--feature_dim\",type=int,   default=512,\n",
    "                    help=\"backbone output dim (for SimCLR we set fc→feature_dim)\")\n",
    "parser.add_argument(\"--proj_dim\",   type=int,   default=CONFIG[\"PROJ_DIM\"],\n",
    "                    help=\"projection MLP output dim (usually 128)\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c63b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_encoder = resnet18(\n",
    "    args,\n",
    "    num_classes=args.feature_dim,     # make fc output = feature_dim\n",
    "    zero_init_residual=False\n",
    ")\n",
    "proj_head = projection_MLP(args)\n",
    "\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, base_encoder, proj_head):\n",
    "        super().__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.projection_head = proj_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        proj = self.projection_head(feat)\n",
    "        return feat, proj\n",
    "    \n",
    "simclr_model = SimCLRModel(base_encoder, proj_head).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cd7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting run with seed 42 ===\n",
      "Loading metadata from cache (eurosat_metadata_cache.pkl)\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Loaders: train=84, val=11, test=11 batches\n",
      "Starting SimCLR training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/homes/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/wandb/run-20250805_003730-rwhnium1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/rwhnium1' target=\"_blank\">BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS2</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/rwhnium1' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/rwhnium1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SimCLR training at 2025-08-05 00:37:31\n",
      "Measuring time...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597e8a0e3b384c4f93c0cc70bcc0c3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final contrastive accuracy on val split: 2.46%\n",
      "Final contrastive accuracy on train split: 2.60%\n",
      "Final kNN (k=5) on train: 100.00%\n",
      "Final kNN (k=5) on val: 75.93%\n",
      "Final kNN (k=1) on train: 100.00%\n",
      "Final kNN (k=1) on val: 75.93%\n",
      "\n",
      "=== Timing Breakdown ===\n",
      "load_batch     : 1.0s (0.5s/epoch)\n",
      "forward        : 64.8s (32.4s/epoch)\n",
      "loss+backward+opt: 140.7s (70.4s/epoch)\n",
      "scheduler      : 0.0s (0.0s/epoch)\n",
      "val_forward    : 8.2s (4.1s/epoch)\n",
      "contrastive_acc: 0.0s (0.0s/epoch)\n",
      "linear_probe   : 0.0s (0.0s/epoch)\n",
      "knn            : 0.0s (0.0s/epoch)\n",
      "checkpoint     : 0.2s (0.1s/epoch)\n",
      "logging        : 0.0s (0.0s/epoch)\n",
      "SimCLR training completed in 479.77 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>contrastive_train_acc</td><td>▁▁</td></tr><tr><td>contrastive_val_acc</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>final_contrastive_accuracy</td><td>▁</td></tr><tr><td>final_contrastive_accuracy_train</td><td>▁</td></tr><tr><td>final_knn_acc</td><td>▁</td></tr><tr><td>final_knn_acc_k1</td><td>▁</td></tr><tr><td>final_knn_train_acc</td><td>▁</td></tr><tr><td>final_knn_train_acc_k1</td><td>▁</td></tr><tr><td>knn_val_acc</td><td>▁▁</td></tr><tr><td>logistic_probe_acc</td><td>▁▁</td></tr><tr><td>logistic_probe_train_acc</td><td>▁▁</td></tr><tr><td>simclr_train_loss</td><td>█▁</td></tr><tr><td>simclr_val_loss</td><td>█▁</td></tr><tr><td>training_time_seconds</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>contrastive_train_acc</td><td>0</td></tr><tr><td>contrastive_val_acc</td><td>0</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>final_contrastive_accuracy</td><td>0.02463</td></tr><tr><td>final_contrastive_accuracy_train</td><td>0.02597</td></tr><tr><td>final_knn_acc</td><td>0.75926</td></tr><tr><td>final_knn_acc_k1</td><td>0.75926</td></tr><tr><td>final_knn_train_acc</td><td>1</td></tr><tr><td>final_knn_train_acc_k1</td><td>1</td></tr><tr><td>knn_val_acc</td><td>0</td></tr><tr><td>logistic_probe_acc</td><td>0</td></tr><tr><td>logistic_probe_train_acc</td><td>0</td></tr><tr><td>model_summary</td><td>SimCLRModel(<br>  (enco...</td></tr><tr><td>simclr_train_loss</td><td>12.76377</td></tr><tr><td>simclr_val_loss</td><td>12.50781</td></tr><tr><td>training_time_seconds</td><td>479.76506</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BS256_LR4e-04_SEED42_TEMPERATURE0.2_EPOCHS2</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/rwhnium1' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch/runs/rwhnium1</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-contrastive-scratch' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-contrastive-scratch</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250805_003730-rwhnium1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All runs completed.\n"
     ]
    }
   ],
   "source": [
    "seeds = [GLOBAL_SEED]\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Starting run with seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "    if YAWARE:\n",
    "        loaders = information_extraction.get_data_loaders(MS_PATH, RGB_PATH, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        loaders = simclr_datamodule.get_data_loaders(RGB_PATH, BATCH_SIZE)\n",
    "    train_loader, val_loader, test_loader, val_subset_no_transform, num_classes = loaders\n",
    "\n",
    "    wd =  0.5 \n",
    "    optimizer, scheduler = make_optimizer_scheduler(\n",
    "        simclr_model.parameters(),\n",
    "        CONFIG[\"LR\"],\n",
    "        CONFIG[\"WD\"],\n",
    "        len(train_loader),\n",
    "        CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "        )\n",
    "    \n",
    "    bs = CONFIG[\"BATCH_SIZE\"]\n",
    "    if YAWARE:\n",
    "        if CONFIG[\"ORIGINAL_Y_AWARE\"]:\n",
    "            loss_fn = GeneralizedSupervisedNTXenLoss(\n",
    "                temperature=TEMPERATURE,\n",
    "                return_logits=True,\n",
    "                sigma=0.8\n",
    "            ).to(DEVICE)\n",
    "        else:\n",
    "            loss_fn = HaversineRBFNTXenLoss(temperature=0.9, sigma=0.003).to(DEVICE)\n",
    "    else:\n",
    "        loss_fn = NTXentLoss(\n",
    "            batch_size=bs,\n",
    "            temperature=TEMPERATURE,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    print(\"Starting SimCLR training...\")\n",
    "    epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "    lr = CONFIG[\"LR\"]\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"eurosat-contrastive-scratch\",\n",
    "        name=f\"BS{bs}_LR{lr:.0e}_SEED{seed}_TEMPERATURE{TEMPERATURE}_EPOCHS{epochs_simclr}\",\n",
    "        tags=[\"SimCLR\", \"EuroSAT\", \"Contrastive Learning\"],\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "    wandb.log({\"model_summary\": str(simclr_model)})\n",
    "\n",
    "    eval_transform, augment_transform = get_transforms(\n",
    "        mean = CONFIG[\"MEAN\"],\n",
    "        std = CONFIG[\"STD\"]\n",
    "    )  # these must match the transforms used in test_loader\n",
    "\n",
    "\n",
    "    probe_train_loader, probe_val_loader = get_probe_loaders(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        eval_transform,               # must match transforms used in test_loader\n",
    "        probe_batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "        yaware=YAWARE\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Starting SimCLR training at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start))}\")\n",
    "    filename_pretrained_weights = train_simclr(\n",
    "        simclr_model,\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        probe_train_loader, \n",
    "        probe_val_loader,\n",
    "        optimizer, \n",
    "        loss_fn, \n",
    "        DEVICE,\n",
    "        simclr_epochs=CONFIG[\"EPOCHS_SIMCLR\"],\n",
    "        probe_lr=CONFIG[\"LR_LINEAR\"],\n",
    "        probe_epochs=1,            # 1 pass per epoch is typical\n",
    "        feature_dim=CONFIG[\"FEATURE_DIM\"],\n",
    "        num_classes=num_classes,\n",
    "        augment_transform=augment_transform,\n",
    "        val_subset_no_transform=val_subset_no_transform,\n",
    "        wandb_run=wandb_run,\n",
    "        scheduler=scheduler,\n",
    "        seed=seed,\n",
    "        yaware=YAWARE \n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"SimCLR training completed in {end - start:.2f} seconds.\")\n",
    "    wandb_run.log({\n",
    "        \"training_time_seconds\": end - start,\n",
    "    })\n",
    "\n",
    "    wandb_run.finish()\n",
    "\n",
    "print(\"All runs completed.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c7dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model path: models/2025-08-05_00-37-31/simclr_seed42_bs256_temp0.2_Tepochs2_lr0.000375_epoch_002.pth\n",
      "Using mean: [0.3441457152366638, 0.3800986111164093, 0.40766361355781555]\n",
      "Using std: [0.09299743920564651, 0.06464490294456482, 0.054139167070388794]\n",
      "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "Stratified split sizes: train=21600, val=2700, test=2700\n",
      "Train/Test loaders: 94/11 batches\n",
      "Successfully set to use GPU: 1 (NVIDIA RTX A6000)\n",
      "Final DEVICE variable is set to: cuda:1\n",
      "Current PyTorch default device: 1\n",
      "Current PyTorch default device (after set_device): 1\n",
      "Dummy tensor is on device: cuda:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/homes/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/notebooks/wandb/run-20250805_004532-5gxwrtsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5gxwrtsg' target=\"_blank\">logistic_probe_seed42_temperature0.2_bs256</a></strong> to <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5gxwrtsg' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5gxwrtsg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] train+val loader has 94 batches\n",
      "Fitting logistic regression probe…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/c/carvalhj/miniconda3/envs/yaware_eurosat/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Probe] Train+Val Acc: 52.58%,  Test Acc: 59.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>probe_test_accuracy</td><td>▁</td></tr><tr><td>probe_trainval_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>probe_test_accuracy</td><td>59.22222</td></tr><tr><td>probe_trainval_accuracy</td><td>52.57646</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logistic_probe_seed42_temperature0.2_bs256</strong> at: <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5gxwrtsg' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr/runs/5gxwrtsg</a><br> View project at: <a href='https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr' target=\"_blank\">https://wandb.ai/analiju-paris/logistic_probe_eurosat-simclr</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250805_004532-5gxwrtsg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved probe + encoder to models/2025-08-05_00-37-31/logistic_probe_seed42_bs256_temp0.2.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5257646276595744, 0.5922222222222222)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the saved model and run linear probe\n",
    "seed = CONFIG[\"SEED\"]\n",
    "bs = CONFIG[\"BATCH_SIZE\"]\n",
    "epochs_simclr = CONFIG[\"EPOCHS_SIMCLR\"]\n",
    "simclr_lr = CONFIG[\"LR\"]\n",
    "lr_str = f\"{simclr_lr:.0e}\" if simclr_lr < 0.0001 else f\"{simclr_lr:.6f}\"\n",
    "# model_path = f\"models/simclr_seed{seed}_bs{bs}_temp{TEMPERATURE}_Tepochs{epochs_simclr}_lr{lr_str}.pth\"\n",
    "model_path = filename_pretrained_weights\n",
    "print(f\"Using model path: {model_path}\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model {model_path} does not exist. Please run the SimCLR pretraining first.\")\n",
    "\n",
    "checkpoint_path = model_path\n",
    "state_dict = torch.load(checkpoint_path, map_location=torch.device(DEVICE), weights_only=True)\n",
    "simclr_model.load_state_dict(state_dict)\n",
    "\n",
    "# Perform linear probe on train+val as train set, and test as test set\n",
    "train_loader, test_loader, num_classes = get_data_loaders_train_test_linear_probe(CONFIG[\"DATA_DIR_LOCAL\"], CONFIG[\"BATCH_SIZE\"])\n",
    "run_logistic_probe_experiment(\n",
    "    42,\n",
    "    train_loader,\n",
    "    None,  # No validation loader for linear probe\n",
    "    test_loader,\n",
    "    num_classes,\n",
    "    simclr_model,\n",
    "    bs,\n",
    "    save_dir=os.path.dirname(model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e70d99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/2025-08-05_00-11-51/logistic_probe_seed42_bs256_temp0.2.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaware_eurosat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
