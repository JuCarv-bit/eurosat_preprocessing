{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaliju\u001b[0m (\u001b[33manaliju-paris\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()  # Opens a browser once to authenticate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os, ssl, zipfile, urllib\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LinearLR, SequentialLR, MultiStepLR\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "LOCAL_OR_COLAB = \"LOCAL\"\n",
        "SEED           = 42\n",
        "NUM_EPOCHS     = 34\n",
        "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TRAIN_FRAC = 0.8\n",
        "VAL_FRAC   = 0.1\n",
        "TEST_FRAC  = 0.1\n",
        "\n",
        "# hyperparameter grid\n",
        "# BATCH_SIZES = [64, 128, 256]\n",
        "BATCH_SIZES = [512]  # Using a single batch size for simplicity\n",
        "LRS = [1e-4, 3e-4]\n",
        "\n",
        "GRID = product(\n",
        "    [0.1, 0.01],    # learning rate\n",
        "    [0.01, 0.0001]  # weight decay\n",
        ")\n",
        "\n",
        "TRAINING_SCHEDULES = {\n",
        "    \"short\": {\"p\": [750, 1500, 2250, 2500], \"w\": 200, \"unit\": \"steps\"},\n",
        "    \"medium\": {\"p\": [3000, 6000, 9000, 10000], \"w\": 500, \"unit\": \"steps\"},\n",
        "    \"long\": {\"p\": [30, 60, 80, 90], \"w\": 5, \"unit\": \"epochs\"}\n",
        "}\n",
        "\n",
        "# BETAS=(0.9,0.98)\n",
        "# EPS = 1e-8\n",
        "\n",
        "if LOCAL_OR_COLAB == \"LOCAL\":\n",
        "    DATA_DIR = \"/users/c/carvalhj/datasets/EuroSAT_RGB/\"\n",
        "else:\n",
        "    data_root = \"/content/EuroSAT_RGB\"\n",
        "    zip_path  = \"/content/EuroSAT.zip\"\n",
        "    if not os.path.exists(data_root):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\", zip_path\n",
        "        )\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(\"/content\")\n",
        "        os.rename(\"/content/2750\", data_root)\n",
        "    DATA_DIR = data_root\n",
        "\n",
        "NUM_WORKERS = 4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lr_scheduler(optimizer, total_training_steps, schedule_cfg, steps_per_epoch):\n",
        "    \"\"\"\n",
        "    Builds the learning rate scheduler based on the specified schedule configuration.\n",
        "\n",
        "    Args:\n",
        "        optimizer: The PyTorch optimizer.\n",
        "        total_training_steps: Total number of optimization steps for the entire training.\n",
        "        schedule_cfg: Dictionary containing 'p', 'w', and 'unit' for the schedule.\n",
        "        steps_per_epoch: Number of optimization steps in one epoch.\n",
        "    \"\"\"\n",
        "    warmup_iters = schedule_cfg[\"w\"]\n",
        "    milestones = [] # Points at which LR drops\n",
        "\n",
        "    if schedule_cfg[\"unit\"] == \"steps\":\n",
        "        milestones = schedule_cfg[\"p\"]\n",
        "    elif schedule_cfg[\"unit\"] == \"epochs\":\n",
        "        # Convert epoch milestones to step milestones\n",
        "        milestones = [m * steps_per_epoch for m in schedule_cfg[\"p\"]]\n",
        "        warmup_iters = schedule_cfg[\"w\"] * steps_per_epoch # Convert warmup epochs to steps\n",
        "\n",
        "    # Linear warm-up scheduler\n",
        "    warmup_scheduler = LinearLR(optimizer, start_factor=1e-6, end_factor=1.0, total_iters=warmup_iters)\n",
        "\n",
        "    # Step decay scheduler\n",
        "    # The image states \"decrease the learning rate by 10 per each learning phase p\"\n",
        "    # This means multiplying current LR by 0.1 at each milestone.\n",
        "    decay_scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "    # Combine them sequentially: warmup first, then decay\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        schedulers=[warmup_scheduler, decay_scheduler],\n",
        "        milestones=[warmup_iters]\n",
        "    )\n",
        "    return scheduler\n",
        "\n",
        "def hyperparam_search(pretrained=True):\n",
        "    best_val = -1.0\n",
        "    best_cfg = None\n",
        "    best_model = None\n",
        "\n",
        "    # Iterate over batch sizes, learning rates, weight decays, and training schedules\n",
        "    for bs, (lr, wd), schedule_name in product(BATCH_SIZES, GRID, TRAINING_SCHEDULES.keys()):\n",
        "\n",
        "        print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}, WD={wd:.1e}, Schedule={schedule_name}\")\n",
        "\n",
        "        tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs) # Assuming get_data_loaders is adapted for preprocessing\n",
        "\n",
        "\n",
        "        steps_per_epoch = len(tr_dl)\n",
        "\n",
        "        schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "        if schedule_cfg[\"unit\"] == \"steps\":\n",
        "\n",
        "            # Let's set total_steps to the last 'p' value for simplicity, or slightly more.\n",
        "            # A common approach is to set total_steps = max(schedule_cfg['p'])\n",
        "            total_steps = max(schedule_cfg[\"p\"]) # This is the total number of steps for the scheduler's milestones.\n",
        "            # We need to ensure NUM_EPOCHS is large enough to cover these steps.\n",
        "            NUM_EPOCHS_FOR_RUN = int(np.ceil(total_steps / steps_per_epoch)) + 1 # Add a buffer epoch\n",
        "        else: # schedule_cfg[\"unit\"] == \"epochs\"\n",
        "            total_epochs_from_schedule = max(schedule_cfg[\"p\"]) + schedule_cfg[\"w\"] # max 'p' + warmup epochs\n",
        "            NUM_EPOCHS_FOR_RUN = total_epochs_from_schedule # Total epochs to run\n",
        "            total_steps = NUM_EPOCHS_FOR_RUN * steps_per_epoch\n",
        "\n",
        "\n",
        "        # Build model (ResNet50 v2, assuming build_model handles this)\n",
        "        # Note: The document states \"ResNet50 v2 architecture (He et al., 2016)\".\n",
        "        # PyTorch's `torchvision.models.resnet50` is ResNet v1.\n",
        "        # ResNet v2 typically involves pre-activation. You might need a custom `build_model`\n",
        "        # or a specific implementation like from `timm` library if you want ResNet50 v2 exactly.\n",
        "        # For now, assuming your `build_model` can handle it or you are okay with standard ResNet.\n",
        "        model = build_model(n_cls, pretrained=pretrained)\n",
        "        model.to(DEVICE) # Move model to device\n",
        "\n",
        "        # Optimizer: SGD with momentum set to 0.9\n",
        "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Build the learning rate scheduler based on the current schedule\n",
        "        sched = build_lr_scheduler(opt, total_steps, schedule_cfg, steps_per_epoch)\n",
        "\n",
        "        # Start a W&B run\n",
        "        wandb_run = wandb.init(\n",
        "            project=\"eurosat-supervised-scratch-grid-search-lrsched\",\n",
        "            name=f\"BS{bs}_LR{lr:.0e}_WD{wd:.0e}_Sched_{schedule_name}\",\n",
        "            config={\n",
        "                \"batch_size\": bs,\n",
        "                \"learning_rate\": lr,\n",
        "                \"weight_decay\": wd,\n",
        "                \"schedule_name\": schedule_name,\n",
        "                \"total_epochs_for_run\": NUM_EPOCHS_FOR_RUN,\n",
        "                \"pretrained\": pretrained,\n",
        "                \"optimizer\": \"SGD_momentum_0.9\",\n",
        "                \"scheduler_type\": \"LinearWarmup_MultiStepLR\",\n",
        "                \"warmup_steps_or_epochs\": schedule_cfg[\"w\"],\n",
        "                \"decay_milestones\": schedule_cfg[\"p\"],\n",
        "                \"decay_unit\": schedule_cfg[\"unit\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        for ep in range(NUM_EPOCHS_FOR_RUN):\n",
        "            tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched, DEVICE) # Pass DEVICE to train_one_epoch\n",
        "            # Compute validation loss & accuracy\n",
        "            model.eval()\n",
        "            val_loss, corr, tot = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_dl:\n",
        "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                    logits = model(xb)\n",
        "                    loss = crit(logits, yb)\n",
        "                    val_loss += loss.item()\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                    corr += (preds == yb).sum().item()\n",
        "                    tot  += yb.size(0)\n",
        "            val_loss /= len(val_dl)\n",
        "            val_acc = 100.0 * corr / tot\n",
        "\n",
        "            print(f\"  Ep{ep+1}/{NUM_EPOCHS_FOR_RUN}: train_acc={tr_acc:.1f}%  train_loss={tr_loss:.4f}, \"\n",
        "                  f\"val_acc={val_acc:.1f}%, val_loss={val_loss:.4f}\")\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\":       ep + 1,\n",
        "                \"train_loss\":  tr_loss,\n",
        "                \"train_acc\":   tr_acc,\n",
        "                \"val_loss\":    val_loss,\n",
        "                \"val_acc\":     val_acc,\n",
        "                \"learning_rate\": opt.param_groups[0]['lr'] # Log current LR\n",
        "            })\n",
        "\n",
        "        wandb_run.finish()\n",
        "\n",
        "        # Only use val_acc to pick best\n",
        "        if val_acc > best_val:\n",
        "            best_val   = val_acc\n",
        "            best_cfg   = (bs, lr, wd, schedule_name)\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, WD={best_cfg[2]:.1e}, Schedule={best_cfg[3]}, val_acc={best_val:.1f}%\")\n",
        "\n",
        "    return best_cfg, best_model\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step() # <--- IMPORTANT: Step the scheduler after each batch\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0) # Accumulate weighted by batch size\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = 100 * correct_predictions / total_samples\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_mean_std(dataset, batch_size):\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=2)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        batch_samples = data.size(0)\n",
        "        data = data.view(batch_samples, data.size(1), -1)  # (B, C, H*W)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "def get_data_loaders(data_dir, batch_size):\n",
        "\n",
        "    base_tf = transforms.ToTensor()\n",
        "    ds_all = datasets.ImageFolder(root=data_dir, transform=base_tf)\n",
        "    labels = np.array(ds_all.targets)   # numpy array of shape (N,)\n",
        "    num_classes = len(ds_all.classes)\n",
        "    total_count = len(ds_all)\n",
        "    print(f\"Total samples in folder: {total_count}, classes: {ds_all.classes}\")\n",
        "\n",
        "    train_idx, val_idx, test_idx = get_split_indexes(labels, total_count)\n",
        "\n",
        "    train_subset_for_stats = Subset(ds_all, train_idx)\n",
        "    mean, std = compute_mean_std(train_subset_for_stats, batch_size)\n",
        "    print(f\"Computed mean: {mean}\")\n",
        "    print(f\"Computed std:  {std}\")\n",
        "\n",
        "    tf_final = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "    #  full ImageFolder but now with normalization baked in\n",
        "    ds_all_norm = datasets.ImageFolder(root=data_dir, transform=tf_final)\n",
        "\n",
        "    train_ds = Subset(ds_all_norm, train_idx)\n",
        "    val_ds   = Subset(ds_all_norm, val_idx)\n",
        "    test_ds  = Subset(ds_all_norm, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "    print(f\"Train/Val/Test splits: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, num_classes\n",
        "\n",
        "def get_proportion(num_classes, dataset):\n",
        "    return np.bincount(np.array(dataset.dataset.targets)[dataset.indices], minlength=num_classes) / len(dataset)\n",
        "\n",
        "def get_split_indexes(labels, total_count):\n",
        "    n_train = int(np.floor(TRAIN_FRAC * total_count))\n",
        "    n_temp = total_count - n_train   # this is val + test\n",
        "\n",
        "    sss1 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_train,\n",
        "        test_size=n_temp,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    # Train and temp(val+test) indices\n",
        "    train_idx, temp_idx = next(sss1.split(np.zeros(total_count), labels))\n",
        "\n",
        "    n_val = int(np.floor(VAL_FRAC * total_count))\n",
        "    n_test = total_count - n_train - n_val\n",
        "    assert n_temp == n_val + n_test, \"Fractions must sum to 1.\"\n",
        "\n",
        "    labels_temp = labels[temp_idx]\n",
        "\n",
        "    sss2 = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_val,\n",
        "        test_size=n_test,\n",
        "        random_state=SEED\n",
        "    )\n",
        "    val_idx_in_temp, test_idx_in_temp = next(sss2.split(np.zeros(len(temp_idx)), labels_temp))\n",
        "\n",
        "    val_idx = temp_idx[val_idx_in_temp]\n",
        "    test_idx = temp_idx[test_idx_in_temp]\n",
        "\n",
        "    assert len(train_idx) == n_train\n",
        "    assert len(val_idx) == n_val\n",
        "    assert len(test_idx) == n_test\n",
        "\n",
        "    print(f\"Stratified split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
        "    return train_idx,val_idx,test_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def build_model(n_cls, pretrained=False):\n",
        "    m = resnet50(weights=None if not pretrained else \"DEFAULT\")\n",
        "    m.fc = nn.Linear(m.fc.in_features, n_cls)\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "# def train_one_epoch(model, loader, opt, crit, sched=None):\n",
        "#     model.train()\n",
        "#     tot_loss, corr, tot = 0.0, 0, 0\n",
        "#     for xb, yb in loader:\n",
        "#         xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "#         opt.zero_grad()\n",
        "#         logits = model(xb)\n",
        "\n",
        "#         loss   = crit(logits, yb)\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#         if sched: sched.step()\n",
        "#         tot_loss += loss.item()\n",
        "#         preds    = logits.argmax(dim=1)\n",
        "#         corr    += (preds==yb).sum().item()\n",
        "#         tot     += yb.size(0)\n",
        "#         avg_loss = tot_loss / len(loader)\n",
        "\n",
        "#     avg_loss = tot_loss / len(loader)\n",
        "#     acc = 100.0 * corr / tot\n",
        "#     return avg_loss, acc\n",
        "\n",
        "def evaluate(model, loader, num_classes):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    correct_per_class = torch.zeros(num_classes, dtype=torch.int64)\n",
        "    total_per_class   = torch.zeros(num_classes, dtype=torch.int64)\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds  = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            preds  = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "            total_correct += (preds == yb).sum().item()\n",
        "            total_samples += yb.size(0)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                # mask of samples in this batch whose true label == c\n",
        "                class_mask = (yb == c)\n",
        "                if class_mask.sum().item() == 0:\n",
        "                    continue\n",
        "\n",
        "                total_per_class[c] += class_mask.sum().item()\n",
        "\n",
        "                correct_per_class[c] += ((preds == yb) & class_mask).sum().item()\n",
        "\n",
        "    overall_acc = 100.0 * total_correct / total_samples\n",
        "\n",
        "    acc_per_class = {}\n",
        "    for c in range(num_classes):\n",
        "        if total_per_class[c].item() > 0:\n",
        "            acc = 100.0 * correct_per_class[c].item() / total_per_class[c].item()\n",
        "        else:\n",
        "            acc = 0.0\n",
        "        acc_per_class[c] = acc\n",
        "\n",
        "    return overall_acc, acc_per_class, all_labels, all_preds\n",
        "\n",
        "def plot_confusion_matrix_from_preds(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # normalize by true-label counts (row‐wise) to get percentages\n",
        "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    \n",
        "    ticks = np.arange(len(class_names))\n",
        "    plt.xticks(ticks, class_names, rotation=90)\n",
        "    plt.yticks(ticks, class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    # threshold for text color\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            pct = cm_norm[i, j] * 100\n",
        "            plt.text(\n",
        "                j, i,\n",
        "                f\"{cm[i, j]}\\n{pct:.1f}%\",\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
        "            )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_class_acc_prop(te_dl, acc_vals, class_proportions_test):\n",
        "    classes = te_dl.dataset.dataset.classes\n",
        "    x = np.arange(len(classes))\n",
        "\n",
        "    acc   = acc_vals\n",
        "    prop  = class_proportions_test * 100\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "    bars = ax1.bar(x, acc, color='C0', alpha=0.7)\n",
        "    ax1.set_ylabel('Accuracy (%)', color='C0')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.tick_params(axis='y', labelcolor='C0')\n",
        "\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}%', ha='center', va='bottom', color='C0')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    line = ax2.plot(x, prop, color='C1', marker='o', linewidth=2)\n",
        "    ax2.set_ylabel('Test Proportion (%)', color='C1')\n",
        "    ax2.set_ylim(0, max(prop)*1.2)\n",
        "    ax2.tick_params(axis='y', labelcolor='C1')\n",
        "\n",
        "    for xi, yi in zip(x, prop):\n",
        "        ax2.text(xi, yi + max(prop)*0.02, f'{yi:.1f}%', ha='center', va='bottom', color='C1')\n",
        "\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
        "    plt.title('Per-class Accuracy vs. Test Proportion')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# def hyperparam_search(pretrained=False):\n",
        "#     best_val = -1.0\n",
        "#     best_cfg = None\n",
        "#     best_model = None\n",
        "\n",
        "#     for bs, (lr, wd) in product(BATCH_SIZES, GRID):\n",
        "\n",
        "#         print(f\"\\n>>> Testing BS={bs}, LR={lr:.1e}\")\n",
        "        \n",
        "#         tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "#         model = build_model(n_cls, pretrained=pretrained)\n",
        "        \n",
        "#         total_steps  = NUM_EPOCHS * len(tr_dl)\n",
        "#         warmup_steps = len(tr_dl)\n",
        "#         opt = optim.AdamW(model.parameters(), lr=lr, betas=BETAS, eps=float(EPS), weight_decay=wd)\n",
        "#         sched = SequentialLR(\n",
        "#             opt,\n",
        "#             schedulers=[\n",
        "#                 LinearLR(opt,  start_factor=1e-6, end_factor=1.0, total_iters=warmup_steps),\n",
        "#                 CosineAnnealingLR(opt, T_max=total_steps-warmup_steps)\n",
        "#             ],\n",
        "#             milestones=[warmup_steps]\n",
        "#         )\n",
        "#         crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Start a W&B run\n",
        "#         wandb_run = wandb.init(\n",
        "#             project=\"eurosat-supervised-scratch-grid-search\",\n",
        "#             name=f\"BS{bs}_LR{lr:.0e}_TR{TRAIN_FRAC}\",\n",
        "#             config={\n",
        "#                 \"batch_size\": bs,\n",
        "#                 \"learning_rate\": lr,\n",
        "#                 \"epochs\": NUM_EPOCHS,\n",
        "#                 \"pretrained\": pretrained,\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         for ep in range(NUM_EPOCHS):\n",
        "#             tr_loss, tr_acc = train_one_epoch(model, tr_dl, opt, crit, sched)\n",
        "#             # Compute validation loss & accuracy in one pass\n",
        "#             model.eval()\n",
        "#             val_loss, corr, tot = 0.0, 0, 0\n",
        "#             with torch.no_grad():\n",
        "#                 for xb, yb in val_dl:\n",
        "#                     xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "#                     logits = model(xb)\n",
        "#                     loss = crit(logits, yb)\n",
        "#                     val_loss += loss.item()\n",
        "#                     preds = logits.argmax(dim=1)\n",
        "#                     corr += (preds == yb).sum().item()\n",
        "#                     tot  += yb.size(0)\n",
        "#             val_loss /= len(val_dl)\n",
        "#             val_acc = 100.0 * corr / tot\n",
        "\n",
        "#             print(f\"  Ep{ep+1}/{NUM_EPOCHS}: train_acc={tr_acc:.1f}%  train_loss={tr_loss:.4f}, \"\n",
        "#                   f\"val_acc={val_acc:.1f}%, val_loss={val_loss:.4f}\")\n",
        "\n",
        "#             wandb.log({\n",
        "#                 \"epoch\":       ep + 1,\n",
        "#                 \"train_loss\":  tr_loss,\n",
        "#                 \"train_acc\":   tr_acc,\n",
        "#                 \"val_loss\":    val_loss,\n",
        "#                 \"val_acc\":     val_acc\n",
        "#             })\n",
        "\n",
        "#         wandb_run.finish()\n",
        "\n",
        "#         # Only use val_acc to pick best\n",
        "#         if val_acc > best_val:\n",
        "#             best_val   = val_acc\n",
        "#             best_cfg   = (bs, lr, wd)\n",
        "#             best_model = copy.deepcopy(model)\n",
        "\n",
        "#     print(f\"\\n>>> Best config: BS={best_cfg[0]}, LR={best_cfg[1]:.1e}, val_acc={best_val:.1f}%\")\n",
        "    \n",
        "#     return best_cfg, best_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform Hyperparameter Search, Retrain on Train + Validation Set, Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Assuming build_lr_scheduler and TRAINING_SCHEDULES are defined as before\n",
        "\n",
        "# Alternative (better) make_optimizer_scheduler using the helper\n",
        "def make_optimizer_scheduler_reused(params, lr, wd, schedule_name, steps_per_epoch):\n",
        "    \"\"\"\n",
        "    Builds the SGD optimizer and the specific learning rate scheduler\n",
        "    by reusing the build_lr_scheduler function.\n",
        "\n",
        "    Args:\n",
        "        params: Model parameters.\n",
        "        lr: Learning rate.\n",
        "        wd: Weight decay.\n",
        "        schedule_name: Name of the training schedule ('short', 'medium', 'long').\n",
        "        steps_per_epoch: Number of optimization steps in one epoch.\n",
        "    \"\"\"\n",
        "    opt = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "    # We need to provide a `total_training_steps` to build_lr_scheduler,\n",
        "    # though MultiStepLR doesn't strictly use it beyond its milestones.\n",
        "    # For consistency, we can pass the max step from milestones or a very large number.\n",
        "    # Let's pass the max of the milestones as an effective 'total_steps' for the schedule.\n",
        "    # The actual NUM_EPOCHS for training will be determined by the schedule logic.\n",
        "    total_steps_for_scheduler_config = max(schedule_cfg['p']) if schedule_cfg['unit'] == 'steps' else max(schedule_cfg['p']) * steps_per_epoch\n",
        "\n",
        "    scheduler = build_lr_scheduler(opt, total_steps_for_scheduler_config, schedule_cfg, steps_per_epoch)\n",
        "    return opt, scheduler\n",
        "\n",
        "\n",
        "# Assuming build_model, train_one_epoch, DEVICE, and TRAINING_SCHEDULES are defined\n",
        "\n",
        "def retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, schedule_name): # Added schedule_name\n",
        "\n",
        "    print(\"\\n>>> Retraining final model on TRAIN+VAL combined with best hyperparameters\")\n",
        "    combined_ds = ConcatDataset([tr_dl.dataset, val_dl.dataset])\n",
        "\n",
        "    combined_dl = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=4) # Assuming 4 workers\n",
        "\n",
        "    model = build_model(n_cls, pretrained=False)\n",
        "    model.to(DEVICE) # Move model to device\n",
        "\n",
        "    # Determine total epochs for this specific schedule\n",
        "    steps_per_epoch = len(combined_dl)\n",
        "    schedule_cfg = TRAINING_SCHEDULES[schedule_name]\n",
        "\n",
        "    if schedule_cfg[\"unit\"] == \"steps\":\n",
        "        total_steps_for_run = max(schedule_cfg[\"p\"]) # Run for at least the last milestone\n",
        "        num_epochs_for_run = int(np.ceil(total_steps_for_run / steps_per_epoch)) + 1 # Add buffer\n",
        "    else: # schedule_cfg[\"unit\"] == \"epochs\"\n",
        "        num_epochs_for_run = max(schedule_cfg[\"p\"]) + schedule_cfg[\"w\"]\n",
        "\n",
        "\n",
        "    # Use the new make_optimizer_scheduler\n",
        "    optimizer, scheduler = make_optimizer_scheduler_reused( # Changed function name\n",
        "        model.parameters(), lr, wd, schedule_name, steps_per_epoch\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs_for_run): # Changed num_epochs to num_epochs_for_run\n",
        "        loss, acc = train_one_epoch(model, combined_dl, optimizer, criterion, scheduler, DEVICE) # Pass DEVICE\n",
        "        print(f\"  Ep {ep+1}/{num_epochs_for_run}: train_acc={acc:.1f}%\")\n",
        "    return model, combined_ds\n",
        "\n",
        "#  Evaluate & log to wandb\n",
        "def evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr):\n",
        "    \"\"\"\n",
        "    Evaluate on test set, print per-class stats, log to wandb, and plot.\n",
        "    \"\"\"\n",
        "    final_test_acc, acc_per_class, y_true, y_pred = evaluate(final_model, te_dl, n_cls)\n",
        "    plot_confusion_matrix_from_preds(y_true, y_pred, te_dl.dataset.dataset.classes)\n",
        "\n",
        "    test_targs = np.array(te_dl.dataset.dataset.targets)[te_dl.dataset.indices]\n",
        "    prop_test = np.bincount(test_targs, minlength=n_cls) / len(test_targs)\n",
        "\n",
        "    combined_targs = np.concatenate([\n",
        "        np.array(ds.dataset.targets)[ds.indices] for ds in combined_ds.datasets\n",
        "    ])\n",
        "    prop_trainval = np.bincount(combined_targs, minlength=n_cls) / len(combined_targs)\n",
        "\n",
        "    acc_vals = np.array([acc_per_class[c] for c in range(n_cls)])\n",
        "    weighted_acc = (acc_vals * prop_test).sum()\n",
        "\n",
        "    print(\"\\n>>> Final Test Accuracy:\")\n",
        "    print(f\"  Overall:             {final_test_acc:5.1f}%\")\n",
        "    print(f\"  Weighted class acc.: {weighted_acc:5.1f}%\\n\")\n",
        "    hdr = f\"{'Class':20s}  {'Acc':>6s}   {'Train+Val':>9s}   {'Test':>6s}\"\n",
        "    print(hdr); print(\"-\"*len(hdr))\n",
        "    for c, name in enumerate(te_dl.dataset.dataset.classes):\n",
        "        print(f\"{name:20s}  {acc_vals[c]:6.1f}%   {prop_trainval[c]*100:8.0f}%   {prop_test[c]*100:6.0f}%\")\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"eurosat-supervised-scratch-final-lrsched\",\n",
        "        name=f\"BS{bs}_LR{lr:.0e}_final\",\n",
        "        config={\n",
        "            \"batch_size\": bs, \"learning_rate\": lr, \"epochs\": NUM_EPOCHS,\n",
        "            \"pretrained\": False, \"final_retrain\": True\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"final_test_acc\":     final_test_acc,\n",
        "        \"weighted_class_acc\": weighted_acc,\n",
        "        \"per_class_acc\":      acc_vals\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    plot_class_acc_prop(te_dl, acc_vals, prop_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=512, LR=1.0e-01, WD=1.0e-02, Schedule=short\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed mean: [0.3441457152366638, 0.3800985515117645, 0.40766361355781555]\n",
            "Computed std:  [0.09299741685390472, 0.06464490294456482, 0.05413917079567909]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250610_153643-nj9lyioj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/nj9lyioj' target=\"_blank\">BS512_LR1e-01_WD1e-02_Sched_short</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/nj9lyioj' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/nj9lyioj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/60: train_acc=25.8%  train_loss=2.0838, val_acc=31.2%, val_loss=33.5076\n",
            "  Ep2/60: train_acc=59.7%  train_loss=1.1405, val_acc=35.0%, val_loss=1.9872\n",
            "  Ep3/60: train_acc=71.4%  train_loss=0.8384, val_acc=21.1%, val_loss=2.8427\n",
            "  Ep4/60: train_acc=65.9%  train_loss=1.0572, val_acc=21.6%, val_loss=2.1569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep5/60: train_acc=68.7%  train_loss=0.8811, val_acc=21.5%, val_loss=2.2104\n",
            "  Ep6/60: train_acc=72.8%  train_loss=0.7819, val_acc=21.5%, val_loss=2.3453\n",
            "  Ep7/60: train_acc=71.2%  train_loss=0.8449, val_acc=36.1%, val_loss=1.6493\n",
            "  Ep8/60: train_acc=74.2%  train_loss=0.7450, val_acc=31.9%, val_loss=1.9516\n",
            "  Ep9/60: train_acc=71.8%  train_loss=0.8444, val_acc=32.7%, val_loss=1.8682\n",
            "  Ep10/60: train_acc=73.2%  train_loss=0.7777, val_acc=43.1%, val_loss=1.6699\n",
            "  Ep11/60: train_acc=73.7%  train_loss=0.7708, val_acc=32.6%, val_loss=3.5736\n",
            "  Ep12/60: train_acc=73.0%  train_loss=0.7876, val_acc=41.1%, val_loss=2.0222\n",
            "  Ep13/60: train_acc=74.5%  train_loss=0.7356, val_acc=61.5%, val_loss=1.0721\n",
            "  Ep14/60: train_acc=75.9%  train_loss=0.6984, val_acc=42.1%, val_loss=1.8844\n",
            "  Ep15/60: train_acc=78.1%  train_loss=0.6420, val_acc=52.3%, val_loss=1.3692\n",
            "  Ep16/60: train_acc=76.7%  train_loss=0.6840, val_acc=37.3%, val_loss=2.2963\n",
            "  Ep17/60: train_acc=78.1%  train_loss=0.6389, val_acc=47.8%, val_loss=1.8601\n",
            "  Ep18/60: train_acc=76.7%  train_loss=0.6810, val_acc=24.0%, val_loss=4.5506\n",
            "  Ep19/60: train_acc=79.0%  train_loss=0.6129, val_acc=49.7%, val_loss=1.4755\n",
            "  Ep20/60: train_acc=77.2%  train_loss=0.6800, val_acc=21.7%, val_loss=4.4284\n",
            "  Ep21/60: train_acc=73.0%  train_loss=0.8014, val_acc=41.8%, val_loss=1.7484\n",
            "  Ep22/60: train_acc=79.0%  train_loss=0.6130, val_acc=61.7%, val_loss=1.1741\n",
            "  Ep23/60: train_acc=85.2%  train_loss=0.4599, val_acc=83.3%, val_loss=0.5327\n",
            "  Ep24/60: train_acc=88.6%  train_loss=0.3555, val_acc=82.1%, val_loss=0.5257\n",
            "  Ep25/60: train_acc=89.5%  train_loss=0.3253, val_acc=86.1%, val_loss=0.4380\n",
            "  Ep26/60: train_acc=90.5%  train_loss=0.2986, val_acc=83.0%, val_loss=0.5155\n",
            "  Ep27/60: train_acc=90.7%  train_loss=0.2870, val_acc=80.2%, val_loss=0.6015\n",
            "  Ep28/60: train_acc=91.0%  train_loss=0.2808, val_acc=87.0%, val_loss=0.4055\n",
            "  Ep29/60: train_acc=91.7%  train_loss=0.2580, val_acc=81.9%, val_loss=0.4849\n",
            "  Ep30/60: train_acc=92.6%  train_loss=0.2390, val_acc=85.9%, val_loss=0.4070\n",
            "  Ep31/60: train_acc=92.1%  train_loss=0.2465, val_acc=81.2%, val_loss=0.6644\n",
            "  Ep32/60: train_acc=92.9%  train_loss=0.2272, val_acc=79.6%, val_loss=0.7162\n",
            "  Ep33/60: train_acc=93.0%  train_loss=0.2213, val_acc=64.6%, val_loss=1.3797\n",
            "  Ep34/60: train_acc=93.0%  train_loss=0.2216, val_acc=79.2%, val_loss=0.6419\n",
            "  Ep35/60: train_acc=93.3%  train_loss=0.2159, val_acc=80.6%, val_loss=0.5829\n",
            "  Ep36/60: train_acc=93.6%  train_loss=0.2083, val_acc=87.0%, val_loss=0.3876\n",
            "  Ep37/60: train_acc=93.9%  train_loss=0.1998, val_acc=84.2%, val_loss=0.4954\n",
            "  Ep38/60: train_acc=94.3%  train_loss=0.1854, val_acc=87.7%, val_loss=0.3627\n",
            "  Ep39/60: train_acc=94.5%  train_loss=0.1846, val_acc=69.7%, val_loss=1.2088\n",
            "  Ep40/60: train_acc=94.5%  train_loss=0.1825, val_acc=91.9%, val_loss=0.2568\n",
            "  Ep41/60: train_acc=97.2%  train_loss=0.1157, val_acc=92.5%, val_loss=0.2444\n",
            "  Ep42/60: train_acc=98.0%  train_loss=0.0988, val_acc=92.9%, val_loss=0.2336\n",
            "  Ep43/60: train_acc=98.0%  train_loss=0.0927, val_acc=92.6%, val_loss=0.2271\n",
            "  Ep44/60: train_acc=98.4%  train_loss=0.0866, val_acc=92.7%, val_loss=0.2358\n",
            "  Ep45/60: train_acc=98.2%  train_loss=0.0884, val_acc=92.5%, val_loss=0.2406\n",
            "  Ep46/60: train_acc=98.7%  train_loss=0.0807, val_acc=93.0%, val_loss=0.2255\n",
            "  Ep47/60: train_acc=98.7%  train_loss=0.0769, val_acc=93.1%, val_loss=0.2218\n",
            "  Ep48/60: train_acc=98.9%  train_loss=0.0738, val_acc=92.7%, val_loss=0.2369\n",
            "  Ep49/60: train_acc=98.9%  train_loss=0.0691, val_acc=92.3%, val_loss=0.2447\n",
            "  Ep50/60: train_acc=98.9%  train_loss=0.0693, val_acc=92.8%, val_loss=0.2336\n",
            "  Ep51/60: train_acc=99.1%  train_loss=0.0667, val_acc=93.0%, val_loss=0.2273\n",
            "  Ep52/60: train_acc=99.1%  train_loss=0.0646, val_acc=92.8%, val_loss=0.2278\n",
            "  Ep53/60: train_acc=99.1%  train_loss=0.0633, val_acc=91.8%, val_loss=0.2758\n",
            "  Ep54/60: train_acc=99.1%  train_loss=0.0622, val_acc=92.9%, val_loss=0.2318\n",
            "  Ep55/60: train_acc=99.3%  train_loss=0.0575, val_acc=93.3%, val_loss=0.2349\n",
            "  Ep56/60: train_acc=99.3%  train_loss=0.0561, val_acc=92.6%, val_loss=0.2376\n",
            "  Ep57/60: train_acc=99.3%  train_loss=0.0557, val_acc=93.2%, val_loss=0.2333\n",
            "  Ep58/60: train_acc=99.6%  train_loss=0.0489, val_acc=93.2%, val_loss=0.2266\n",
            "  Ep59/60: train_acc=99.6%  train_loss=0.0486, val_acc=93.1%, val_loss=0.2274\n",
            "  Ep60/60: train_acc=99.6%  train_loss=0.0492, val_acc=93.0%, val_loss=0.2249\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>▂▄▆▇███████████▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▆▅▆▆▆▆▆▆▅▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_loss</td><td>█▆▆▅▆▆▅▅▅▅▅▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▁▁▁▃▂▃▅▃▃▁▄▁▃▇▇▇▇▇▇▅▇▇▇███████████████</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_acc</td><td>99.59259</td></tr><tr><td>train_loss</td><td>0.04921</td></tr><tr><td>val_acc</td><td>93.03704</td></tr><tr><td>val_loss</td><td>0.22493</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS512_LR1e-01_WD1e-02_Sched_short</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/nj9lyioj' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/nj9lyioj</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250610_153643-nj9lyioj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=512, LR=1.0e-01, WD=1.0e-02, Schedule=medium\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441457152366638, 0.3800985515117645, 0.40766361355781555]\n",
            "Computed std:  [0.09299741685390472, 0.06464490294456482, 0.05413917079567909]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250610_162034-qc6p92ll</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/qc6p92ll' target=\"_blank\">BS512_LR1e-01_WD1e-02_Sched_medium</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/qc6p92ll' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/qc6p92ll</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/234: train_acc=22.7%  train_loss=2.0879, val_acc=21.2%, val_loss=2.8882\n",
            "  Ep2/234: train_acc=48.1%  train_loss=1.3973, val_acc=52.1%, val_loss=1.6670\n",
            "  Ep3/234: train_acc=68.0%  train_loss=0.9018, val_acc=37.7%, val_loss=1.8779\n",
            "  Ep4/234: train_acc=73.7%  train_loss=0.7480, val_acc=23.4%, val_loss=3.5837\n",
            "  Ep5/234: train_acc=75.7%  train_loss=0.7064, val_acc=29.6%, val_loss=2.2233\n",
            "  Ep6/234: train_acc=77.9%  train_loss=0.6502, val_acc=28.7%, val_loss=2.1823\n",
            "  Ep7/234: train_acc=76.4%  train_loss=0.6760, val_acc=23.7%, val_loss=2.3774\n",
            "  Ep8/234: train_acc=78.7%  train_loss=0.6253, val_acc=24.6%, val_loss=2.3478\n",
            "  Ep9/234: train_acc=77.4%  train_loss=0.6782, val_acc=26.4%, val_loss=2.1369\n",
            "  Ep10/234: train_acc=76.2%  train_loss=0.7009, val_acc=29.0%, val_loss=2.2424\n",
            "  Ep11/234: train_acc=78.3%  train_loss=0.6517, val_acc=30.5%, val_loss=1.9619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep12/234: train_acc=76.6%  train_loss=0.6944, val_acc=24.0%, val_loss=2.7840\n",
            "  Ep13/234: train_acc=76.8%  train_loss=0.6880, val_acc=35.0%, val_loss=1.9749\n",
            "  Ep14/234: train_acc=76.5%  train_loss=0.6940, val_acc=44.4%, val_loss=1.7644\n",
            "  Ep15/234: train_acc=80.3%  train_loss=0.5831, val_acc=36.7%, val_loss=2.0977\n",
            "  Ep16/234: train_acc=75.9%  train_loss=0.7209, val_acc=45.4%, val_loss=1.8836\n",
            "  Ep17/234: train_acc=79.5%  train_loss=0.6056, val_acc=46.4%, val_loss=1.8234\n",
            "  Ep18/234: train_acc=77.0%  train_loss=0.6780, val_acc=36.5%, val_loss=2.7140\n",
            "  Ep19/234: train_acc=79.2%  train_loss=0.6252, val_acc=61.3%, val_loss=1.1872\n",
            "  Ep20/234: train_acc=79.9%  train_loss=0.5995, val_acc=32.3%, val_loss=2.9145\n",
            "  Ep21/234: train_acc=79.4%  train_loss=0.6238, val_acc=54.5%, val_loss=1.4266\n",
            "  Ep22/234: train_acc=81.1%  train_loss=0.5712, val_acc=35.5%, val_loss=1.9005\n",
            "  Ep23/234: train_acc=74.6%  train_loss=0.7605, val_acc=66.4%, val_loss=0.9400\n",
            "  Ep24/234: train_acc=80.7%  train_loss=0.5727, val_acc=37.5%, val_loss=2.3330\n",
            "  Ep25/234: train_acc=76.5%  train_loss=0.7055, val_acc=62.3%, val_loss=1.0879\n",
            "  Ep26/234: train_acc=80.8%  train_loss=0.5599, val_acc=36.1%, val_loss=2.9841\n",
            "  Ep27/234: train_acc=76.5%  train_loss=0.7108, val_acc=40.9%, val_loss=2.1069\n",
            "  Ep28/234: train_acc=82.0%  train_loss=0.5405, val_acc=38.4%, val_loss=2.4221\n",
            "  Ep29/234: train_acc=78.5%  train_loss=0.6483, val_acc=32.6%, val_loss=2.3548\n",
            "  Ep30/234: train_acc=76.9%  train_loss=0.6772, val_acc=49.8%, val_loss=1.3705\n",
            "  Ep31/234: train_acc=77.0%  train_loss=0.6962, val_acc=61.8%, val_loss=1.2555\n",
            "  Ep32/234: train_acc=79.6%  train_loss=0.6034, val_acc=62.4%, val_loss=1.0510\n",
            "  Ep33/234: train_acc=80.2%  train_loss=0.5879, val_acc=49.6%, val_loss=1.6209\n",
            "  Ep34/234: train_acc=79.3%  train_loss=0.6130, val_acc=52.9%, val_loss=1.7435\n",
            "  Ep35/234: train_acc=79.2%  train_loss=0.6233, val_acc=44.5%, val_loss=1.8594\n",
            "  Ep36/234: train_acc=78.2%  train_loss=0.6504, val_acc=38.4%, val_loss=2.4324\n",
            "  Ep37/234: train_acc=78.6%  train_loss=0.6381, val_acc=52.6%, val_loss=1.6563\n",
            "  Ep38/234: train_acc=76.2%  train_loss=0.7068, val_acc=49.5%, val_loss=1.6182\n",
            "  Ep39/234: train_acc=79.1%  train_loss=0.6148, val_acc=58.0%, val_loss=1.1508\n",
            "  Ep40/234: train_acc=76.5%  train_loss=0.7245, val_acc=14.4%, val_loss=15.8022\n",
            "  Ep41/234: train_acc=69.6%  train_loss=0.8754, val_acc=57.6%, val_loss=1.1684\n",
            "  Ep42/234: train_acc=77.8%  train_loss=0.6520, val_acc=40.7%, val_loss=1.9604\n",
            "  Ep43/234: train_acc=74.8%  train_loss=0.7402, val_acc=39.6%, val_loss=2.3641\n",
            "  Ep44/234: train_acc=79.6%  train_loss=0.6017, val_acc=47.7%, val_loss=1.8779\n",
            "  Ep45/234: train_acc=60.6%  train_loss=1.1465, val_acc=54.9%, val_loss=1.3149\n",
            "  Ep46/234: train_acc=73.2%  train_loss=0.7725, val_acc=22.0%, val_loss=3.4101\n",
            "  Ep47/234: train_acc=68.3%  train_loss=0.9114, val_acc=41.5%, val_loss=1.9969\n",
            "  Ep48/234: train_acc=71.7%  train_loss=0.8077, val_acc=40.1%, val_loss=1.7381\n",
            "  Ep49/234: train_acc=70.4%  train_loss=0.8506, val_acc=38.0%, val_loss=2.4071\n",
            "  Ep50/234: train_acc=72.1%  train_loss=0.7985, val_acc=38.3%, val_loss=2.5345\n",
            "  Ep51/234: train_acc=68.7%  train_loss=0.9238, val_acc=32.5%, val_loss=2.4468\n",
            "  Ep52/234: train_acc=76.4%  train_loss=0.6870, val_acc=38.1%, val_loss=2.0976\n",
            "  Ep53/234: train_acc=73.1%  train_loss=0.7823, val_acc=17.2%, val_loss=6.4969\n",
            "  Ep54/234: train_acc=69.6%  train_loss=0.8729, val_acc=33.6%, val_loss=3.1897\n",
            "  Ep55/234: train_acc=75.5%  train_loss=0.7117, val_acc=57.6%, val_loss=1.2068\n",
            "  Ep56/234: train_acc=76.9%  train_loss=0.6767, val_acc=61.1%, val_loss=1.0788\n",
            "  Ep57/234: train_acc=77.9%  train_loss=0.6442, val_acc=45.0%, val_loss=2.6029\n",
            "  Ep58/234: train_acc=68.0%  train_loss=0.9054, val_acc=34.8%, val_loss=2.5819\n",
            "  Ep59/234: train_acc=72.7%  train_loss=0.7727, val_acc=55.8%, val_loss=1.1628\n",
            "  Ep60/234: train_acc=73.7%  train_loss=0.7605, val_acc=42.7%, val_loss=1.8533\n",
            "  Ep61/234: train_acc=73.2%  train_loss=0.7775, val_acc=33.6%, val_loss=2.5852\n",
            "  Ep62/234: train_acc=71.5%  train_loss=0.8265, val_acc=36.5%, val_loss=1.9558\n",
            "  Ep63/234: train_acc=70.8%  train_loss=0.8401, val_acc=55.7%, val_loss=1.2708\n",
            "  Ep64/234: train_acc=73.9%  train_loss=0.7429, val_acc=28.6%, val_loss=2.3849\n",
            "  Ep65/234: train_acc=74.8%  train_loss=0.7271, val_acc=43.7%, val_loss=1.7473\n",
            "  Ep66/234: train_acc=72.3%  train_loss=0.8043, val_acc=42.7%, val_loss=1.7752\n",
            "  Ep67/234: train_acc=71.2%  train_loss=0.8142, val_acc=45.6%, val_loss=1.4690\n",
            "  Ep68/234: train_acc=74.3%  train_loss=0.7384, val_acc=30.1%, val_loss=2.9308\n",
            "  Ep69/234: train_acc=71.8%  train_loss=0.8091, val_acc=42.7%, val_loss=1.6909\n",
            "  Ep70/234: train_acc=77.6%  train_loss=0.6557, val_acc=47.6%, val_loss=1.6347\n",
            "  Ep71/234: train_acc=69.4%  train_loss=0.8666, val_acc=27.2%, val_loss=6.0886\n",
            "  Ep72/234: train_acc=64.9%  train_loss=0.9918, val_acc=32.4%, val_loss=2.5156\n",
            "  Ep73/234: train_acc=71.0%  train_loss=0.8308, val_acc=32.4%, val_loss=3.3520\n",
            "  Ep74/234: train_acc=72.2%  train_loss=0.7939, val_acc=46.3%, val_loss=1.7172\n",
            "  Ep75/234: train_acc=72.0%  train_loss=0.8112, val_acc=48.2%, val_loss=1.5585\n",
            "  Ep76/234: train_acc=72.3%  train_loss=0.7988, val_acc=58.6%, val_loss=1.2417\n",
            "  Ep77/234: train_acc=73.6%  train_loss=0.7554, val_acc=31.6%, val_loss=3.2894\n",
            "  Ep78/234: train_acc=72.5%  train_loss=0.7888, val_acc=54.1%, val_loss=1.4674\n",
            "  Ep79/234: train_acc=73.6%  train_loss=0.7698, val_acc=37.9%, val_loss=2.2798\n",
            "  Ep80/234: train_acc=73.4%  train_loss=0.7672, val_acc=55.4%, val_loss=1.1827\n",
            "  Ep81/234: train_acc=74.2%  train_loss=0.7418, val_acc=47.1%, val_loss=1.8989\n",
            "  Ep82/234: train_acc=78.2%  train_loss=0.6426, val_acc=73.8%, val_loss=0.7326\n",
            "  Ep83/234: train_acc=83.5%  train_loss=0.4839, val_acc=80.1%, val_loss=0.5742\n",
            "  Ep84/234: train_acc=85.5%  train_loss=0.4300, val_acc=81.6%, val_loss=0.5483\n",
            "  Ep85/234: train_acc=86.5%  train_loss=0.4061, val_acc=76.4%, val_loss=0.7510\n",
            "  Ep86/234: train_acc=87.3%  train_loss=0.3848, val_acc=72.9%, val_loss=0.9958\n",
            "  Ep87/234: train_acc=87.3%  train_loss=0.3782, val_acc=73.8%, val_loss=0.8265\n",
            "  Ep88/234: train_acc=88.3%  train_loss=0.3523, val_acc=74.1%, val_loss=0.7716\n",
            "  Ep89/234: train_acc=88.0%  train_loss=0.3559, val_acc=81.5%, val_loss=0.5226\n",
            "  Ep90/234: train_acc=88.6%  train_loss=0.3465, val_acc=76.6%, val_loss=0.6673\n",
            "  Ep91/234: train_acc=89.4%  train_loss=0.3198, val_acc=76.2%, val_loss=0.6970\n",
            "  Ep92/234: train_acc=89.5%  train_loss=0.3155, val_acc=84.0%, val_loss=0.4633\n",
            "  Ep93/234: train_acc=90.1%  train_loss=0.3061, val_acc=75.1%, val_loss=0.8226\n",
            "  Ep94/234: train_acc=90.1%  train_loss=0.3029, val_acc=76.6%, val_loss=0.7529\n",
            "  Ep95/234: train_acc=90.2%  train_loss=0.3034, val_acc=69.7%, val_loss=1.2549\n",
            "  Ep96/234: train_acc=90.0%  train_loss=0.3057, val_acc=84.3%, val_loss=0.4773\n",
            "  Ep97/234: train_acc=91.3%  train_loss=0.2691, val_acc=84.7%, val_loss=0.4783\n",
            "  Ep98/234: train_acc=90.9%  train_loss=0.2724, val_acc=69.5%, val_loss=1.1510\n",
            "  Ep99/234: train_acc=91.6%  train_loss=0.2616, val_acc=74.3%, val_loss=0.8758\n",
            "  Ep100/234: train_acc=91.2%  train_loss=0.2727, val_acc=67.2%, val_loss=1.2738\n",
            "  Ep101/234: train_acc=91.4%  train_loss=0.2650, val_acc=73.4%, val_loss=1.0396\n",
            "  Ep102/234: train_acc=91.0%  train_loss=0.2740, val_acc=65.1%, val_loss=1.2529\n",
            "  Ep103/234: train_acc=90.5%  train_loss=0.2888, val_acc=80.3%, val_loss=0.6589\n",
            "  Ep104/234: train_acc=93.0%  train_loss=0.2253, val_acc=78.2%, val_loss=0.7717\n",
            "  Ep105/234: train_acc=91.2%  train_loss=0.2772, val_acc=72.2%, val_loss=1.3166\n",
            "  Ep106/234: train_acc=92.0%  train_loss=0.2462, val_acc=76.2%, val_loss=0.6682\n",
            "  Ep107/234: train_acc=92.9%  train_loss=0.2216, val_acc=84.6%, val_loss=0.4612\n",
            "  Ep108/234: train_acc=93.8%  train_loss=0.1999, val_acc=79.0%, val_loss=0.7817\n",
            "  Ep109/234: train_acc=93.4%  train_loss=0.2143, val_acc=69.8%, val_loss=1.1061\n",
            "  Ep110/234: train_acc=93.6%  train_loss=0.2087, val_acc=80.5%, val_loss=0.6073\n",
            "  Ep111/234: train_acc=93.4%  train_loss=0.2067, val_acc=64.6%, val_loss=1.4723\n",
            "  Ep112/234: train_acc=93.3%  train_loss=0.2069, val_acc=83.2%, val_loss=0.4955\n",
            "  Ep113/234: train_acc=94.0%  train_loss=0.1988, val_acc=82.6%, val_loss=0.5444\n",
            "  Ep114/234: train_acc=93.5%  train_loss=0.2060, val_acc=72.4%, val_loss=1.2010\n",
            "  Ep115/234: train_acc=94.6%  train_loss=0.1758, val_acc=77.3%, val_loss=0.8054\n",
            "  Ep116/234: train_acc=94.4%  train_loss=0.1862, val_acc=76.6%, val_loss=0.8493\n",
            "  Ep117/234: train_acc=93.3%  train_loss=0.2120, val_acc=70.0%, val_loss=1.1189\n",
            "  Ep118/234: train_acc=94.5%  train_loss=0.1808, val_acc=79.5%, val_loss=0.6917\n",
            "  Ep119/234: train_acc=94.1%  train_loss=0.1925, val_acc=79.3%, val_loss=0.6689\n",
            "  Ep120/234: train_acc=94.4%  train_loss=0.1795, val_acc=81.0%, val_loss=0.6561\n",
            "  Ep121/234: train_acc=96.0%  train_loss=0.1425, val_acc=63.4%, val_loss=1.7558\n",
            "  Ep122/234: train_acc=95.1%  train_loss=0.1657, val_acc=84.9%, val_loss=0.4953\n",
            "  Ep123/234: train_acc=95.2%  train_loss=0.1612, val_acc=83.4%, val_loss=0.5424\n",
            "  Ep124/234: train_acc=94.7%  train_loss=0.1699, val_acc=81.0%, val_loss=0.6362\n",
            "  Ep125/234: train_acc=95.8%  train_loss=0.1433, val_acc=72.3%, val_loss=1.0464\n",
            "  Ep126/234: train_acc=96.5%  train_loss=0.1261, val_acc=73.3%, val_loss=0.9457\n",
            "  Ep127/234: train_acc=95.0%  train_loss=0.1621, val_acc=74.7%, val_loss=0.9227\n",
            "  Ep128/234: train_acc=93.9%  train_loss=0.1948, val_acc=75.7%, val_loss=0.7996\n",
            "  Ep129/234: train_acc=95.9%  train_loss=0.1418, val_acc=85.3%, val_loss=0.4766\n",
            "  Ep130/234: train_acc=95.3%  train_loss=0.1585, val_acc=75.1%, val_loss=0.9645\n",
            "  Ep131/234: train_acc=96.0%  train_loss=0.1418, val_acc=75.1%, val_loss=0.8811\n",
            "  Ep132/234: train_acc=94.7%  train_loss=0.1706, val_acc=67.0%, val_loss=1.1323\n",
            "  Ep133/234: train_acc=94.0%  train_loss=0.1961, val_acc=74.9%, val_loss=1.0094\n",
            "  Ep134/234: train_acc=96.2%  train_loss=0.1356, val_acc=70.6%, val_loss=1.2441\n",
            "  Ep135/234: train_acc=97.0%  train_loss=0.1143, val_acc=72.0%, val_loss=0.9963\n",
            "  Ep136/234: train_acc=96.7%  train_loss=0.1171, val_acc=71.7%, val_loss=1.0548\n",
            "  Ep137/234: train_acc=95.9%  train_loss=0.1398, val_acc=83.8%, val_loss=0.5405\n",
            "  Ep138/234: train_acc=96.0%  train_loss=0.1376, val_acc=69.7%, val_loss=1.3192\n",
            "  Ep139/234: train_acc=96.8%  train_loss=0.1170, val_acc=75.0%, val_loss=0.9621\n",
            "  Ep140/234: train_acc=94.9%  train_loss=0.1654, val_acc=81.0%, val_loss=0.6572\n",
            "  Ep141/234: train_acc=95.7%  train_loss=0.1443, val_acc=79.9%, val_loss=0.7475\n",
            "  Ep142/234: train_acc=97.3%  train_loss=0.1049, val_acc=63.4%, val_loss=1.4832\n",
            "  Ep143/234: train_acc=95.1%  train_loss=0.1621, val_acc=63.6%, val_loss=1.4556\n",
            "  Ep144/234: train_acc=95.9%  train_loss=0.1440, val_acc=75.3%, val_loss=0.8930\n",
            "  Ep145/234: train_acc=97.1%  train_loss=0.1094, val_acc=82.4%, val_loss=0.5365\n",
            "  Ep146/234: train_acc=96.4%  train_loss=0.1290, val_acc=75.9%, val_loss=0.9471\n",
            "  Ep147/234: train_acc=95.0%  train_loss=0.1645, val_acc=71.7%, val_loss=1.0840\n",
            "  Ep148/234: train_acc=96.1%  train_loss=0.1341, val_acc=61.6%, val_loss=1.7079\n",
            "  Ep149/234: train_acc=95.7%  train_loss=0.1486, val_acc=85.5%, val_loss=0.5038\n",
            "  Ep150/234: train_acc=97.1%  train_loss=0.1097, val_acc=79.1%, val_loss=0.7272\n",
            "  Ep151/234: train_acc=97.6%  train_loss=0.0911, val_acc=75.3%, val_loss=0.8635\n",
            "  Ep152/234: train_acc=98.5%  train_loss=0.0704, val_acc=90.0%, val_loss=0.3420\n",
            "  Ep153/234: train_acc=99.7%  train_loss=0.0364, val_acc=90.4%, val_loss=0.3345\n",
            "  Ep154/234: train_acc=99.8%  train_loss=0.0317, val_acc=90.5%, val_loss=0.3405\n",
            "  Ep155/234: train_acc=99.7%  train_loss=0.0328, val_acc=90.7%, val_loss=0.3346\n",
            "  Ep156/234: train_acc=99.8%  train_loss=0.0313, val_acc=90.0%, val_loss=0.3573\n",
            "  Ep157/234: train_acc=99.9%  train_loss=0.0291, val_acc=90.4%, val_loss=0.3393\n",
            "  Ep158/234: train_acc=99.9%  train_loss=0.0277, val_acc=89.7%, val_loss=0.3480\n",
            "  Ep159/234: train_acc=99.9%  train_loss=0.0279, val_acc=90.4%, val_loss=0.3431\n",
            "  Ep160/234: train_acc=99.9%  train_loss=0.0263, val_acc=90.3%, val_loss=0.3406\n",
            "  Ep161/234: train_acc=99.9%  train_loss=0.0278, val_acc=90.0%, val_loss=0.3535\n",
            "  Ep162/234: train_acc=99.8%  train_loss=0.0296, val_acc=89.9%, val_loss=0.3585\n",
            "  Ep163/234: train_acc=99.9%  train_loss=0.0264, val_acc=90.4%, val_loss=0.3447\n",
            "  Ep164/234: train_acc=99.9%  train_loss=0.0251, val_acc=88.7%, val_loss=0.3977\n",
            "  Ep165/234: train_acc=99.9%  train_loss=0.0251, val_acc=90.3%, val_loss=0.3444\n",
            "  Ep166/234: train_acc=99.9%  train_loss=0.0246, val_acc=90.4%, val_loss=0.3417\n",
            "  Ep167/234: train_acc=99.9%  train_loss=0.0261, val_acc=90.0%, val_loss=0.3547\n",
            "  Ep168/234: train_acc=99.9%  train_loss=0.0251, val_acc=89.1%, val_loss=0.3839\n",
            "  Ep169/234: train_acc=99.9%  train_loss=0.0248, val_acc=90.2%, val_loss=0.3404\n",
            "  Ep170/234: train_acc=100.0%  train_loss=0.0241, val_acc=90.4%, val_loss=0.3404\n",
            "  Ep171/234: train_acc=100.0%  train_loss=0.0237, val_acc=90.6%, val_loss=0.3442\n",
            "  Ep172/234: train_acc=99.9%  train_loss=0.0241, val_acc=90.4%, val_loss=0.3392\n",
            "  Ep173/234: train_acc=99.9%  train_loss=0.0250, val_acc=90.7%, val_loss=0.3393\n",
            "  Ep174/234: train_acc=99.9%  train_loss=0.0243, val_acc=90.3%, val_loss=0.3473\n",
            "  Ep175/234: train_acc=99.9%  train_loss=0.0245, val_acc=90.3%, val_loss=0.3505\n",
            "  Ep176/234: train_acc=100.0%  train_loss=0.0247, val_acc=90.6%, val_loss=0.3361\n",
            "  Ep177/234: train_acc=99.9%  train_loss=0.0248, val_acc=90.0%, val_loss=0.3433\n",
            "  Ep178/234: train_acc=100.0%  train_loss=0.0241, val_acc=90.4%, val_loss=0.3405\n",
            "  Ep179/234: train_acc=100.0%  train_loss=0.0243, val_acc=90.6%, val_loss=0.3377\n",
            "  Ep180/234: train_acc=100.0%  train_loss=0.0244, val_acc=90.1%, val_loss=0.3425\n",
            "  Ep181/234: train_acc=99.9%  train_loss=0.0240, val_acc=90.6%, val_loss=0.3390\n",
            "  Ep182/234: train_acc=99.9%  train_loss=0.0252, val_acc=90.4%, val_loss=0.3380\n",
            "  Ep183/234: train_acc=100.0%  train_loss=0.0243, val_acc=90.1%, val_loss=0.3479\n",
            "  Ep184/234: train_acc=100.0%  train_loss=0.0240, val_acc=90.5%, val_loss=0.3388\n",
            "  Ep185/234: train_acc=100.0%  train_loss=0.0235, val_acc=90.1%, val_loss=0.3384\n",
            "  Ep186/234: train_acc=100.0%  train_loss=0.0238, val_acc=90.5%, val_loss=0.3322\n",
            "  Ep187/234: train_acc=99.9%  train_loss=0.0232, val_acc=90.8%, val_loss=0.3333\n",
            "  Ep188/234: train_acc=99.9%  train_loss=0.0236, val_acc=90.5%, val_loss=0.3411\n",
            "  Ep189/234: train_acc=100.0%  train_loss=0.0243, val_acc=90.6%, val_loss=0.3383\n",
            "  Ep190/234: train_acc=100.0%  train_loss=0.0234, val_acc=90.4%, val_loss=0.3413\n",
            "  Ep191/234: train_acc=100.0%  train_loss=0.0230, val_acc=90.4%, val_loss=0.3397\n",
            "  Ep192/234: train_acc=99.9%  train_loss=0.0235, val_acc=90.6%, val_loss=0.3373\n",
            "  Ep193/234: train_acc=100.0%  train_loss=0.0239, val_acc=90.4%, val_loss=0.3411\n",
            "  Ep194/234: train_acc=100.0%  train_loss=0.0235, val_acc=91.0%, val_loss=0.3309\n",
            "  Ep195/234: train_acc=99.9%  train_loss=0.0237, val_acc=91.0%, val_loss=0.3331\n",
            "  Ep196/234: train_acc=99.9%  train_loss=0.0247, val_acc=88.5%, val_loss=0.3974\n",
            "  Ep197/234: train_acc=100.0%  train_loss=0.0241, val_acc=90.2%, val_loss=0.3617\n",
            "  Ep198/234: train_acc=100.0%  train_loss=0.0240, val_acc=90.7%, val_loss=0.3415\n",
            "  Ep199/234: train_acc=100.0%  train_loss=0.0232, val_acc=90.2%, val_loss=0.3462\n",
            "  Ep200/234: train_acc=100.0%  train_loss=0.0241, val_acc=89.7%, val_loss=0.3657\n",
            "  Ep201/234: train_acc=99.9%  train_loss=0.0246, val_acc=90.4%, val_loss=0.3502\n",
            "  Ep202/234: train_acc=100.0%  train_loss=0.0235, val_acc=90.5%, val_loss=0.3412\n",
            "  Ep203/234: train_acc=99.9%  train_loss=0.0246, val_acc=90.8%, val_loss=0.3380\n",
            "  Ep204/234: train_acc=100.0%  train_loss=0.0231, val_acc=90.2%, val_loss=0.3481\n",
            "  Ep205/234: train_acc=99.9%  train_loss=0.0244, val_acc=90.9%, val_loss=0.3385\n",
            "  Ep206/234: train_acc=99.9%  train_loss=0.0251, val_acc=90.2%, val_loss=0.3490\n",
            "  Ep207/234: train_acc=99.9%  train_loss=0.0263, val_acc=90.7%, val_loss=0.3375\n",
            "  Ep208/234: train_acc=100.0%  train_loss=0.0236, val_acc=90.8%, val_loss=0.3381\n",
            "  Ep209/234: train_acc=100.0%  train_loss=0.0226, val_acc=90.7%, val_loss=0.3373\n",
            "  Ep210/234: train_acc=100.0%  train_loss=0.0234, val_acc=90.5%, val_loss=0.3393\n",
            "  Ep211/234: train_acc=100.0%  train_loss=0.0236, val_acc=89.1%, val_loss=0.3860\n",
            "  Ep212/234: train_acc=100.0%  train_loss=0.0224, val_acc=90.9%, val_loss=0.3371\n",
            "  Ep213/234: train_acc=100.0%  train_loss=0.0226, val_acc=90.8%, val_loss=0.3413\n",
            "  Ep214/234: train_acc=100.0%  train_loss=0.0233, val_acc=90.7%, val_loss=0.3310\n",
            "  Ep215/234: train_acc=100.0%  train_loss=0.0232, val_acc=90.4%, val_loss=0.3433\n",
            "  Ep216/234: train_acc=100.0%  train_loss=0.0238, val_acc=90.9%, val_loss=0.3324\n",
            "  Ep217/234: train_acc=100.0%  train_loss=0.0243, val_acc=91.0%, val_loss=0.3314\n",
            "  Ep218/234: train_acc=99.9%  train_loss=0.0247, val_acc=89.7%, val_loss=0.3710\n",
            "  Ep219/234: train_acc=99.9%  train_loss=0.0263, val_acc=90.6%, val_loss=0.3421\n",
            "  Ep220/234: train_acc=100.0%  train_loss=0.0233, val_acc=90.6%, val_loss=0.3375\n",
            "  Ep221/234: train_acc=100.0%  train_loss=0.0230, val_acc=90.4%, val_loss=0.3493\n",
            "  Ep222/234: train_acc=100.0%  train_loss=0.0227, val_acc=90.6%, val_loss=0.3336\n",
            "  Ep223/234: train_acc=100.0%  train_loss=0.0225, val_acc=91.0%, val_loss=0.3328\n",
            "  Ep224/234: train_acc=100.0%  train_loss=0.0235, val_acc=90.6%, val_loss=0.3391\n",
            "  Ep225/234: train_acc=100.0%  train_loss=0.0229, val_acc=91.0%, val_loss=0.3319\n",
            "  Ep226/234: train_acc=100.0%  train_loss=0.0224, val_acc=90.8%, val_loss=0.3358\n",
            "  Ep227/234: train_acc=100.0%  train_loss=0.0226, val_acc=91.0%, val_loss=0.3314\n",
            "  Ep228/234: train_acc=100.0%  train_loss=0.0225, val_acc=91.0%, val_loss=0.3333\n",
            "  Ep229/234: train_acc=100.0%  train_loss=0.0227, val_acc=91.1%, val_loss=0.3300\n",
            "  Ep230/234: train_acc=100.0%  train_loss=0.0229, val_acc=91.0%, val_loss=0.3281\n",
            "  Ep231/234: train_acc=100.0%  train_loss=0.0239, val_acc=90.7%, val_loss=0.3366\n",
            "  Ep232/234: train_acc=100.0%  train_loss=0.0232, val_acc=90.8%, val_loss=0.3327\n",
            "  Ep233/234: train_acc=100.0%  train_loss=0.0230, val_acc=90.6%, val_loss=0.3346\n",
            "  Ep234/234: train_acc=100.0%  train_loss=0.0231, val_acc=91.0%, val_loss=0.3301\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▆██████████████▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▄▄▅▅▄▄▁▃▃▃▃▃▄▆▆▆▆▆▇▇▇▇▇████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▅▄▅▅▄▄▅▅▄▅▅▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▄▂▄▅▄▃▄▃▃▄▄▇▆▇▇▆▇▅▆▆▆▅▇███████████████</td></tr><tr><td>val_loss</td><td>▂▂▂▂▁▁▂█▁▁▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>234</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_acc</td><td>99.98148</td></tr><tr><td>train_loss</td><td>0.02307</td></tr><tr><td>val_acc</td><td>91</td></tr><tr><td>val_loss</td><td>0.33006</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS512_LR1e-01_WD1e-02_Sched_medium</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/qc6p92ll' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/qc6p92ll</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250610_162034-qc6p92ll/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=512, LR=1.0e-01, WD=1.0e-02, Schedule=long\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441457152366638, 0.3800985515117645, 0.40766361355781555]\n",
            "Computed std:  [0.09299741685390472, 0.06464490294456482, 0.05413917079567909]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250610_173424-g0gxas63</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/g0gxas63' target=\"_blank\">BS512_LR1e-01_WD1e-02_Sched_long</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/g0gxas63' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/g0gxas63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/95: train_acc=26.0%  train_loss=2.0657, val_acc=26.5%, val_loss=5.8963\n",
            "  Ep2/95: train_acc=57.2%  train_loss=1.2006, val_acc=39.0%, val_loss=2.0896\n",
            "  Ep3/95: train_acc=70.8%  train_loss=0.8438, val_acc=17.4%, val_loss=3.1725\n",
            "  Ep4/95: train_acc=56.3%  train_loss=1.4307, val_acc=16.4%, val_loss=2.9891\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep5/95: train_acc=69.4%  train_loss=0.8638, val_acc=21.5%, val_loss=2.6482\n",
            "  Ep6/95: train_acc=69.0%  train_loss=0.9159, val_acc=22.3%, val_loss=2.3186\n",
            "  Ep7/95: train_acc=70.9%  train_loss=0.8563, val_acc=24.6%, val_loss=2.2777\n",
            "  Ep8/95: train_acc=75.4%  train_loss=0.7192, val_acc=34.3%, val_loss=2.2257\n",
            "  Ep9/95: train_acc=71.6%  train_loss=0.8345, val_acc=39.1%, val_loss=1.8986\n",
            "  Ep10/95: train_acc=68.0%  train_loss=0.9247, val_acc=42.7%, val_loss=2.0064\n",
            "  Ep11/95: train_acc=74.6%  train_loss=0.7395, val_acc=45.1%, val_loss=1.6581\n",
            "  Ep12/95: train_acc=73.1%  train_loss=0.7757, val_acc=32.2%, val_loss=3.8916\n",
            "  Ep13/95: train_acc=71.4%  train_loss=0.8380, val_acc=61.9%, val_loss=1.2716\n",
            "  Ep14/95: train_acc=76.0%  train_loss=0.6970, val_acc=42.7%, val_loss=1.5920\n",
            "  Ep15/95: train_acc=76.3%  train_loss=0.6882, val_acc=39.7%, val_loss=2.3777\n",
            "  Ep16/95: train_acc=77.0%  train_loss=0.6659, val_acc=47.3%, val_loss=1.6255\n",
            "  Ep17/95: train_acc=78.7%  train_loss=0.6272, val_acc=43.7%, val_loss=2.0517\n",
            "  Ep18/95: train_acc=73.3%  train_loss=0.7808, val_acc=48.6%, val_loss=1.7086\n",
            "  Ep19/95: train_acc=77.3%  train_loss=0.6664, val_acc=51.0%, val_loss=1.5309\n",
            "  Ep20/95: train_acc=78.5%  train_loss=0.6374, val_acc=40.1%, val_loss=1.9412\n",
            "  Ep21/95: train_acc=76.1%  train_loss=0.7034, val_acc=49.1%, val_loss=1.7085\n",
            "  Ep22/95: train_acc=76.2%  train_loss=0.7002, val_acc=54.0%, val_loss=1.4190\n",
            "  Ep23/95: train_acc=79.0%  train_loss=0.6213, val_acc=43.2%, val_loss=1.8971\n",
            "  Ep24/95: train_acc=80.6%  train_loss=0.5837, val_acc=43.1%, val_loss=1.7514\n",
            "  Ep25/95: train_acc=76.1%  train_loss=0.7088, val_acc=60.1%, val_loss=1.5231\n",
            "  Ep26/95: train_acc=80.0%  train_loss=0.5864, val_acc=40.6%, val_loss=2.2048\n",
            "  Ep27/95: train_acc=81.9%  train_loss=0.5391, val_acc=40.0%, val_loss=2.3629\n",
            "  Ep28/95: train_acc=75.0%  train_loss=0.7412, val_acc=35.2%, val_loss=4.0720\n",
            "  Ep29/95: train_acc=76.1%  train_loss=0.6984, val_acc=46.6%, val_loss=1.5067\n",
            "  Ep30/95: train_acc=79.5%  train_loss=0.6009, val_acc=44.6%, val_loss=1.8501\n",
            "  Ep31/95: train_acc=73.3%  train_loss=0.8133, val_acc=29.4%, val_loss=5.7965\n",
            "  Ep32/95: train_acc=69.5%  train_loss=0.8947, val_acc=39.4%, val_loss=1.7618\n",
            "  Ep33/95: train_acc=74.9%  train_loss=0.7219, val_acc=46.0%, val_loss=1.6248\n",
            "  Ep34/95: train_acc=78.1%  train_loss=0.6428, val_acc=52.6%, val_loss=1.6519\n",
            "  Ep35/95: train_acc=73.4%  train_loss=0.7820, val_acc=54.4%, val_loss=1.3571\n",
            "  Ep36/95: train_acc=82.3%  train_loss=0.5406, val_acc=82.2%, val_loss=0.5576\n",
            "  Ep37/95: train_acc=85.9%  train_loss=0.4236, val_acc=84.1%, val_loss=0.4767\n",
            "  Ep38/95: train_acc=87.8%  train_loss=0.3713, val_acc=77.1%, val_loss=0.7395\n",
            "  Ep39/95: train_acc=88.5%  train_loss=0.3473, val_acc=82.4%, val_loss=0.5405\n",
            "  Ep40/95: train_acc=89.0%  train_loss=0.3288, val_acc=85.7%, val_loss=0.4421\n",
            "  Ep41/95: train_acc=90.1%  train_loss=0.3069, val_acc=83.2%, val_loss=0.4743\n",
            "  Ep42/95: train_acc=90.7%  train_loss=0.2890, val_acc=88.2%, val_loss=0.3746\n",
            "  Ep43/95: train_acc=91.1%  train_loss=0.2766, val_acc=75.4%, val_loss=0.8021\n",
            "  Ep44/95: train_acc=90.9%  train_loss=0.2829, val_acc=83.9%, val_loss=0.4873\n",
            "  Ep45/95: train_acc=91.6%  train_loss=0.2637, val_acc=84.5%, val_loss=0.4758\n",
            "  Ep46/95: train_acc=92.0%  train_loss=0.2574, val_acc=85.0%, val_loss=0.4546\n",
            "  Ep47/95: train_acc=91.8%  train_loss=0.2544, val_acc=69.3%, val_loss=1.4308\n",
            "  Ep48/95: train_acc=92.5%  train_loss=0.2392, val_acc=84.6%, val_loss=0.4697\n",
            "  Ep49/95: train_acc=92.8%  train_loss=0.2308, val_acc=78.1%, val_loss=0.7763\n",
            "  Ep50/95: train_acc=93.3%  train_loss=0.2156, val_acc=85.7%, val_loss=0.4479\n",
            "  Ep51/95: train_acc=93.5%  train_loss=0.2057, val_acc=68.3%, val_loss=1.1994\n",
            "  Ep52/95: train_acc=93.6%  train_loss=0.2028, val_acc=75.5%, val_loss=0.8763\n",
            "  Ep53/95: train_acc=93.6%  train_loss=0.2073, val_acc=79.0%, val_loss=0.6380\n",
            "  Ep54/95: train_acc=93.5%  train_loss=0.2046, val_acc=84.8%, val_loss=0.4497\n",
            "  Ep55/95: train_acc=93.2%  train_loss=0.2131, val_acc=75.3%, val_loss=0.8023\n",
            "  Ep56/95: train_acc=93.3%  train_loss=0.2155, val_acc=71.5%, val_loss=1.1042\n",
            "  Ep57/95: train_acc=93.8%  train_loss=0.2011, val_acc=79.1%, val_loss=0.8190\n",
            "  Ep58/95: train_acc=94.6%  train_loss=0.1765, val_acc=70.7%, val_loss=1.1773\n",
            "  Ep59/95: train_acc=94.4%  train_loss=0.1807, val_acc=83.8%, val_loss=0.5378\n",
            "  Ep60/95: train_acc=95.2%  train_loss=0.1619, val_acc=88.2%, val_loss=0.3782\n",
            "  Ep61/95: train_acc=94.7%  train_loss=0.1716, val_acc=86.3%, val_loss=0.4595\n",
            "  Ep62/95: train_acc=94.3%  train_loss=0.1866, val_acc=75.3%, val_loss=0.8939\n",
            "  Ep63/95: train_acc=95.1%  train_loss=0.1620, val_acc=82.8%, val_loss=0.5531\n",
            "  Ep64/95: train_acc=94.7%  train_loss=0.1736, val_acc=78.6%, val_loss=0.6676\n",
            "  Ep65/95: train_acc=95.7%  train_loss=0.1488, val_acc=71.1%, val_loss=1.2153\n",
            "  Ep66/95: train_acc=97.9%  train_loss=0.0910, val_acc=92.2%, val_loss=0.2500\n",
            "  Ep67/95: train_acc=99.0%  train_loss=0.0643, val_acc=93.1%, val_loss=0.2463\n",
            "  Ep68/95: train_acc=99.2%  train_loss=0.0581, val_acc=93.2%, val_loss=0.2416\n",
            "  Ep69/95: train_acc=99.4%  train_loss=0.0532, val_acc=93.4%, val_loss=0.2430\n",
            "  Ep70/95: train_acc=99.4%  train_loss=0.0511, val_acc=92.5%, val_loss=0.2461\n",
            "  Ep71/95: train_acc=99.5%  train_loss=0.0486, val_acc=92.9%, val_loss=0.2469\n",
            "  Ep72/95: train_acc=99.6%  train_loss=0.0458, val_acc=92.8%, val_loss=0.2455\n",
            "  Ep73/95: train_acc=99.6%  train_loss=0.0431, val_acc=92.5%, val_loss=0.2501\n",
            "  Ep74/95: train_acc=99.6%  train_loss=0.0423, val_acc=93.0%, val_loss=0.2488\n",
            "  Ep75/95: train_acc=99.7%  train_loss=0.0400, val_acc=93.1%, val_loss=0.2464\n",
            "  Ep76/95: train_acc=99.8%  train_loss=0.0380, val_acc=93.0%, val_loss=0.2455\n",
            "  Ep77/95: train_acc=99.7%  train_loss=0.0392, val_acc=92.7%, val_loss=0.2452\n",
            "  Ep78/95: train_acc=99.8%  train_loss=0.0368, val_acc=92.8%, val_loss=0.2507\n",
            "  Ep79/95: train_acc=99.8%  train_loss=0.0363, val_acc=92.2%, val_loss=0.2615\n",
            "  Ep80/95: train_acc=99.8%  train_loss=0.0354, val_acc=92.5%, val_loss=0.2568\n",
            "  Ep81/95: train_acc=99.8%  train_loss=0.0341, val_acc=92.6%, val_loss=0.2503\n",
            "  Ep82/95: train_acc=99.8%  train_loss=0.0359, val_acc=92.2%, val_loss=0.2692\n",
            "  Ep83/95: train_acc=99.8%  train_loss=0.0351, val_acc=92.4%, val_loss=0.2558\n",
            "  Ep84/95: train_acc=99.8%  train_loss=0.0326, val_acc=91.9%, val_loss=0.2908\n",
            "  Ep85/95: train_acc=99.9%  train_loss=0.0323, val_acc=92.6%, val_loss=0.2545\n",
            "  Ep86/95: train_acc=99.9%  train_loss=0.0317, val_acc=92.6%, val_loss=0.2554\n",
            "  Ep87/95: train_acc=99.9%  train_loss=0.0313, val_acc=92.6%, val_loss=0.2518\n",
            "  Ep88/95: train_acc=99.9%  train_loss=0.0307, val_acc=92.7%, val_loss=0.2518\n",
            "  Ep89/95: train_acc=99.9%  train_loss=0.0311, val_acc=92.7%, val_loss=0.2531\n",
            "  Ep90/95: train_acc=99.9%  train_loss=0.0309, val_acc=92.8%, val_loss=0.2491\n",
            "  Ep91/95: train_acc=99.9%  train_loss=0.0303, val_acc=92.8%, val_loss=0.2539\n",
            "  Ep92/95: train_acc=99.9%  train_loss=0.0303, val_acc=92.8%, val_loss=0.2471\n",
            "  Ep93/95: train_acc=99.9%  train_loss=0.0303, val_acc=92.7%, val_loss=0.2510\n",
            "  Ep94/95: train_acc=99.9%  train_loss=0.0299, val_acc=93.1%, val_loss=0.2483\n",
            "  Ep95/95: train_acc=99.9%  train_loss=0.0304, val_acc=92.8%, val_loss=0.2494\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>▄▇████████████▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▄▃▃▄▄▄▅▄▅▄▅▄▄▄▄▆▆▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▄▆▄▄▃▄▃▃▃▃▃▃▃▄▄▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▁▁▂▂▃▄▂▃▄▄▄▃▃▅▃▄▂▄▄▇▇▆▇▆▆▆▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▃▅▄▄▃▃▃▆▃▃▄▆▃▃▂▁▁▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>95</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>train_acc</td><td>99.88889</td></tr><tr><td>train_loss</td><td>0.03044</td></tr><tr><td>val_acc</td><td>92.81481</td></tr><tr><td>val_loss</td><td>0.24943</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS512_LR1e-01_WD1e-02_Sched_long</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/g0gxas63' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/g0gxas63</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250610_173424-g0gxas63/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=512, LR=1.0e-01, WD=1.0e-04, Schedule=short\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441457152366638, 0.3800985515117645, 0.40766361355781555]\n",
            "Computed std:  [0.09299741685390472, 0.06464490294456482, 0.05413917079567909]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250610_180135-66evxdu3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/66evxdu3' target=\"_blank\">BS512_LR1e-01_WD1e-04_Sched_short</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/66evxdu3' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/66evxdu3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/60: train_acc=24.1%  train_loss=2.1721, val_acc=21.4%, val_loss=70.9680\n",
            "  Ep2/60: train_acc=55.7%  train_loss=1.2217, val_acc=48.7%, val_loss=2.7230\n",
            "  Ep3/60: train_acc=69.9%  train_loss=0.8667, val_acc=61.5%, val_loss=1.0913\n",
            "  Ep4/60: train_acc=69.4%  train_loss=0.9786, val_acc=42.0%, val_loss=12.7844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep5/60: train_acc=75.6%  train_loss=0.6980, val_acc=65.1%, val_loss=1.2664\n",
            "  Ep6/60: train_acc=81.2%  train_loss=0.5370, val_acc=76.2%, val_loss=0.7054\n",
            "  Ep7/60: train_acc=83.4%  train_loss=0.4704, val_acc=81.6%, val_loss=0.4924\n",
            "  Ep8/60: train_acc=87.0%  train_loss=0.3782, val_acc=78.8%, val_loss=0.6390\n",
            "  Ep9/60: train_acc=89.8%  train_loss=0.2923, val_acc=82.4%, val_loss=0.5696\n",
            "  Ep10/60: train_acc=90.5%  train_loss=0.2663, val_acc=60.7%, val_loss=3.1739\n",
            "  Ep11/60: train_acc=90.9%  train_loss=0.2584, val_acc=64.8%, val_loss=4.8810\n",
            "  Ep12/60: train_acc=91.5%  train_loss=0.2406, val_acc=83.2%, val_loss=0.5728\n",
            "  Ep13/60: train_acc=94.5%  train_loss=0.1574, val_acc=78.0%, val_loss=0.9296\n",
            "  Ep14/60: train_acc=94.7%  train_loss=0.1549, val_acc=83.9%, val_loss=0.5315\n",
            "  Ep15/60: train_acc=96.2%  train_loss=0.1130, val_acc=85.3%, val_loss=0.5495\n",
            "  Ep16/60: train_acc=96.5%  train_loss=0.0965, val_acc=82.4%, val_loss=0.6820\n",
            "  Ep17/60: train_acc=95.4%  train_loss=0.1386, val_acc=75.1%, val_loss=1.6448\n",
            "  Ep18/60: train_acc=69.5%  train_loss=1.3079, val_acc=67.7%, val_loss=4.1898\n",
            "  Ep19/60: train_acc=76.8%  train_loss=0.6612, val_acc=73.3%, val_loss=0.8166\n",
            "  Ep20/60: train_acc=80.2%  train_loss=0.5690, val_acc=65.9%, val_loss=56.6339\n",
            "  Ep21/60: train_acc=79.8%  train_loss=0.5766, val_acc=77.6%, val_loss=0.6723\n",
            "  Ep22/60: train_acc=85.9%  train_loss=0.3977, val_acc=85.3%, val_loss=0.4314\n",
            "  Ep23/60: train_acc=90.5%  train_loss=0.2763, val_acc=87.3%, val_loss=0.3674\n",
            "  Ep24/60: train_acc=92.6%  train_loss=0.2138, val_acc=88.1%, val_loss=0.3498\n",
            "  Ep25/60: train_acc=93.5%  train_loss=0.1889, val_acc=88.3%, val_loss=0.3477\n",
            "  Ep26/60: train_acc=94.2%  train_loss=0.1663, val_acc=88.2%, val_loss=0.3581\n",
            "  Ep27/60: train_acc=95.0%  train_loss=0.1470, val_acc=88.3%, val_loss=0.3603\n",
            "  Ep28/60: train_acc=95.8%  train_loss=0.1258, val_acc=88.0%, val_loss=0.3887\n",
            "  Ep29/60: train_acc=96.6%  train_loss=0.1050, val_acc=88.0%, val_loss=0.4000\n",
            "  Ep30/60: train_acc=97.3%  train_loss=0.0839, val_acc=88.3%, val_loss=0.4250\n",
            "  Ep31/60: train_acc=97.5%  train_loss=0.0821, val_acc=88.1%, val_loss=0.4159\n",
            "  Ep32/60: train_acc=98.3%  train_loss=0.0593, val_acc=88.5%, val_loss=0.4400\n",
            "  Ep33/60: train_acc=98.8%  train_loss=0.0448, val_acc=87.7%, val_loss=0.4778\n",
            "  Ep34/60: train_acc=99.1%  train_loss=0.0357, val_acc=87.7%, val_loss=0.5014\n",
            "  Ep35/60: train_acc=98.4%  train_loss=0.0513, val_acc=87.1%, val_loss=0.5452\n",
            "  Ep36/60: train_acc=99.1%  train_loss=0.0330, val_acc=87.9%, val_loss=0.5309\n",
            "  Ep37/60: train_acc=99.4%  train_loss=0.0247, val_acc=88.0%, val_loss=0.5515\n",
            "  Ep38/60: train_acc=99.7%  train_loss=0.0139, val_acc=87.6%, val_loss=0.5736\n",
            "  Ep39/60: train_acc=99.7%  train_loss=0.0139, val_acc=87.0%, val_loss=0.6336\n",
            "  Ep40/60: train_acc=99.6%  train_loss=0.0140, val_acc=87.9%, val_loss=0.6007\n",
            "  Ep41/60: train_acc=99.8%  train_loss=0.0084, val_acc=87.7%, val_loss=0.6101\n",
            "  Ep42/60: train_acc=99.9%  train_loss=0.0067, val_acc=87.9%, val_loss=0.6074\n",
            "  Ep43/60: train_acc=99.9%  train_loss=0.0067, val_acc=87.8%, val_loss=0.6100\n",
            "  Ep44/60: train_acc=99.9%  train_loss=0.0069, val_acc=87.8%, val_loss=0.6093\n",
            "  Ep45/60: train_acc=99.8%  train_loss=0.0090, val_acc=87.9%, val_loss=0.6005\n",
            "  Ep46/60: train_acc=99.8%  train_loss=0.0074, val_acc=87.7%, val_loss=0.6039\n",
            "  Ep47/60: train_acc=99.9%  train_loss=0.0061, val_acc=87.9%, val_loss=0.6029\n",
            "  Ep48/60: train_acc=99.8%  train_loss=0.0079, val_acc=87.9%, val_loss=0.6084\n",
            "  Ep49/60: train_acc=99.9%  train_loss=0.0055, val_acc=88.3%, val_loss=0.6019\n",
            "  Ep50/60: train_acc=99.9%  train_loss=0.0056, val_acc=88.1%, val_loss=0.6046\n",
            "  Ep51/60: train_acc=99.9%  train_loss=0.0055, val_acc=88.2%, val_loss=0.5990\n",
            "  Ep52/60: train_acc=99.9%  train_loss=0.0050, val_acc=88.1%, val_loss=0.6090\n",
            "  Ep53/60: train_acc=99.9%  train_loss=0.0070, val_acc=88.0%, val_loss=0.6055\n",
            "  Ep54/60: train_acc=99.8%  train_loss=0.0079, val_acc=88.2%, val_loss=0.6059\n",
            "  Ep55/60: train_acc=99.9%  train_loss=0.0054, val_acc=88.0%, val_loss=0.6088\n",
            "  Ep56/60: train_acc=99.9%  train_loss=0.0064, val_acc=88.1%, val_loss=0.6105\n",
            "  Ep57/60: train_acc=99.9%  train_loss=0.0053, val_acc=88.1%, val_loss=0.6114\n",
            "  Ep58/60: train_acc=99.9%  train_loss=0.0047, val_acc=88.1%, val_loss=0.6080\n",
            "  Ep59/60: train_acc=99.9%  train_loss=0.0048, val_acc=88.1%, val_loss=0.6103\n",
            "  Ep60/60: train_acc=99.9%  train_loss=0.0066, val_acc=88.1%, val_loss=0.6084\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▄▇█████████████▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▇▇▇▇▇█▆▆▆▇▇▇███████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▂▂▂▂▂▂▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▄▁▄▆▇▄▄▇▆▇▅▆▅▆██████████████████████████</td></tr><tr><td>val_loss</td><td>█▁▁▂▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_acc</td><td>99.89352</td></tr><tr><td>train_loss</td><td>0.00662</td></tr><tr><td>val_acc</td><td>88.11111</td></tr><tr><td>val_loss</td><td>0.60835</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BS512_LR1e-01_WD1e-04_Sched_short</strong> at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/66evxdu3' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/66evxdu3</a><br> View project at: <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250610_180135-66evxdu3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Testing BS=512, LR=1.0e-01, WD=1.0e-04, Schedule=medium\n",
            "Total samples in folder: 27000, classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Stratified split sizes: train=21600, val=2700, test=2700\n",
            "Computed mean: [0.3441457152366638, 0.3800985515117645, 0.40766361355781555]\n",
            "Computed std:  [0.09299741685390472, 0.06464490294456482, 0.05413917079567909]\n",
            "Train/Val/Test splits: 21600/2700/2700\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/users/c/carvalhj/projects/eurosat_preprocessing/exploratory_notebooks/wandb/run-20250610_181852-08f53xxk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/08f53xxk' target=\"_blank\">BS512_LR1e-01_WD1e-04_Sched_medium</a></strong> to <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/08f53xxk' target=\"_blank\">https://wandb.ai/analiju-paris/eurosat-supervised-scratch-grid-search-lrsched/runs/08f53xxk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep1/234: train_acc=23.1%  train_loss=2.0663, val_acc=20.8%, val_loss=7.6332\n",
            "  Ep2/234: train_acc=46.4%  train_loss=1.4680, val_acc=43.7%, val_loss=3.4499\n",
            "  Ep3/234: train_acc=67.5%  train_loss=0.9260, val_acc=44.0%, val_loss=2.3131\n",
            "  Ep4/234: train_acc=73.4%  train_loss=0.7557, val_acc=53.0%, val_loss=1.8635\n",
            "  Ep5/234: train_acc=77.7%  train_loss=0.6357, val_acc=56.3%, val_loss=3.0373\n",
            "  Ep6/234: train_acc=80.3%  train_loss=0.5666, val_acc=58.4%, val_loss=1.4696\n",
            "  Ep7/234: train_acc=82.8%  train_loss=0.4984, val_acc=71.1%, val_loss=0.8551\n",
            "  Ep8/234: train_acc=84.8%  train_loss=0.4334, val_acc=69.6%, val_loss=0.9583\n",
            "  Ep9/234: train_acc=86.9%  train_loss=0.3814, val_acc=65.1%, val_loss=1.5002\n",
            "  Ep10/234: train_acc=86.5%  train_loss=0.3977, val_acc=52.2%, val_loss=3.1940\n",
            "  Ep11/234: train_acc=79.6%  train_loss=0.6554, val_acc=52.1%, val_loss=32.0678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/c/carvalhj/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ep12/234: train_acc=84.9%  train_loss=0.4436, val_acc=80.8%, val_loss=0.5640\n",
            "  Ep13/234: train_acc=88.9%  train_loss=0.3210, val_acc=83.6%, val_loss=0.5362\n",
            "  Ep14/234: train_acc=90.3%  train_loss=0.2874, val_acc=83.0%, val_loss=0.5162\n",
            "  Ep15/234: train_acc=61.9%  train_loss=1.4221, val_acc=30.5%, val_loss=7541.6324\n",
            "  Ep16/234: train_acc=67.2%  train_loss=0.9244, val_acc=57.6%, val_loss=1.1676\n",
            "  Ep17/234: train_acc=73.2%  train_loss=0.7587, val_acc=74.0%, val_loss=0.7297\n",
            "  Ep18/234: train_acc=76.9%  train_loss=0.6428, val_acc=64.9%, val_loss=1.0262\n",
            "  Ep19/234: train_acc=80.4%  train_loss=0.5535, val_acc=80.7%, val_loss=0.5325\n",
            "  Ep20/234: train_acc=82.9%  train_loss=0.4856, val_acc=77.1%, val_loss=0.6481\n",
            "  Ep21/234: train_acc=83.5%  train_loss=0.4711, val_acc=74.2%, val_loss=0.9843\n",
            "  Ep22/234: train_acc=85.1%  train_loss=0.4200, val_acc=83.4%, val_loss=0.4672\n",
            "  Ep23/234: train_acc=87.1%  train_loss=0.3636, val_acc=71.7%, val_loss=0.9703\n",
            "  Ep24/234: train_acc=88.9%  train_loss=0.3140, val_acc=82.3%, val_loss=0.5510\n",
            "  Ep25/234: train_acc=89.2%  train_loss=0.3032, val_acc=76.7%, val_loss=0.9114\n",
            "  Ep26/234: train_acc=91.1%  train_loss=0.2592, val_acc=77.2%, val_loss=1.0350\n",
            "  Ep27/234: train_acc=91.6%  train_loss=0.2344, val_acc=85.7%, val_loss=0.4631\n",
            "  Ep28/234: train_acc=91.0%  train_loss=0.2534, val_acc=82.7%, val_loss=0.5584\n",
            "  Ep29/234: train_acc=93.4%  train_loss=0.1948, val_acc=72.7%, val_loss=1.2941\n",
            "  Ep30/234: train_acc=94.2%  train_loss=0.1662, val_acc=80.0%, val_loss=0.7750\n",
            "  Ep31/234: train_acc=93.4%  train_loss=0.1902, val_acc=85.3%, val_loss=0.5610\n",
            "  Ep32/234: train_acc=94.6%  train_loss=0.1497, val_acc=86.8%, val_loss=0.4640\n",
            "  Ep33/234: train_acc=94.1%  train_loss=0.1701, val_acc=84.7%, val_loss=0.5552\n",
            "  Ep34/234: train_acc=95.9%  train_loss=0.1170, val_acc=79.7%, val_loss=1.0906\n",
            "  Ep35/234: train_acc=95.6%  train_loss=0.1254, val_acc=87.9%, val_loss=0.4289\n",
            "  Ep36/234: train_acc=96.8%  train_loss=0.0922, val_acc=82.3%, val_loss=0.7166\n",
            "  Ep37/234: train_acc=97.2%  train_loss=0.0851, val_acc=89.1%, val_loss=0.4040\n",
            "  Ep38/234: train_acc=97.8%  train_loss=0.0642, val_acc=83.7%, val_loss=0.7079\n",
            "  Ep39/234: train_acc=98.1%  train_loss=0.0563, val_acc=85.3%, val_loss=0.7184\n",
            "  Ep40/234: train_acc=98.0%  train_loss=0.0611, val_acc=69.5%, val_loss=2.6635\n",
            "  Ep41/234: train_acc=96.1%  train_loss=0.1185, val_acc=87.5%, val_loss=0.4770\n",
            "  Ep42/234: train_acc=97.7%  train_loss=0.0639, val_acc=84.4%, val_loss=0.6475\n",
            "  Ep43/234: train_acc=98.8%  train_loss=0.0356, val_acc=78.0%, val_loss=1.3133\n",
            "  Ep44/234: train_acc=97.5%  train_loss=0.0739, val_acc=82.7%, val_loss=0.7767\n",
            "  Ep45/234: train_acc=97.8%  train_loss=0.0650, val_acc=83.0%, val_loss=0.7031\n",
            "  Ep46/234: train_acc=96.9%  train_loss=0.0936, val_acc=79.4%, val_loss=0.9687\n",
            "  Ep47/234: train_acc=98.1%  train_loss=0.0553, val_acc=77.9%, val_loss=1.1898\n",
            "  Ep48/234: train_acc=99.2%  train_loss=0.0206, val_acc=83.9%, val_loss=0.8181\n",
            "  Ep49/234: train_acc=99.2%  train_loss=0.0230, val_acc=87.4%, val_loss=0.6246\n",
            "  Ep50/234: train_acc=99.1%  train_loss=0.0264, val_acc=89.3%, val_loss=0.5222\n",
            "  Ep51/234: train_acc=98.7%  train_loss=0.0389, val_acc=83.6%, val_loss=0.7372\n",
            "  Ep52/234: train_acc=98.9%  train_loss=0.0325, val_acc=89.4%, val_loss=0.4648\n",
            "  Ep53/234: train_acc=99.2%  train_loss=0.0237, val_acc=80.7%, val_loss=1.1878\n",
            "  Ep54/234: train_acc=99.4%  train_loss=0.0198, val_acc=86.9%, val_loss=0.6226\n",
            "  Ep55/234: train_acc=98.3%  train_loss=0.0497, val_acc=85.4%, val_loss=0.6704\n",
            "  Ep56/234: train_acc=99.3%  train_loss=0.0197, val_acc=81.3%, val_loss=1.2389\n",
            "  Ep57/234: train_acc=99.2%  train_loss=0.0256, val_acc=88.3%, val_loss=0.5663\n",
            "  Ep58/234: train_acc=99.5%  train_loss=0.0171, val_acc=78.3%, val_loss=1.1537\n",
            "  Ep59/234: train_acc=98.9%  train_loss=0.0365, val_acc=85.3%, val_loss=0.6651\n",
            "  Ep60/234: train_acc=99.2%  train_loss=0.0254, val_acc=89.5%, val_loss=0.5196\n",
            "  Ep61/234: train_acc=99.4%  train_loss=0.0173, val_acc=89.7%, val_loss=0.5587\n",
            "  Ep62/234: train_acc=99.2%  train_loss=0.0235, val_acc=90.6%, val_loss=0.4547\n",
            "  Ep63/234: train_acc=99.7%  train_loss=0.0100, val_acc=90.3%, val_loss=0.4986\n",
            "  Ep64/234: train_acc=99.7%  train_loss=0.0093, val_acc=89.1%, val_loss=0.5778\n",
            "  Ep65/234: train_acc=99.6%  train_loss=0.0123, val_acc=83.7%, val_loss=1.0048\n",
            "  Ep66/234: train_acc=98.8%  train_loss=0.0380, val_acc=85.6%, val_loss=0.6348\n",
            "  Ep67/234: train_acc=99.3%  train_loss=0.0214, val_acc=89.8%, val_loss=0.4559\n",
            "  Ep68/234: train_acc=98.2%  train_loss=0.0616, val_acc=89.8%, val_loss=0.4316\n",
            "  Ep69/234: train_acc=99.1%  train_loss=0.0276, val_acc=87.4%, val_loss=0.5649\n",
            "  Ep70/234: train_acc=99.6%  train_loss=0.0127, val_acc=83.9%, val_loss=0.8910\n",
            "  Ep71/234: train_acc=99.6%  train_loss=0.0126, val_acc=88.3%, val_loss=0.6025\n",
            "  Ep72/234: train_acc=99.8%  train_loss=0.0064, val_acc=90.4%, val_loss=0.5016\n",
            "  Ep73/234: train_acc=99.3%  train_loss=0.0204, val_acc=89.0%, val_loss=0.5394\n",
            "  Ep74/234: train_acc=99.8%  train_loss=0.0070, val_acc=89.6%, val_loss=0.5431\n",
            "  Ep75/234: train_acc=99.9%  train_loss=0.0054, val_acc=87.4%, val_loss=0.6562\n",
            "  Ep76/234: train_acc=99.4%  train_loss=0.0170, val_acc=79.4%, val_loss=1.3920\n",
            "  Ep77/234: train_acc=99.5%  train_loss=0.0158, val_acc=88.0%, val_loss=0.6148\n",
            "  Ep78/234: train_acc=99.7%  train_loss=0.0089, val_acc=90.4%, val_loss=0.5229\n",
            "  Ep79/234: train_acc=99.7%  train_loss=0.0088, val_acc=90.1%, val_loss=0.5113\n",
            "  Ep80/234: train_acc=99.6%  train_loss=0.0141, val_acc=88.9%, val_loss=0.5790\n",
            "  Ep81/234: train_acc=99.8%  train_loss=0.0066, val_acc=87.8%, val_loss=0.6966\n",
            "  Ep82/234: train_acc=99.9%  train_loss=0.0030, val_acc=90.8%, val_loss=0.4984\n",
            "  Ep83/234: train_acc=99.9%  train_loss=0.0022, val_acc=91.1%, val_loss=0.4975\n",
            "  Ep84/234: train_acc=100.0%  train_loss=0.0007, val_acc=91.0%, val_loss=0.4909\n",
            "  Ep85/234: train_acc=100.0%  train_loss=0.0007, val_acc=91.3%, val_loss=0.4930\n",
            "  Ep86/234: train_acc=100.0%  train_loss=0.0005, val_acc=91.4%, val_loss=0.5030\n",
            "  Ep87/234: train_acc=100.0%  train_loss=0.0004, val_acc=91.1%, val_loss=0.4864\n",
            "  Ep88/234: train_acc=100.0%  train_loss=0.0007, val_acc=91.1%, val_loss=0.4905\n",
            "  Ep89/234: train_acc=100.0%  train_loss=0.0004, val_acc=91.3%, val_loss=0.4911\n",
            "  Ep90/234: train_acc=100.0%  train_loss=0.0005, val_acc=91.2%, val_loss=0.4863\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[1;32m      2\u001b[0m set_seed(SEED)\n\u001b[0;32m----> 4\u001b[0m best_cfg, _    \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m bs, lr, wd     \u001b[38;5;241m=\u001b[39m best_cfg\n\u001b[1;32m      6\u001b[0m tr_dl, val_dl, te_dl, n_cls \u001b[38;5;241m=\u001b[39m get_data_loaders(DATA_DIR, bs)\n",
            "Cell \u001b[0;32mIn[3], line 103\u001b[0m, in \u001b[0;36mhyperparam_search\u001b[0;34m(pretrained)\u001b[0m\n\u001b[1;32m     84\u001b[0m wandb_run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     85\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meurosat-supervised-scratch-grid-search-lrsched\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_LR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_WD\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Sched_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     }\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS_FOR_RUN):\n\u001b[0;32m--> 103\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass DEVICE to train_one_epoch\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Compute validation loss & accuracy\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
            "Cell \u001b[0;32mIn[3], line 159\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, scheduler, device)\u001b[0m\n\u001b[1;32m    156\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    157\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# <--- IMPORTANT: Step the scheduler after each batch\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Accumulate weighted by batch size\u001b[39;00m\n\u001b[1;32m    160\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    161\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Main\n",
        "set_seed(SEED)\n",
        "\n",
        "best_cfg, _    = hyperparam_search(pretrained=False)\n",
        "bs, lr, wd     = best_cfg\n",
        "tr_dl, val_dl, te_dl, n_cls = get_data_loaders(DATA_DIR, bs)\n",
        "\n",
        "# Retrain on TRAIN+VAL\n",
        "final_model, combined_ds = retrain_final_model(tr_dl, val_dl, n_cls, bs, lr, wd, NUM_EPOCHS)\n",
        "\n",
        "evaluate_and_log(final_model, te_dl, combined_ds, n_cls, bs, lr)\n",
        "\n",
        "final_path = f\"models/eurosat_supervised_final_bs{bs}_lr{lr:.0e}_epcs{NUM_EPOCHS}.pth\"\n",
        "torch.save(final_model.state_dict(), final_path)\n",
        "print(f\"Final model saved to {final_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
